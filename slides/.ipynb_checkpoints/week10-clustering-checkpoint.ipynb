{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Clustering\n",
        "author: PSTAT100 Spring 2023\n",
        "date: 'Week 10, Lecture 1'\n",
        "format:\n",
        "  revealjs:\n",
        "    smaller: true\n",
        "    incremental: true\n",
        "    slide-number: true\n",
        "    scrollable: true\n",
        "    code-fold: true\n",
        "execute:\n",
        "  echo: true\n",
        "  eval: true\n",
        "---"
      ],
      "id": "40b707c3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        },
        "tags": []
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "import statsmodels.api as sm\n",
        "alt.data_transformers.disable_max_rows()\n",
        "\n",
        "## LOOSE ENDS\n",
        "diabetes = pd.read_csv('data/diabetes.csv')\n",
        "gender_indicator = pd.get_dummies(diabetes.Gender, drop_first = True)\n",
        "x_vars = pd.concat([gender_indicator, diabetes.loc[:, ['Age', 'BMI']]], axis = 1)\n",
        "x = sm.add_constant(x_vars)\n",
        "y = (diabetes.Diabetes == 'Yes').astype('int')\n",
        "model = sm.Logit(endog = y, exog = x)\n",
        "fit = model.fit()\n",
        "fitted_probs = 1/(1 + np.exp(-fit.fittedvalues))"
      ],
      "id": "a858c7af",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Announcements\n",
        "\n",
        "* Lab 7 optional, turn in by end of day Friday 6/9\n",
        "* HW4 due Wednesday 6/7 with late submissions by Friday 6/9\n",
        "* Course project due Friday 6/16; groups preferred, max of 3\n",
        "* Extra OH Tuesday 1-3pm (scheduled final time)?\n",
        "\n",
        "\n",
        "## Last time\n",
        "\n",
        "We fit the logistic regression model:\n",
        "\n",
        "$$\n",
        "\\log\\left(\\frac{Pr(\\text{diabetic}_i)}{1 - Pr(\\text{diabetic}_i)}\\right) = \\beta_0 + \\beta_1\\text{age}_i +  \\beta_2\\text{male}_i+\\beta_3\\text{BMI}_i\n",
        "$$\n",
        "\n",
        "And discussed:\n",
        "\n",
        "* model specification\n",
        "* parameter estimation\n",
        "* parameter interpretation\n",
        "\n",
        "## Last time\n",
        "\n",
        "The logistic regression model can be employed as a *classifier* by articulating a rule:\n",
        "\n",
        "$$\n",
        "\\text{classify as diabetic} \n",
        "\\quad\\Longleftrightarrow\\quad\n",
        "\\widehat{Pr}(\\text{diabetic}) > c\n",
        "$$\n",
        "\n",
        "where $\\widehat{Pr}(\\text{diabetic})$ is computed from the fitted logistic regression model.\n",
        "\n",
        ". . .\n",
        "\n",
        "Any classifier has some:\n",
        "\n",
        "* sensitivity (true positive rate)\n",
        "* specificity (true negative rate)\n",
        "\n",
        ". . .\n",
        "\n",
        "These rates vary depending on $c$.\n",
        "\n",
        "## ROC Curves\n",
        "\n",
        "A plot of sensitivity against specificity across all unique classification thresholds is known as a *receiver operating characteristic* (ROC) curve.\n",
        "\n",
        "::: {.columns}\n",
        "\n",
        "::: {.column width=50%}\n"
      ],
      "id": "0d3b2716"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# compute \n",
        "from sklearn import metrics\n",
        "fpr, tpr, thresh = metrics.roc_curve(y, fitted_probs)\n",
        "\n",
        "roc = pd.DataFrame({\n",
        "    'fpr': fpr,\n",
        "    'tpr': tpr,\n",
        "    'thresh': thresh\n",
        "})\n",
        "\n",
        "roc_opt_ix = [(roc.tpr - roc.fpr).argmax(), ((1 - roc.tpr)**2 + roc.fpr**2).argmin()]\n",
        "roc_opt = roc.loc[roc_opt_ix]\n",
        "\n",
        "roc_plot = alt.Chart(roc).mark_line().encode(\n",
        "    x = alt.X('fpr', title = '1 - specificity'),\n",
        "    y = alt.Y('tpr', title = 'sensitivity')\n",
        ")\n",
        "\n",
        "roc_pts = alt.Chart(roc_opt).mark_circle(\n",
        "    fill = 'red',\n",
        "    size = 100\n",
        ").encode(\n",
        "    x = 'fpr',\n",
        "    y = 'tpr'\n",
        ")\n",
        "\n",
        "(roc_plot + roc_pts).configure_axis(\n",
        "    labelFontSize = 14,\n",
        "    titleFontSize = 16\n",
        ")"
      ],
      "id": "e27b6cf4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=50%}\n",
        "\n",
        "* closer to the upper left corner $\\longrightarrow$ less trade-off $\\longrightarrow$ better classifier\n",
        "* area under the curve often used as an accuracy metric\n",
        "* two common choices for $c$ highlighted:\n",
        "    + the point closest to the upper left corner\n",
        "    + the point that maximizes sensitivity + specificity (sometimes called *Youden's J statistic*)\n",
        "\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        "## Probit regression model\n",
        "\n",
        "Logistic regression is not the only regression model for binary data. One common alternative  is *probit regression*, where:\n",
        "\n",
        "$$\n",
        "P(Y = 1) = \\Phi\\left(x'\\beta\\right)\n",
        "$$\n",
        "\n",
        "* $\\Phi$ is the standard normal CDF\n",
        "* similar assumptions to logistic regression -- independence and monotonicity\n",
        "\n",
        "## Probit v. logit\n",
        "\n",
        "The logistic function has heavier tails.\n"
      ],
      "id": "3259a722"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy.stats import norm\n",
        "# grid of values\n",
        "vals = pd.DataFrame({\n",
        "    'x': np.linspace(-5, 5, 200)\n",
        "})\n",
        "\n",
        "# compute probit and logit\n",
        "vals['logit'] = 1/(1 + np.exp(-vals.x))\n",
        "vals['probit'] = norm.cdf(vals.x)\n",
        "vals = vals.melt(id_vars = 'x', var_name = 'model', value_name = 'pr')\n",
        "\n",
        "# plot\n",
        "alt.Chart(vals).mark_line().encode(\n",
        "    x = alt.X('x', title = 'x*beta'),\n",
        "    y = alt.Y('pr', title = 'Pr(Y = 1)'),\n",
        "    color = 'model',\n",
        "    stroke = 'model'\n",
        ").configure_axis(\n",
        "    labelFontSize = 14,\n",
        "    titleFontSize = 16\n",
        ").configure_legend(\n",
        "    labelFontSize = 14,\n",
        "    titleFontSize = 16\n",
        ")"
      ],
      "id": "9405b107",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Probit fit to diabetes data\n",
        "\n",
        "Here are the estimates from the probit model:\n"
      ],
      "id": "1aab64ed"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-line-numbers: '2-3'\n",
        "# fit model\n",
        "model_probit = sm.Probit(endog = y, exog = x)\n",
        "fit_probit = model_probit.fit()\n",
        "\n",
        "# parameter estimates\n",
        "coef_tbl_probit = pd.DataFrame({\n",
        "    'estimate': fit_probit.params,\n",
        "    'standard error': np.sqrt(fit_probit.cov_params().values.diagonal())},\n",
        "    index = x.columns\n",
        ")\n",
        "coef_tbl_probit"
      ],
      "id": "97e974f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ". . .\n",
        "\n",
        "Trickier to interpret coefficients directly, since $\\Phi^{-1}(p)$ is not a natural quantity\n",
        "\n",
        "## Challenges of interpretation\n",
        "\n",
        "The effect of incremental changes in explanatory variables on predicted probabilities depends on your starting point.\n"
      ],
      "id": "d0da2f4c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# means of explanatory variables\n",
        "x1 = x.mean()\n",
        "\n",
        "# increment bmi twice\n",
        "x2 = x1 + np.array([0, 0, 0, 1])\n",
        "x3 = x2 + np.array([0, 0, 0, 1])\n",
        "x_pred = pd.concat([x1, x2, x3], axis = 1).T\n",
        "x_pred['male'] = np.array([0, 0, 0])\n",
        "\n",
        "# compute predictions and differences in probability\n",
        "preds = fit_probit.predict(x_pred)\n",
        "preds.diff()"
      ],
      "id": "c0765115",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* a one-unit increase in BMI from 26.45 (sample mean) for a 37.6 year old woman is associated with an estimated change in probability of diabetes of 0.0036\n",
        "* a one-unit increase in BMI from 27.45 (sample mean plus one) for a 37.6 year old woman is associated with an estimated change in probability of diabetes of 0.0039\n",
        "\n",
        "## Centering for interpretability\n",
        "\n",
        "If explanatory variables are centered, then the change in estimated probability associated with a 1-unit change from the mean (and reference level(s)) is:\n",
        "\n",
        "$$\n",
        "\\Phi\\left(\\hat{\\beta}_0 + \\hat{\\beta}_j\\right) - \\Phi\\left(\\hat{\\beta}_0\\right)\n",
        "$$\n",
        "\n",
        ". . .\n",
        "\n",
        "Refitting the model after centering age and BMI and computing the above yields:\n"
      ],
      "id": "296a9f5a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# center explanatory variables\n",
        "x_vars_ctr = x_vars - x.mean()\n",
        "x_ctr = pd.concat([x.loc[:, ['const', 'male']], x_vars_ctr.loc[:, ['Age', 'BMI']]], axis = 1)\n",
        "\n",
        "# fit model\n",
        "model_probit_ctr = sm.Probit(endog = y, exog = x_ctr)\n",
        "fit_probit_ctr = model_probit_ctr.fit()\n",
        "\n",
        "# baseline\n",
        "probit_baseline = norm.cdf(fit_probit_ctr.params[0])\n",
        "\n",
        "# changes in estimated probabilities associated with one-unit change from mean, keeping other variables at mean/reference\n",
        "prob_diffs = norm.cdf(fit_probit_ctr.params[1:4] + fit_probit_ctr.params[0]) - probit_baseline\n",
        "\n",
        "# print\n",
        "pd.DataFrame({'change in probability': prob_diffs}, index = np.array(['male', 'age', 'BMI']))"
      ],
      "id": "741dc2eb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Centering for interpretation\n",
        "\n",
        "Now the coefficent interpretations are:\n",
        "\n",
        "* the estimated probability that a woman of average age and BMI has diabetes is 0.029 \n",
        "    + $\\Phi(\\hat{\\beta}_0)$\n",
        "* among people of average age and BMI, men are more likely than women to be diabetic with an estimated difference in probability of 0.009 \n",
        "    + $0.008659 = \\Phi(\\hat{\\beta}_0 + \\hat{\\beta}_1) - \\Phi(\\hat{\\beta}_0)$\n",
        "* a one-year increase from the average age is associated with a change in the estimated probability that a woman of average BMI is diabetic of 0.002 \n",
        "    + $0.001772 = \\Phi(\\hat{\\beta}_0 + \\hat{\\beta}_2) - \\Phi(\\hat{\\beta}_0)$\n",
        "* a one-unit increase in BMI from the average is associated with a change in the estimated probability that a woman of average age is diabetic of 0.004 \n",
        "    + $0.003587 = \\Phi(\\hat{\\beta}_0 + \\hat{\\beta}_3) - \\Phi(\\hat{\\beta}_0)$\n",
        "\n",
        "## Un/Supervised problems\n",
        "\n",
        "Regression and classification are known as 'supervised' problems:\n",
        "\n",
        "* the response variable/outcome is observed\n",
        "* the modeling of data is guided by observation\n",
        "\n",
        ". . . \n",
        "\n",
        "By contrast, in 'unsupervised' problems:\n",
        "\n",
        "* the response variable/outcome is *not* observed\n",
        "* no ground truth to guide/supervise the modeling process\n",
        "\n",
        "## Clustering\n",
        "\n",
        "Clustering is the unsupervised version of classification:\n",
        "\n",
        "> *Can we classify observations into two or more groups based on $p$ variables without knowing the true grouping structure?*\n",
        "\n",
        "* can think of this as modeling an unobserved response\n",
        "* however, not necessary that there exist subpopulations in the data -- often a useful exploratory technique for exploring multimodal distributions\n",
        "\n",
        "## Voting records, 116th House\n",
        "\n",
        "Roll call votes of the 116th House of Representatives on bills and resolutions:\n"
      ],
      "id": "7be15b78"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "members = pd.read_csv('data/members.csv').set_index('name_id')\n",
        "votes = pd.read_csv('data/votes-clean.csv').set_index('name_id')\n",
        "vote_info = pd.read_csv('data/votes-info.csv').set_index('rollcall_id')\n",
        "\n",
        "votes.head(3)"
      ],
      "id": "179d10d3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* each column is a roll call ($p = 144$ total)\n",
        "* each row is a representative ($n = 430$ total)\n",
        "* 1 is a \"yes\" vote; 0 is an abstention; -1 is a \"no\" vote\n",
        "\n",
        "## Clustering voting data\n",
        "\n",
        "> Question: *Can we identify groups of representatives that voted similarly?*\n",
        "\n",
        "* Can cluster the representatives according to roll call votes\n",
        "* But how many clusters to expect?\n",
        "\n",
        "## EDA with PCA\n",
        "\n",
        "Projecting the data onto the first few principal components provides a way to visualize the data:\n",
        "\n",
        "::: {.columns}\n",
        "\n",
        "::: {.column width=60%}\n"
      ],
      "id": "b0f9d54a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pca = sm.PCA(votes)\n",
        "alt.Chart(pca.scores).mark_circle(opacity = 0.5).encode(\n",
        "    x = alt.X('comp_000', title = 'PC1'),\n",
        "    y = alt.Y('comp_001', title = 'PC2')\n",
        ").configure_axis(\n",
        "    labelFontSize = 14,\n",
        "    titleFontSize = 16\n",
        ").configure_legend(\n",
        "    labelFontSize = 14,\n",
        "    titleFontSize = 16\n",
        ")"
      ],
      "id": "d6bfaa01",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=40%}\n",
        "\n",
        "* can see at least two clusters\n",
        "\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        "## Clustering with $K$-means\n",
        "\n",
        "The most widely used clustering method is known as $K$-means.\n",
        "\n",
        "::: {.columns}\n",
        "\n",
        "::: {.column width=40%}\n",
        "![](figures/kmeans.PNG)\n",
        ":::\n",
        "\n",
        "::: {.column width=60%}\n",
        "\n",
        "* cluster labels are based on shortest Euclidean distance to one of $K$ centers\n",
        "* centers are found by minimizing the variance within each cluster\n",
        "\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        "## Clustering with $K$-means\n",
        "\n",
        "Technically, given $n$ observations of $p$ variables $\\mathbf{X} = \\{x_{ij}\\}$, the $k$-means problem is:\n",
        "\n",
        "$$\n",
        "\\text{minimize}_{C_1, \\dots, C_K} \\left\\{\n",
        "    \\sum_{k = 1}^K |C_k|^{-1} \\sum_{i, i' \\in C_k} \\sum_{j = 1}^p (x_{ij} - x_{i'j})^2\n",
        "\\right\\}\n",
        "$$\n",
        "\n",
        ". . .\n",
        "\n",
        "A local solution is found by starting with random cluster assignments and then interatively:\n",
        "\n",
        "1. Update the cluster centers\n",
        "2. Reassign the cluster labels\n",
        "\n",
        "## Clustering voting data\n",
        "\n",
        "The method is very easy to implement:\n"
      ],
      "id": "ab0bac71"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "#| code-fold: false\n",
        "from sklearn.cluster import KMeans\n",
        "np.random.seed(60623)\n",
        "\n",
        "clust = KMeans(n_clusters = 2)\n",
        "clust.fit(votes)\n",
        "clust_labels = clust.predict(votes)"
      ],
      "id": "6c9e7366",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ". . .\n",
        "\n",
        "Cluster labels will be returned in the same order as the rows input to `.predict()`\n",
        "\n",
        ". . .\n",
        "\n",
        "Initialization is random, so solutions may differ from run to run (usually just permutes labels).\n",
        "\n",
        "## Clustering voting data\n",
        "\n",
        "We could again visualize the clusters using PCA:\n"
      ],
      "id": "8e82820c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_df = pca.scores.iloc[:, 0:2].copy()\n",
        "plot_df['cluster'] = clust_labels\n",
        "\n",
        "alt.Chart(plot_df).mark_circle(opacity = 0.5).encode(\n",
        "    x = alt.X('comp_000', title = 'PC1'),\n",
        "    y = alt.Y('comp_001', title = 'PC2'),\n",
        "    color = 'cluster:N'\n",
        ").configure_axis(\n",
        "    labelFontSize = 14,\n",
        "    titleFontSize = 16\n",
        ").configure_legend(\n",
        "    labelFontSize = 14,\n",
        "    titleFontSize = 16\n",
        ")"
      ],
      "id": "d7b382ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cluster composition by party\n",
        "\n",
        "We could also cross-tabulate the cluster labels with party affiliations: \n"
      ],
      "id": "de8754b9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "label_df = pd.DataFrame({'cluster': clust_labels}, index = votes.index)\n",
        "\n",
        "pd.merge(members, label_df, left_index = True, right_index = True).groupby(['current_party', 'cluster']).size().reset_index().pivot(columns = 'current_party', index = 'cluster')"
      ],
      "id": "df83b9de",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cluster composition by party\n",
        "\n",
        "Who are those three representatives that vote with the democrats?\n"
      ],
      "id": "f20008c8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "members_labeled = pd.merge(members, label_df, left_index = True, right_index = True)\n",
        "\n",
        "members_labeled[(members_labeled.cluster == 0) & (members_labeled.current_party == 'Republican')]"
      ],
      "id": "73a7a907",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further possible questions\n",
        "\n",
        "We could use the same technique to explore a variety of additional questions:\n",
        "\n",
        "* identify voting blocs by issue or policy area (use a subset of columns)\n",
        "* find within-party voting blocs (increase $K$)\n",
        "* identify representatives that *don't* tend to vote together with others (assign a score based on how 'quickly' a representative is isolated)"
      ],
      "id": "bf8a9b26"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}