[
  {
    "objectID": "activities/week3-activity-miller.html",
    "href": "activities/week3-activity-miller.html",
    "title": "In class activity",
    "section": "",
    "text": "This notebook details the voter survey simulation presented in lecture as part of the Miller case study. Recall that the point of the simulation is to show that, under some assumptions about the sampling design and missing data mechanism, a strongly biased result is expected even when the actual rate of erroneous/fraudulent mail ballot requests is very low.\nFor this activity you’ll get to tinker with the simulation settings to better understand the example and the factors that might impact bias in the results.\nimport numpy as np\nimport pandas as pd\nThe cell below simulates a hypothetical population of 150,000 voters who were issued mail ballots according to state records.\nThe quantity true_prop is the population parameter we will ultimately estimate; this parameter is the proportion of the voters who were issued mail ballots according to state records but who did not request mail ballots. For these voters, either the mail ballots were issued erroneously or they were fraudulently requested. In context, a large estimate for this quantity is suggestive of some irregularities pertaining to the mail-in vote.\nBelow this parameter of interest is set at \\(0.5\\%\\).\nBased on true_prop, an indicator is assigned to each voter that is a 1 if the voter requested the mail ballot and a 0 otherwise; for simplicity, all zeroes are added to the top \\(N\\times\\)true_prop rows and the remaining rows are assigned ones.\n# for reproducibility\nnp.random.seed(41021)\n\n# proportion of fraud/error\ntrue_prop = 0.005\n\n# generate population of voters\nN = 150000\npopulation = pd.DataFrame(data = {'requested': np.ones(N)})\n\n# add a label indicating whether the voter requested a mail ballot\nnum_nrequest = round(N*true_prop) - 1\npopulation.iloc[0:num_nrequest, 0] = 0"
  },
  {
    "objectID": "activities/week3-activity-miller.html#simulating-sampling-mechanisms",
    "href": "activities/week3-activity-miller.html#simulating-sampling-mechanisms",
    "title": "In class activity",
    "section": "Simulating sampling mechanisms",
    "text": "Simulating sampling mechanisms\nThe cell below assigns sampling weights that represent the probability a voter in the population answers the phone and agrees to an interview.\nThe weights can be thought of as expected conditional response rates – the probabilities that (a) a voter is interviewed given they did request a mail ballot and (b) a voter is interviewed given that they did not request a mail ballot.\nThe assumption figuring in the weight calculation is that voters who did not request mail ballots are more likely to agree to an interview. This would naturally occur if the interviewer is not careful about the interview request and discloses immediately that they are investigating irregularities in mail ballot requests – those who didn’t experience any irregularities are much more likely to hang up or decline.\nCurrently, it is assumed that voters who did not request ballots are 10 times more likely to talk than those who did. This factor is stored as talk_factor. The overall response rate is set at \\(5\\%\\) and stored as p_talk. The weight calculation proceeds using the law of total probability:\n\\[\nP(T) = P(T|R)\\left(P(R) + \\underbrace{\\frac{P(T|NR)}{P(T|R)}}_{\\text{talk factor}}P(NR)\\right)\n\\]\nRearrangement yields an expression for \\(P(T|R)\\) in terms of the request rates, overall response rate, and talking factor.\n\n# probability that a randomly chosen voter requested a mail ballot\np_request = 1 - true_prop\n\n# probability that a randomly chosen voter did not request a mail ballot\np_nrequest = true_prop\n\n# assume respondents who did not request are more likely to talk by this factor\ntalk_factor = 10\n\n# overall response rate\np_talk = 0.05\n\n# conditional response rates \np_talk_request = p_talk/(p_request + talk_factor*p_nrequest) \np_talk_nrequest = talk_factor*p_talk_request\n\n# print\nprint('rate for requesters: ', p_talk_request)\nprint('rate for non-requesters: ', p_talk_nrequest)\n\nrate for requesters:  0.04784688995215312\nrate for non-requesters:  0.47846889952153115\n\n\nIf you like, feel free to adjust the overall response rate and talking factor to values that interest you empirically. The question to ask to determine these values is:\n\nIf I assume the response rate is \\(x\\) and that those who did not request mail ballots are \\(y\\) times more likely to talk, how much bias will that induce for the estimated proportion of erroneous/fraudulent requests?\n\nChoose values for \\(x\\) and \\(y\\) for which you’d like to know the answer. Make sure the conditional rates are valid probabilities. The cell below will draw a sample for your specifications.\n\n# sample size\nn = 2500\n\n# draw sample weighted by conditional probabilities\nnp.random.seed(41923)\npopulation.loc[population.requested == 1, 'sample_weight'] = p_talk_request\npopulation.loc[population.requested == 0, 'sample_weight'] = p_talk_nrequest\nsamp = population.sample(n = n, replace = False, weights = 'sample_weight')\n\nThe cell below returns the estimated proportion of erroneous/fraudulent requests and the error associated with this estimate.\n\n\nprint('estimated fraudulent/erroneous requests: ', 1 - samp.requested.mean())\nprint('true value: ', true_prop)\nprint('estimation error: ', 1 - samp.requested.mean() - true_prop)\n\nestimated fraudulent/erroneous requests:  0.04400000000000004\ntrue value:  0.005\nestimation error:  0.03900000000000004\n\n\nExtrapolating this estimate to a raw vote count among the population yields the following:\n\n\nprint('estimated fraudulent/erroneous requests: ', \n      np.round(N*(1 - samp.requested.mean())))\nprint('true value: ', N*true_prop)\nprint('estimation error: ', \n      np.round(N*(1 - samp.requested.mean() - true_prop)))\n\nestimated fraudulent/erroneous requests:  6600.0\ntrue value:  750.0\nestimation error:  5850.0\n\n\nThe bias – average error across samples – can be estimated by repeating this sampling scheme many times. The cell below computes estimates for nsim simulated samples.\n\n# for reproducibility\nnp.random.seed(41923)\n\n# number of simulated samples\nnsim = 1000\n\n# storage for the estimates from each sample\nestimates = np.zeros(nsim)\n\n# for each simulation ...\nfor i in range(0, nsim):\n    # draw a sample and compute the estimated proportion\n    estimates[i] = population.sample(n = n, \n                                 replace = False, \n                                 weights = 'sample_weight'\n                                 ).requested.mean()\n\nThe average error for this sampling design is given below.\n\nprint('average estimate: ', np.mean((1 - estimates)))\nprint('standard deviation of estimates: ', np.std(estimates))\nprint('truth: ', true_prop)\nprint('bias (proportion): ', np.mean((1 - estimates) - true_prop))\nprint('bias (count): ', np.mean(N*((1 - estimates) - true_prop)))\n\naverage estimate:  0.04465079999999999\nstandard deviation of estimates:  0.0037278169697558933\ntruth:  0.005\nbias (proportion):  0.0396508\nbias (count):  5947.62\n\n\n\nActivity 1: tinker with the sampling mechanism\nTake note of these results or make a duplicate of the cell and re-run it so that you have a copy for later reference. Now go back and adjust the settings. Repeat the simulation and compare changes.\nSome questions you could explore are:\n\nhow does increasing the overall response rate impact the bias?\ndoes sample size matter?\nwhat response rate(s) and talking factor(s) would produce estimates of 10% or more?"
  },
  {
    "objectID": "activities/week3-activity-miller.html#simulating-missingness",
    "href": "activities/week3-activity-miller.html#simulating-missingness",
    "title": "In class activity",
    "section": "Simulating missingness",
    "text": "Simulating missingness\nSome interviews were terminated early because the respondent hung up or declined to proceed. We can think of these instances as missing values.\nThe cell below creates missingness probabilities under the assumption that those who did request mail ballots are more likely to terminate interviews than those who did not. The calculation is exactly the same as that used to figure sampling weights.\n\n# assume requesters are more likely to terminate early by this factor\nmissing_factor = 12\n\n# overall observed missing rate\np_missing = 0.5\n\n# proportions of requesters/nonrequesters in sample\np_request_samp = samp.requested.mean()\np_nrequest_samp = 1 - p_request_samp\n\n# conditional probabilities of missing given request status\np_missing_nrequest = p_missing/(p_nrequest + missing_factor*p_request) \np_missing_request = missing_factor*p_missing_nrequest\n\nprint('missing rate for requesters: ', p_missing_request)\nprint('missing rate for nonrequesters: ', p_missing_nrequest)\n\nmissing rate for requesters:  0.502302218501465\nmissing rate for nonrequesters:  0.04185851820845542\n\n\nThe following cell inputs missing values at random according to the missingness mechanism specified above.\n\n# append missingness probabilities\nsamp.loc[samp.requested == 1, 'missing_weight'] = p_missing_request\nsamp.loc[samp.requested == 0, 'missing_weight'] = p_missing_nrequest\n\n# make a copy of the sample\nsamp_incomplete = samp.copy()\n\n# input missing values at random\nnp.random.seed(41923)\nsamp_incomplete['missing'] = np.random.binomial(n = 1, p = samp_incomplete.missing_weight.values)\nsamp_incomplete.loc[samp_incomplete.missing == 1, 'requested'] = float('nan')\n\nFinally, ignoring these missing responses yields the estimate below of the proportion of erroneous/fraudulent ballot requests.\n\n\nprint('estimated fraudulent/erroneous requests: ', \n      1 - samp_incomplete.requested.mean())\nprint('true value: ', true_prop)\nprint('estimation error: ', \n      1 - samp_incomplete.requested.mean() - true_prop)\n\nestimated fraudulent/erroneous requests:  0.08307453416149069\ntrue value:  0.005\nestimation error:  0.07807453416149068\n\n\nExtrapolating this estimate to raw vote counts among the population yields the following:\n\n\nprint('estimated fraudulent/erroneous requests: ', \n      np.round(N*(1 - samp_incomplete.requested.mean())))\nprint('true value: ', N*true_prop)\nprint('estimation error: ', \n      np.round(N*(1 - samp_incomplete.requested.mean() - true_prop)))\n\nestimated fraudulent/erroneous requests:  12461.0\ntrue value:  750.0\nestimation error:  11711.0\n\n\nRepeating the entire experiment – sampling from the population and then introducing missing values – many times will allow for an assessment of the additional bias due to missingness.\n\n# for reproducibility\nnp.random.seed(41923)\n\n# number of simulated samples\nnsim = 1000\n\n# storage for estimates\nestimates = np.zeros([1000, 2])\n\n# for each simulation\nfor i in range(0, nsim):\n    # draw sample from population\n    samp_complete = population.sample(n = 2500, \n                                 replace = False, \n                                 weights = 'sample_weight'\n                                 )\n    \n    # compute mean from complete data\n    estimates[i, 0] = samp_complete.requested.mean()\n    \n    # introduce missing values\n    samp_complete.loc[samp_complete.requested == 1, 'missing_weight'] = p_missing_request\n    samp_complete.loc[samp_complete.requested == 0, 'missing_weight'] = p_missing_nrequest\n\n    # make a copy of the sample\n    samp_incomplete = samp.copy()\n\n    # input missing values at random\n    samp_incomplete['missing'] = np.random.binomial(n = 1, p = samp_incomplete.missing_weight.values)\n    samp_incomplete.loc[samp_incomplete.missing == 1, 'requested'] = float('nan')\n    estimates[i, 1] = samp_incomplete.requested.mean()\n\nNote that both an estimate with complete data (no missing values) and with incomplete data (with missing values that are dropped) are computed. This allows us to compute average errors with and without missingness, and thus, average excess error due to missingness.\n\navg_estimates = 1 - np.mean(estimates, axis = 0)\n\nprint('average estimate without missingness: ', avg_estimates[0])\nprint('average estimate with missingness: ', avg_estimates[1])\nprint('total bias: ', avg_estimates[1] - true_prop)\nprint('bias due to sampling: ', avg_estimates[0] - true_prop)\nprint('excess bias due to missingness: ', avg_estimates[1] - avg_estimates[0])\n\n\naverage estimate without missingness:  0.04469400000000001\naverage estimate with missingness:  0.08155802934699397\ntotal bias:  0.07655802934699396\nbias due to sampling:  0.039694000000000014\nexcess bias due to missingness:  0.036864029346993954\n\n\nIn terms of raw vote counts, these same quantities are:\n\nprint('average estimate without missingness: ', N*avg_estimates[0])\nprint('average estimate with missingness: ', N*avg_estimates[1])\nprint('total bias: ', N*(avg_estimates[1] - true_prop))\nprint('bias due to sampling: ', N*(avg_estimates[0] - true_prop))\nprint('excess bias due to missingness: ', N*(avg_estimates[1] - avg_estimates[0]))\n\n\naverage estimate without missingness:  6704.100000000001\naverage estimate with missingness:  12233.704402049094\ntotal bias:  11483.704402049094\nbias due to sampling:  5954.100000000002\nexcess bias due to missingness:  5529.604402049093\n\n\n\nActivity 2: tinker with missingness mechanism\nGo back and adjust the settings for inputting missing values. Choose a missingness factor and overall nonresponse rate that interest you. Some questions you could explore are:\n\nwhat is the effect of a very high nonresponse rate with little differentiation between requesters and nonrequesters?\nare there any missing data mechanisms that would actually reduce bias?\nif the missing mechanism is similar to the sampling mechanism in how it favors nonrequesters, which has the larger effect?"
  },
  {
    "objectID": "activities/week3-activity-miller.html#extra-credit-assignment",
    "href": "activities/week3-activity-miller.html#extra-credit-assignment",
    "title": "In class activity",
    "section": "Extra credit assignment",
    "text": "Extra credit assignment\nDesign and carry out a simulation to further explore how bias due to sampling changes as a function of the factor by which respondents who did not request ballots are more likely to be interviewed. Ignore the potential impact of missing values and focus just on the sampling design.\nFix an evenly-spaced grid of values for the talking factor between 1 and 25. For each value, simulate 1000 samples and calculate the estimate of the proportion of fraudulent/erroneous ballot requests for each sample. For each set of 1000 samples, store: (1) the average estimate; (2) the standard deviation of estimates. Plot the estimated bias (average estimate - true proportion) as a function of talking factor, and add uncertainty bands at \\(\\pm 2SD\\). Repeat the entire procedure for overall response rates of \\(10\\%\\), \\(20\\%\\), and \\(30\\%\\).\nPrepare and submit a notebook detailing the simulation study and briefly explaining the results."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Course syllabus",
    "section": "",
    "text": "Instructor: Laura Baracaldo\nTeaching assistants: Lauren Hughes, Zimo Zhu, and Christopher Muzquiz\nClass meetings: MW 9:30am - 10:45am BUCHN1920.\nSection meetings:\n\nR 8:00am - 8:50am, ILP 3103 (Lauren)\nR 9:00am - 9:50am, ILP 3205 (Zimo)\nR 10:00am - 10:50am, GIRV 2127 (Zimo)\nR 11:00am - 11:50am, NH 1109 (Lauren)\nR 12:00am - 12:50am, NH 1109 (Christopher)\n\nOffice hours:\nLaura Baracaldo: Old Gym 1201. W 3:30pm - 4:30pm, or by appointment\nLauren Hughes: TBA or by appointment\nZimo Zhu: TBA or by appointment\nChristopher Muzquiz: TBA or by appointment"
  },
  {
    "objectID": "about.html#staff-and-class-meetings",
    "href": "about.html#staff-and-class-meetings",
    "title": "Course syllabus",
    "section": "",
    "text": "Instructor: Laura Baracaldo\nTeaching assistants: Lauren Hughes, Zimo Zhu, and Christopher Muzquiz\nClass meetings: MW 9:30am - 10:45am BUCHN1920.\nSection meetings:\n\nR 8:00am - 8:50am, ILP 3103 (Lauren)\nR 9:00am - 9:50am, ILP 3205 (Zimo)\nR 10:00am - 10:50am, GIRV 2127 (Zimo)\nR 11:00am - 11:50am, NH 1109 (Lauren)\nR 12:00am - 12:50am, NH 1109 (Christopher)\n\nOffice hours:\nLaura Baracaldo: Old Gym 1201. W 3:30pm - 4:30pm, or by appointment\nLauren Hughes: TBA or by appointment\nZimo Zhu: TBA or by appointment\nChristopher Muzquiz: TBA or by appointment"
  },
  {
    "objectID": "about.html#content-and-materials",
    "href": "about.html#content-and-materials",
    "title": "Course syllabus",
    "section": "Content and materials",
    "text": "Content and materials\nData Science Concepts and Analysis (PSTAT100) is a hands-on introduction to data science intended for intermediate-level students from any discipline with some exposure to probability and basic computing skills, but few or no upper-division courses in statistics or computer science. The course introduces central concepts in statistics – such as sampling variation, uncertainty, and inference – in an applied setting together with techniques for data exploration and analysis. Applications emphasize end-to-end data analyses. Course activities model standard data science workflow practices by example, and successful students acquire programming skills, project management skills, and subject exposure that will serve them well in upper-division courses as well as in independent research or projects.\n\nCatalog description\nOverview of data science key concepts and the use of tools for data retrieval, analysis, visualization, and reproducible research. Topics include an introduction to inference and prediction, principles of measurement, missing data, and notions of causality, statistical “traps”, and concepts in data ethics and privacy. Case studies will illustrate the importance of domain knowledge. Credit units: 4.\n\n\nPrerequisites\n\nProbability and Statistics I (PSTAT 120A)\nLinear Algebra (MATH 4A)\nPrior experience with Python or another programming language (CMPSC 9 or CMPSC 16).\n\n\n\nLearning outcomes\nSuccessful students will establish foundational data science skills:\n\ncritical assessment of data quality and sampling design\nretrieval, inspection, and cleaning of raw data\nexploratory, descriptive, visual, and inferential techniques\ninterpretation and communication of results in context\n\nThese skills will be discussed in depth during course lectures; students will practice them through lab activities, homework assignments, and project work.\n\n\nAssessments\nAttainment of course learning outcomes will be measured by assessment of submitted work. Submitted work falls into four categories:\n\nLabs (not graded) will be assigned weekly in most weeks. These are structured coding assignments with small exercises throughout that introduce the programming skills needed to complete homework assignments.\nHomeworks will be assigned biweekly. These are fairly involved assignments which apply concepts and techniques from the lectures and programming skills from the labs to real data sets in order to reproduce an analysis and answer substantive questions. Collaboration is encouraged, but students must write up and submit their own work individually.\nMini projects will be assigned biweekly in alternation with homeworks. These assignments prompt students to use skills from the course in an unstructured setting to answer high-level questions pertaining to one or more datasets. Mini projects should be completed collaboratively.\nA course project will be assigned requiring students to carry out an open-ended data analysis. This will be completed in teams.\n\nOverall scores in the course will be calculated for each student as the weighted average of scores on all submitted work; the relative weighting and letter grade assignments will depend entirely on the score distribution of the class as a whole and as such reflect each student’s performance relative to their peers.\n\n\nSchedule\nThe tentative topic and assignment schedule is given below. Assignments are indicated by due date: all assignments are due by Monday 11:59pm in the week indicated. Late submissions are allowed, with a possible penalty, for up to 24 hours.\nThe schedule is subject to change based on the progress of the class.\n\n\n\nWeek\nTopic\nLab\nHomework\nProject\n\n\n\n\n1\nData science life cycle\n\n\n\n\n\n2\nTidy data\nL0\n\n\n\n\n3\nSampling and bias\nL1\n\n\n\n\n4\nStatistical graphics\nL2\nH1\n\n\n\n5\nKernel density estimation\nL3\n\nMP1\n\n\n6\nPrincipal components\nL4\nH2\n\n\n\n7\nSimple regression\n\n\n\n\n\n8\nMultiple regression\nL5\nH3\n\n\n\n9\nClassification and clustering\nL6\n\nMP2\n\n\n10\nCase study\nL7\nH4\n\n\n\n11\nFinals\n\n\nCP\n\n\n\n\nL: lab\nH: homework\nMP: mini project\nCP: course project\n\n\n\nMaterials\nThe course website will link to all course content and resources. Readings for the course will draw on multiple sources, including:\n\nPython Data Science Handbook (PDSH);\nLearning Data Science (LDS);\ncollected articles distributed as assigned.\n\n\n\nSoftware\nComputing will be hosted via the course LSIT server pstat100.lsit.ucsb.edu. Students need only a web browser and stable internet connection to complete all course work. It is strongly recommended that students download backup copies of their assignments from the LSIT server.\nInterested students are encouraged to install the software needed to open, edit, and execute notebooks on their own machine, in particular:\n\na Python install;\n(recommended) Miniconda\nJupyter;\npackages utilized in course materials (primarily numpy, pandas, altair, and scikit-learn).\n\nManaging package installations will require some (straightforward) use of the package installer pip or pip3 in the terminal to retrieve/install packages from the Python Package Index repository. Documentation for specific packages (or a Google search) will indicate the appropriate pip command."
  },
  {
    "objectID": "about.html#policies",
    "href": "about.html#policies",
    "title": "Course syllabus",
    "section": "Policies",
    "text": "Policies\n\nCommunication\nThere are two primary means of communication outside of scheduled class meetings: office hours and a discussion board.\nCourse staff have limited availability via email. Course staff will make every effort to respond to individual communication within 48 weekday-hours on the following (or similar) matters:\n\naccommodations/extensions due to personal circumstances;\nlogistical issues such as access to materials or missing scores;\ngeneral advising.\n\nEmail should not be used to ask content questions or submit assignments (unless specifically requested). Emails related to the following (or similar) matters may not receive replies and should be redirected:\n\n\n\nTopic\nRedirect to…\n\n\n\n\nTroubleshooting codes\nDiscussion board\n\n\nChecking answers\nOffice hours or discussion board\n\n\nClarifying assignment content\nOffice hours or discussion board\n\n\nAssignment submission\nGradescope\n\n\nRe-evaluation request\nGradescope\n\n\n\n\n\nExpected time commitment\nThe course is 4 credit units; each credit unit corresponds to an approximate time commitment of 3 hours. So, students should expect to allocate 12 hours per week to the course on average. Course staff are available to help any students spending considerably more time on the class balance the workload.\n\n\nScores and grades\nScores on submitted work can be monitored on Gradescope to ensure fair assignment of course grades. On any individual assignment, re-evaluation can be requested within one week of receiving a score. Requests for re-evaluation made beyond one week after publication of scores may or may not be considered on a discretionary basis.\nDetermination of letter grade assignments is made entirely at the discretion of the instructor based on the assessments outlined above and consistent with university policy. Students are not permitted to negotiate their grades, and are discouraged from requesting audits, recalculations, or verification of self-calculations after the course has concluded. The instructor is under no obligation to share the details of grade calculations with students or to respond to such requests.\nIf at the end of the course a student believes their grade was unfairly assigned, either due to discrimination or without basis in coursework, they are entitled to contest it according to the procedure outlined here.\n\n\nConduct\nStudents are expected to uphold the student code of conduct and to maintain integrity. All individually-submitted work must be an honest reflection of individual effort. Evidence of dishonest conduct will be discussed with the student(s) involved and reported to the Office of Student Conduct (OSC). Depending on the nature of the evidence and the violation, penalty in the course may range from a warning to loss of credit to automatic failure. For a definition and examples of dishonesty, a discussion of what constitutes an appropriate response from faculty, and an explanation of the reporting and investigation process, see the OSC page on academic integrity.\n\n\nDeadlines and late work\nThere is a one-hour grace period on all submission deadlines. After that, work may be submitted within 48 hours of the original deadline (not the deadline plus grace period) and will be considered late. Late submissions will be evaluated for 75% credit.\n\n\nAccommodations\nReasonable accommodations will be made for any student with a qualifying disability. Such requests should be made through the Disabled Students Program (DSP). More information, instructions on how to access accommodations, and information on related resources can be found on the DSP website.\n\n\nFeedback\nToward the end of the term students will be given an opportunity to provide feedback about the course via ESCI. This feedback is valuable for improvement of the course in future terms, and students are strongly encouraged to provide thoughtful course evaluations. The identities of student respondents to ESCI surveys are not disclosed to instructors."
  },
  {
    "objectID": "slides/week6-covariation.html#this-week",
    "href": "slides/week6-covariation.html#this-week",
    "title": "Covariance and correlation",
    "section": "This week",
    "text": "This week\n\nCovariance and correlation\n\nDefinitions\nCovariance and correlation matrices\n\nEigendecomposition\n\nThe eigenvalue problem\nGeometric interpretation\nComputations\n\nPrincipal components analysis\n\nPCA in the low-dimensional setting\nVariation capture and loss\nInterpreting principal components\nDimension reduction"
  },
  {
    "objectID": "slides/week6-covariation.html#city-sustainability-data",
    "href": "slides/week6-covariation.html#city-sustainability-data",
    "title": "Covariance and correlation",
    "section": "City sustainability data",
    "text": "City sustainability data\nWe’ll use the dataset on the sustainability of U.S. cities introduced last time:\n\n\n\n\n\n\n\n\n\nGEOID_MSA\nName\nEcon_Domain\nSocial_Domain\nEnv_Domain\nSustain_Index\n\n\n\n\n0\n310M300US10100\nAberdeen, SD Micro Area\n0.565264\n0.591259\n0.444472\n1.600995\n\n\n1\n310M300US10140\nAberdeen, WA Micro Area\n0.427671\n0.520744\n0.429274\n1.377689\n\n\n\n\n\n\n\nFor each Metropolitan Statistical Area (MSA), a sustainability index is calculated based on economic, social, and environmental indicators (also indices):\n\\[\\text{sustainability index} = \\text{economic} + \\text{social} + \\text{environmental}\\]"
  },
  {
    "objectID": "slides/week6-covariation.html#about-the-data",
    "href": "slides/week6-covariation.html#about-the-data",
    "title": "Covariance and correlation",
    "section": "About the data",
    "text": "About the data\nThe domain indices are computed from a large number of development indicator variables.\nIf you’re interested, you can dig deeper on the Sustainable Development Report website, which provides detailed data reports related to the U.N.’s 2030 sustainable development goals."
  },
  {
    "objectID": "slides/week6-covariation.html#what-is-covariation",
    "href": "slides/week6-covariation.html#what-is-covariation",
    "title": "Covariance and correlation",
    "section": "What is covariation?",
    "text": "What is covariation?\nCovariation refers to the tendency of two variables to change together across observations. Covariation is about relationships.\n\n\n\nCode\n# scatterplot of social vs economic indices\necon_social = alt.Chart(city_sust).mark_point().encode(\n    x = alt.X('Econ_Domain', scale = alt.Scale(zero = False)),\n    y = alt.Y('Social_Domain', scale = alt.Scale(zero = False))\n).properties(\n    width = 350,\n    height = 200\n)\n\necon_social.configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)\n\n\n\n\n\n\n\n\n\n\nThe social and economic indices do seem to vary together: higher values of the economic index coincide with higher values of the social index. That’s all there is to it."
  },
  {
    "objectID": "slides/week6-covariation.html#how-is-covariation-measured",
    "href": "slides/week6-covariation.html#how-is-covariation-measured",
    "title": "Covariance and correlation",
    "section": "How is covariation measured?",
    "text": "How is covariation measured?\nLet \\((x_1, y_1) \\dots, (x_n, y_n)\\) denote \\(n\\) values of two variables, \\(X\\) and \\(Y\\).\n\nIf \\(X\\) and \\(Y\\) tend to vary together, then whenever \\(X\\) is far from its mean, so is \\(Y\\): in other words, their deviations coincide.\n\n\nThis coincidence (or lack thereof) is measured quantiatively by the (sample) covariance:\n\\[\n\\text{cov}(\\mathbf{x}, \\mathbf{y}) = \\frac{1}{n - 1}\\sum_{i = 1}^n (x_i - \\bar{x})(y_i - \\bar{y})\n\\]\n\n\nNote \\(\\text{cov}(\\mathbf{x}, \\mathbf{x}) = \\text{var}(\\mathbf{x})\\)."
  },
  {
    "objectID": "slides/week6-covariation.html#as-an-inner-product",
    "href": "slides/week6-covariation.html#as-an-inner-product",
    "title": "Covariance and correlation",
    "section": "As an inner product",
    "text": "As an inner product\nThe sum can be written as an inner product. First, ‘center’ \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\):\n\\[\n\\tilde{\\mathbf{x}} = \\left[\\begin{array}{c}\n    x_1 - \\bar{x} \\\\\n    \\vdots \\\\\n    x_n - \\bar{x}\n    \\end{array}\\right]\n\\quad\n\\tilde{\\mathbf{y}} = \\left[\\begin{array}{c}\n    y_1 - \\bar{y} \\\\\n    \\vdots \\\\\n    y_n - \\bar{y}\n    \\end{array}\\right]\n\\]\n\nThen, the sample covariance is:\n\\[\n\\text{cov}(\\mathbf{x}, \\mathbf{y})\n= \\frac{\\tilde{\\mathbf{x}}^T \\tilde{\\mathbf{y}}}{n - 1}\n\\]\n\n\n\n# x = econ, y = social\nxy = city_sust.iloc[:, 2:4].values\n\n# center the observations\nxy_centered = xy - xy.mean(axis = 0)\n\n# compute covariancne\nxy_cov = np.inner(xy_centered[:, 0], xy_centered[:, 1])/(len(xy) - 1)\nxy_cov\n\n0.001985023163431384"
  },
  {
    "objectID": "slides/week6-covariation.html#correlation-standardized-covariance",
    "href": "slides/week6-covariation.html#correlation-standardized-covariance",
    "title": "Covariance and correlation",
    "section": "Correlation: standardized covariance",
    "text": "Correlation: standardized covariance\nCovariance is a little tricky to interpret. Is 0.00199 large or small?\n\nIt is more useful to compute the (sample) correlation:\n\\[\n\\text{corr}(\\mathbf{x}, \\mathbf{y}) = \\frac{\\text{cov}(\\mathbf{x}, \\mathbf{y})}{S_x S_y}\n\\]\n\n\nThis is simply a standardized covariance measure.\n\n\\(\\text{corr}(\\mathbf{x}, \\mathbf{y}) = 1, -1\\) are the strongest possible correlations\n\\(\\text{corr}(\\mathbf{x}, \\mathbf{y}) = 0\\) is the weakest possible correlation\nthe sign indicates whether \\(X\\) and \\(Y\\) vary together or in opposition\n\\(\\text{corr}(\\mathbf{x}, \\mathbf{x}) = 1\\), since any variable’s deviations coincide perfectly with themselves"
  },
  {
    "objectID": "slides/week6-covariation.html#correlation-of-social-and-economic-indices",
    "href": "slides/week6-covariation.html#correlation-of-social-and-economic-indices",
    "title": "Covariance and correlation",
    "section": "Correlation of social and economic indices",
    "text": "Correlation of social and economic indices\nStandardizing the covariance makes it more interpretable:\n\n# cov(x, y)/(sx*sy)\nxy_cov/(xy.std(axis = 0).prod()) \n\n0.5320609060898915\n\n\nThe correlation indicates that the social and economic indices vary together (positive) moderately (halfway from zero to one).\n\nThis is just a number that quantifies what you already knew from the graphic: there is a positive relationship."
  },
  {
    "objectID": "slides/week6-covariation.html#aside-other-correlations",
    "href": "slides/week6-covariation.html#aside-other-correlations",
    "title": "Covariance and correlation",
    "section": "Aside: other correlations",
    "text": "Aside: other correlations\nWhat we will call ‘correlation’ in this class is known as the Pearson correlation coefficient.\n\nThere are other correlation measures:\n\nSpearman correlation: Pearson correlation between ranks of observations\nKendall rank correlation\nDistribution-specific dependence measures (e.g., circular data)"
  },
  {
    "objectID": "slides/week6-covariation.html#common-micsonceptions",
    "href": "slides/week6-covariation.html#common-micsonceptions",
    "title": "Covariance and correlation",
    "section": "Common micsonceptions",
    "text": "Common micsonceptions\nNo correlation does not imply no relationship – symmetry can produce strongly related but uncorrelated data.\n\n\n\n\nCode\nnp.random.seed(50323)\n# simulate observations of x\nn = 100\nx = np.random.uniform(low = 0, high = 1, size = n)\nsim_df = pd.DataFrame({'x': x})\n\n# center x, center y, scale\na, b, c = 0.5, 0.5, 3\n\n# noise\nnoise_sd = 0.1\nnoise = np.random.normal(loc = 0, scale = noise_sd, size = n)\n\n# simulate observations of y\nsim_df['y'] = c*(x - a)*(x - b) + noise\n\n# plot\nscatter = alt.Chart(\n    sim_df\n).mark_point().encode(\n    x = 'x',\n    y = 'y'\n)\n\n# compute correlation\nprint('correlation: ', sim_df.corr().loc['x', 'y'])\nscatter.configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)\n\n\ncorrelation:  0.07227477481863409\n\n\n\n\n\n\n\n\n\n\nCode\nnp.random.seed(50323)\n# simulate observations of x\nn = 100\nx = np.random.uniform(low = 0, high = 1, size = n)\nsim_df = pd.DataFrame({'x': x})\n\n# center x, center y, scale\na, b, c = 0.5, 0.5, 3\n\n# noise\nnoise_sd = 0.2\nnoise = np.random.normal(loc = 0, scale = noise_sd, size = n)\n\n# simulate observations of y\nsim_df['y'] = np.cos(4*np.pi*x) + noise\n\n# plot\nscatter = alt.Chart(\n    sim_df\n).mark_point().encode(\n    x = 'x',\n    y = 'y'\n)\n\n# compute correlation\nprint('correlation: ', sim_df.corr().loc['x', 'y'])\nscatter.configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)\n\n\ncorrelation:  0.044829620336962715"
  },
  {
    "objectID": "slides/week6-covariation.html#linearity",
    "href": "slides/week6-covariation.html#linearity",
    "title": "Covariance and correlation",
    "section": "Linearity",
    "text": "Linearity\nCorrelation measures the strength of linear relationships. But what does “strength” mean exactly?\n\n\\(\\text{cor}(\\mathbf{x}, \\mathbf{y}) = 1\\) implies the data lie exactly on a line with positive slope\n\\(\\text{cor}(\\mathbf{x}, \\mathbf{y}) = -1\\) implies the data lie exactly on a line with negative slope\n\\(\\text{cor}(\\mathbf{x}, \\mathbf{y}) = 0\\) implies that the best linear fit to the data is a horizontal line\nvalues near \\(1\\) or \\(-1\\) imply the data lie near a line with nonzero slope"
  },
  {
    "objectID": "slides/week6-covariation.html#more-common-misconceptions",
    "href": "slides/week6-covariation.html#more-common-misconceptions",
    "title": "Covariance and correlation",
    "section": "More common misconceptions",
    "text": "More common misconceptions\nCorrelations are affected by outliers – low correlation does not imply no relationship.\n\n\nCode\nnp.random.seed(50423)\n# intercept, slope\na, b = 1, -1\n\n# noise\nnoise_sd = 0.1\nnoise = np.random.normal(loc = 0, scale = noise_sd, size = n)\n\n# simulate y\nsim_df['y'] = a + b*x + noise\nsim_df.loc[100] = [3, 3]\n\n# plot\nscatter = alt.Chart(\n    sim_df\n).mark_point().encode(\n    x = 'x',\n    y = 'y'\n)\n\n# compute correlation\nprint('correlation: ', sim_df.corr().loc['x', 'y'])\nsim_df = sim_df.loc[0:99].copy()\nscatter.configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)\n\n\ncorrelation:  -0.05770001990055094"
  },
  {
    "objectID": "slides/week6-covariation.html#more-common-misconceptions-1",
    "href": "slides/week6-covariation.html#more-common-misconceptions-1",
    "title": "Covariance and correlation",
    "section": "More common misconceptions",
    "text": "More common misconceptions\nA strong correlation does not imply a meaningful relationship – it could be practically insignificant.\n\n\nCode\nnp.random.seed(50423)\n# intercept, slope\na, b = -0.002, 0.005\n\n# noise\nnoise_sd = 0.001\nnoise = np.random.normal(loc = 0, scale = noise_sd, size = 100)\n\n# simulate y\nsim_df['y'] = a + b*x + noise\n\n# plot\nscatter = alt.Chart(\n    sim_df\n).mark_point().encode(\n    x = alt.X('x', title = 'rate'),\n    y = alt.Y('y', title = 'earnings (USD)')\n)\n\n# compute correlation\nprint('correlation: ', sim_df.corr().loc['x', 'y'])\nscatter.configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)\n\n\ncorrelation:  0.7941451068317529"
  },
  {
    "objectID": "slides/week6-covariation.html#more-common-misconceptions-2",
    "href": "slides/week6-covariation.html#more-common-misconceptions-2",
    "title": "Covariance and correlation",
    "section": "More common misconceptions",
    "text": "More common misconceptions\nA weak correlation does not imply no linear relationship – it could just be really noisy.\n\n\nCode\nnp.random.seed(50423)\n# intercept, slope\na, b = 1, -3\n\n# noise\nnoise_sd = 4\nnoise = np.random.normal(loc = 0, scale = noise_sd, size = 100)\n\n# simulate y\nsim_df['y'] = a + b*x + noise\n\n# plot\nscatter = alt.Chart(\n    sim_df\n).mark_point().encode(\n    x = 'x',\n    y = 'y'\n)\n\ntrend = scatter.transform_regression('x', 'y').mark_line()\n\n# compute correlation\nprint('correlation: ', sim_df.corr().loc['x', 'y'])\n(scatter + trend).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)\n\n\ncorrelation:  -0.25527538975904007"
  },
  {
    "objectID": "slides/week6-covariation.html#why-measure-at-all",
    "href": "slides/week6-covariation.html#why-measure-at-all",
    "title": "Covariance and correlation",
    "section": "Why measure at all?",
    "text": "Why measure at all?\nIt helps to have a number to quantify the strength of a relationship.\n\nFor instance, which pair is most related? Are some pairs more related than others?\n\n\nCode\n# extract social and economic indices\nx_mx = city_sust.iloc[:, 2:5]\n\n# long form dataframe for plotting panel\nscatter_df = x_mx.melt(\n    var_name = 'row',\n    value_name = 'row_index'\n).join(\n    pd.concat([x_mx, x_mx, x_mx], axis = 0).reset_index(),\n).drop(\n    columns = 'index'\n).melt(\n    id_vars = ['row', 'row_index'],\n    var_name = 'col',\n    value_name = 'col_index'\n)\n\n# panel\nalt.Chart(scatter_df).mark_point(opacity = 0.4).encode(\n    x = alt.X('row_index', scale = alt.Scale(zero = False), title = ''),\n    y = alt.Y('col_index', scale = alt.Scale(zero = False), title = '')\n).properties(\n    width = 150, \n    height = 75\n).facet(\n    column = alt.Column('col', title = ''),\n    row = alt.Row('row', title = '')\n).resolve_scale(x = 'independent', y = 'independent')\n\n\n\n\n\n\n\n\n\nCorrelations give us an exact and concise answer, despite their imperfection."
  },
  {
    "objectID": "slides/week6-covariation.html#correlation-matrix",
    "href": "slides/week6-covariation.html#correlation-matrix",
    "title": "Covariance and correlation",
    "section": "Correlation matrix",
    "text": "Correlation matrix\nThe pairwise correlations among all three variables can be represented in a simple square matrix:\n\n# extract social and economic indices\nx_mx = city_sust.iloc[:, 2:5]\n\n# compute matrix of correlations\nx_mx.corr()\n\n\n\n\n\n\n\n\nEcon_Domain\nSocial_Domain\nEnv_Domain\n\n\n\n\nEcon_Domain\n1.000000\n0.531491\n0.011206\n\n\nSocial_Domain\n0.531491\n1.000000\n-0.138674\n\n\nEnv_Domain\n0.011206\n-0.138674\n1.000000\n\n\n\n\n\n\n\n\nThe strongest linear relationship is between social and economic indices; the weakest is between environmental and economic indices."
  },
  {
    "objectID": "slides/week6-covariation.html#covariance-matrix",
    "href": "slides/week6-covariation.html#covariance-matrix",
    "title": "Covariance and correlation",
    "section": "Covariance matrix",
    "text": "Covariance matrix\nLet \\(\\mathbf{X}\\) denote \\(n\\) observations of \\(p\\) variables: \\(\\mathbf{X} = \\left[\\begin{array}{cccc} \\mathbf{x}_{1} &\\mathbf{x}_{2} &\\cdots &\\mathbf{x}_{p} \\end{array}\\right]\\)\n\nThe (sample) covariance matrix is: \\[\n\\text{cov}(\\mathbf{X}) = \\left[\\begin{array}{cccc}\n    \\text{cov}(\\mathbf{x}_1, \\mathbf{x}_1)\n        &\\text{cov}(\\mathbf{x}_1, \\mathbf{x}_2)\n        &\\cdots\n        &\\text{cov}(\\mathbf{x}_1, \\mathbf{x}_p) \\\\\n    \\text{cov}(\\mathbf{x}_2, \\mathbf{x}_1)\n        &\\text{cov}(\\mathbf{x}_2, \\mathbf{x}_2)\n        &\\cdots\n        &\\text{cov}(\\mathbf{x}_2, \\mathbf{x}_p) \\\\\n    \\vdots &\\vdots &\\ddots &\\vdots \\\\\n    \\text{cov}(\\mathbf{x}_p, \\mathbf{x}_1)\n        &\\text{cov}(\\mathbf{x}_p, \\mathbf{x}_2)\n        &\\cdots\n        &\\text{cov}(\\mathbf{x}_p, \\mathbf{x}_p) \\\\\n    \\end{array}\\right]\n\\]\n\n\nIt is easy to calculate as a matrix product after centering the data:\n\\[\n\\text{cov}(\\mathbf{X})\n= \\frac{(\\mathbf{X} - \\bar{\\mathbf{X}})^T(\\mathbf{X} - \\bar{\\mathbf{X}})}{n - 1}\n\\]\n\n\nHere \\(\\bar{\\mathbf{X}}\\) is a matrix whose rows are \\(n\\) copies of the column means of \\(\\mathbf{X}\\)."
  },
  {
    "objectID": "slides/week6-covariation.html#correlation-matrix-1",
    "href": "slides/week6-covariation.html#correlation-matrix-1",
    "title": "Covariance and correlation",
    "section": "Correlation matrix",
    "text": "Correlation matrix\nThe (sample) correlation matrix is, similarly, the matrix of pariwise correlations:\n\\[\n\\text{corr}(\\mathbf{X}) = \\left[\\begin{array}{cccc}\n    \\text{corr}(\\mathbf{x}_1, \\mathbf{x}_1)\n        &\\text{corr}(\\mathbf{x}_1, \\mathbf{x}_2)\n        &\\cdots\n        &\\text{corr}(\\mathbf{x}_1, \\mathbf{x}_p) \\\\\n    \\text{corr}(\\mathbf{x}_2, \\mathbf{x}_1)\n        &\\text{corr}(\\mathbf{x}_2, \\mathbf{x}_2)\n        &\\cdots\n        &\\text{corr}(\\mathbf{x}_2, \\mathbf{x}_p) \\\\\n    \\vdots &\\vdots &\\ddots &\\vdots \\\\\n    \\text{corr}(\\mathbf{x}_p, \\mathbf{x}_1)\n        &\\text{corr}(\\mathbf{x}_p, \\mathbf{x}_2)\n        &\\cdots\n        &\\text{corr}(\\mathbf{x}_p, \\mathbf{x}_p) \\\\\n    \\end{array}\\right]\n\\]\n\nAnd can be obtained by standardizing the covariance matrix:\n\\[\n\\text{corr}(\\mathbf{X}) = (\\text{diag}(\\text{cov}(\\mathbf{X})))^{-1/2} \\left[\\text{cov}(\\mathbf{X})\\right] (\\text{diag}(\\text{cov}(\\mathbf{X})))^{-1/2}\n\\]"
  },
  {
    "objectID": "slides/week6-covariation.html#correlation-matrix-2",
    "href": "slides/week6-covariation.html#correlation-matrix-2",
    "title": "Covariance and correlation",
    "section": "Correlation matrix",
    "text": "Correlation matrix\nFor another perspective, the correlation matrix can be seen as the covariance after normalizing. Consider: \\[\n\\mathbf{Z}\n= \\left\\{\\frac{x_{ij} - \\bar{x}_j}{s_{x_j}}\\right\\}\n= \\left[\\begin{array}{ccc}\n    \\frac{x_{11} - \\bar{x}_1}{s_{x_1}} &\\cdots &\\frac{x_{1p} - \\bar{x}_p}{s_{x_p}} \\\\\n    \\frac{x_{21} - \\bar{x}_1}{s_{x_1}} &\\cdots &\\frac{x_{2p} - \\bar{x}_p}{s_{x_p}} \\\\\n    \\vdots &\\ddots &\\vdots\\\\\n    \\frac{x_{n1} - \\bar{x}_1}{s_{x_1}} &\\cdots &\\frac{x_{np} - \\bar{x}_p}{s_{x_p}} \\\\\n    \\end{array}\\right]\n\\]\n\nThe (sample) correlation matrix is then:\n\\[\n\\text{corr}(\\mathbf{X}) = \\text{cov}(\\mathbf{Z}) = \\underbrace{\\frac{\\mathbf{Z}'\\mathbf{Z}}{n - 1}}_{\\text{call this } \\mathbf{R}}\n\\]"
  },
  {
    "objectID": "slides/week6-covariation.html#calculations",
    "href": "slides/week6-covariation.html#calculations",
    "title": "Covariance and correlation",
    "section": "Calculations",
    "text": "Calculations\n\n\n# correlation matrix 'by hand'\nn = len(x_mx) # sample size\nz_mx = (x_mx - x_mx.mean())/x_mx.std() # (xi - xbar)/sx\nz_mx.transpose().dot(z_mx)/(n - 1) # Z'Z/(n - 1)\n\n\n\n\n\n\n\n\nEcon_Domain\nSocial_Domain\nEnv_Domain\n\n\n\n\nEcon_Domain\n1.000000\n0.531491\n0.011206\n\n\nSocial_Domain\n0.531491\n1.000000\n-0.138674\n\n\nEnv_Domain\n0.011206\n-0.138674\n1.000000\n\n\n\n\n\n\n\n\n\nLuckily, df.cov() and df.cor() will do all the work for you:\n\nx_mx.corr()\n\n\n\n\n\n\n\n\nEcon_Domain\nSocial_Domain\nEnv_Domain\n\n\n\n\nEcon_Domain\n1.000000\n0.531491\n0.011206\n\n\nSocial_Domain\n0.531491\n1.000000\n-0.138674\n\n\nEnv_Domain\n0.011206\n-0.138674\n1.000000\n\n\n\n\n\n\n\n\n\nHowever, you should understand and be able to verify the calculations."
  },
  {
    "objectID": "slides/week6-covariation.html#heatmap-visualization",
    "href": "slides/week6-covariation.html#heatmap-visualization",
    "title": "Covariance and correlation",
    "section": "Heatmap visualization",
    "text": "Heatmap visualization\n\n\nCode\n# store correlation matrix\ncorr_mx = x_mx.corr()\n\n# melt to long form\ncorr_mx_long = corr_mx.reset_index().rename(\n    columns = {'index': 'row'}\n).melt(\n    id_vars = 'row',\n    var_name = 'col',\n    value_name = 'Correlation'\n)\n\n# visualize\nalt.Chart(corr_mx_long).mark_rect().encode(\n    x = alt.X('col', title = '', sort = {'field': 'Correlation', 'order': 'ascending'}),\n    y = alt.Y('row', title = '', sort = {'field': 'Correlation', 'order': 'ascending'}),\n    color = alt.Color('Correlation', \n                      scale = alt.Scale(scheme = 'blueorange',\n                                        domain = (-1, 1), \n                                        type = 'sqrt'),\n                     legend = alt.Legend(tickCount = 5))\n).properties(width = 200, height = 200)\n\n\n\n\n\n\n\n\nEach cell corresponds to a pair of variables\nCells are colored acccording to the magnitude of correlation between the pair\nRows and columns are sorted in order of correlation strength\nDiverging color scale should always be used!"
  },
  {
    "objectID": "slides/week6-covariation.html#higher-dimensions",
    "href": "slides/week6-covariation.html#higher-dimensions",
    "title": "Covariance and correlation",
    "section": "Higher dimensions",
    "text": "Higher dimensions\nConsider a larger collection of development-related variables measured on countries:\n\n\nCode\nwdi = pd.read_csv('data/wdi-data.csv').iloc[:, 2:].set_index('country')\n\n# store correlation matrix\ncorr_mx = wdi.corr()\n\n# melt to long form\ncorr_mx_long = corr_mx.reset_index().rename(\n    columns = {'index': 'row'}\n).melt(\n    id_vars = 'row',\n    var_name = 'col',\n    value_name = 'Correlation'\n)\n\n# visualize\nalt.Chart(corr_mx_long).mark_rect().encode(\n    x = alt.X('col', title = '', sort = {'field': 'Correlation', 'order': 'ascending'}),\n    y = alt.Y('row', title = '', sort = {'field': 'Correlation', 'order': 'ascending'}),\n    color = alt.Color('Correlation', \n                      scale = alt.Scale(scheme = 'blueorange',\n                                        domain = (-1, 1), \n                                        type = 'sqrt'),\n                     legend = alt.Legend(tickCount = 5))\n).configure_axis(\n    labelFontSize = 14\n).configure_legend(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week6-covariation.html#higher-dimensions-1",
    "href": "slides/week6-covariation.html#higher-dimensions-1",
    "title": "Covariance and correlation",
    "section": "Higher dimensions",
    "text": "Higher dimensions\nFor larger collections of variables, we might wish to:\n\nfind a simplified representation of the correlation structure\nvisualize the data in a lower-dimensional space\n\n\nFactoring the correlation matrix provides a means of doing both. We’ll use eigendecomposition."
  },
  {
    "objectID": "slides/week6-covariation.html#the-eigenvalue-problem",
    "href": "slides/week6-covariation.html#the-eigenvalue-problem",
    "title": "Covariance and correlation",
    "section": "The eigenvalue problem",
    "text": "The eigenvalue problem\nLet \\(\\mathbf{A}\\) be a square \\((n\\times n)\\) matrix.\n\nThe eigenvalue problem refers to finding nonzero \\(\\lambda\\) and \\(\\mathbf{x}\\) that satisfy the equation:\n\\[\n\\mathbf{Ax} = \\lambda\\mathbf{x}\n\\]\n\n\nFor any such solutions:\n\n\\(\\lambda\\) is an eigenvalue of \\(\\mathbf{A}\\);\n\\(\\mathbf{x}\\) is an eigenvector of \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "slides/week6-covariation.html#geometry",
    "href": "slides/week6-covariation.html#geometry",
    "title": "Covariance and correlation",
    "section": "Geometry",
    "text": "Geometry\nFor a simple example, suppose \\(n = 2\\) and \\(\\mathbf{x}\\) is an eigenvector of \\(\\mathbf{A}\\). Then:\n\\[\n\\mathbf{Ax}\n= \\left[\\begin{array}{cc}\n        a_{11} & a_{12} \\\\\n        a_{21} & a_{22}\n      \\end{array}\\right]\n      \\left[\\begin{array}{c}\n        x_1 \\\\ x_2\n      \\end{array}\\right]\n= \\left[\\begin{array}{c}\n        a_{11} x_1 + a_{12} x_2 \\\\\n        a_{21} x_1 + a_{22} x_2\n      \\end{array}\\right]\n= \\left[\\begin{array}{c}\n        \\lambda x_1 \\\\ \\lambda x_2\n        \\end{array}\\right]\n= \\lambda\\mathbf{x}\n\\]\n\n\n\nSo the eigenvalue problem equation says that the linear transformation of \\(\\mathbf{x}\\) by \\(\\mathbf{A}\\) is simply a rescaling of \\(\\mathbf{x}\\) by a factor of \\(\\lambda\\)."
  },
  {
    "objectID": "slides/week6-covariation.html#eigendecomposition",
    "href": "slides/week6-covariation.html#eigendecomposition",
    "title": "Covariance and correlation",
    "section": "Eigendecomposition",
    "text": "Eigendecomposition\nThe eigendecomposition of a square matrix consists in finding the eigenvalues and eigenvectors.\n\nThis is considered a ‘decomposition’ because the eigenvectors \\(\\mathbf{V}\\) and eigenvalues \\(\\Lambda\\) satisfy\n\\[\n\\underbrace{\\mathbf{A}}_{\\text{original matrix}} = \\underbrace{\\mathbf{V}\\Lambda\\mathbf{V}'}_{\\text{eigendecomposition}}\n\\]\nIt’s also known as the ‘spectral decomposition’.\n\n\nSo the original matrix can be reconstructed from the eigenvalues and eigenvectors."
  },
  {
    "objectID": "slides/week6-covariation.html#uniqueness",
    "href": "slides/week6-covariation.html#uniqueness",
    "title": "Covariance and correlation",
    "section": "Uniqueness",
    "text": "Uniqueness\nIf \\(\\lambda, \\mathbf{x}\\) are solutions, then so are \\(c\\lambda, c\\mathbf{x}\\) for any constant \\(c\\).\n\nSo assume that \\(\\|\\mathbf{x}\\| = 1\\).\n\n\nThen solutions are unique to within the sign of \\(\\mathbf{x}\\)."
  },
  {
    "objectID": "slides/week6-covariation.html#special-case",
    "href": "slides/week6-covariation.html#special-case",
    "title": "Covariance and correlation",
    "section": "Special case",
    "text": "Special case\nWe will be applying eigendecomposition to correlation matrices, which have a special form \\(\\mathbf{A} = \\mathbf{Z'Z}\\).\nOne can show that for matrices of this form:\n\n\\(\\mathbf{V'V} = \\mathbf{I}\\), in other words, the eigenvectors are an orthonormal basis\n\ncolumns of \\(\\mathbf{V}\\) are orthogonal\ncolumns of \\(\\mathbf{V}\\) are of unit length\n\n\\(\\lambda_i \\geq 0\\), in other words, all eigenvalues are nonnegative"
  },
  {
    "objectID": "slides/week6-covariation.html#computations",
    "href": "slides/week6-covariation.html#computations",
    "title": "Covariance and correlation",
    "section": "Computations",
    "text": "Computations\nThe eigendecomposition is computed numerically using iterative methods. Luckily, these are very easy to implement:\n\n\n# compute decomposition\ncorr_mx = x_mx.corr()\ndecomp = linalg.eig(corr_mx)\ndecomp\n\n(array([0.44788559+0.j, 1.54664121+0.j, 1.0054732 +0.j]),\n array([[ 0.68276586, -0.68571102,  0.2522522 ],\n        [-0.70523279, -0.7087523 , -0.01780118],\n        [-0.19099079,  0.16574249,  0.96749778]]))\n\n\n\n\n\n# eigenvalues\ndecomp[0]\n\narray([0.44788559+0.j, 1.54664121+0.j, 1.0054732 +0.j])\n\n\n\n\n\n# eigenvectors (columns)\ndecomp[1]\n\narray([[ 0.68276586, -0.68571102,  0.2522522 ],\n       [-0.70523279, -0.7087523 , -0.01780118],\n       [-0.19099079,  0.16574249,  0.96749778]])"
  },
  {
    "objectID": "slides/week6-covariation.html#does-the-decomposition-really-work",
    "href": "slides/week6-covariation.html#does-the-decomposition-really-work",
    "title": "Covariance and correlation",
    "section": "Does the decomposition really work?",
    "text": "Does the decomposition really work?\nLet’s check that in fact \\(\\mathbf{A} = \\mathbf{V\\Lambda V'}\\):\n\n\neigenvecs.dot(eigenvals).dot(eigenvecs.transpose()) # V Lambda V'\n\n\n\n\n\n\n\n\nEcon_Domain\nSocial_Domain\nEnv_Domain\n\n\n\n\nEcon_Domain\n1.000000\n0.531491\n0.011206\n\n\nSocial_Domain\n0.531491\n1.000000\n-0.138674\n\n\nEnv_Domain\n0.011206\n-0.138674\n1.000000\n\n\n\n\n\n\n\n\n\n\ncorr_mx # correlation matrix (our 'A')\n\n\n\n\n\n\n\n\nEcon_Domain\nSocial_Domain\nEnv_Domain\n\n\n\n\nEcon_Domain\n1.000000\n0.531491\n0.011206\n\n\nSocial_Domain\n0.531491\n1.000000\n-0.138674\n\n\nEnv_Domain\n0.011206\n-0.138674\n1.000000"
  },
  {
    "objectID": "slides/week6-covariation.html#orthogonality",
    "href": "slides/week6-covariation.html#orthogonality",
    "title": "Covariance and correlation",
    "section": "Orthogonality",
    "text": "Orthogonality\nLet’s check also that in fact \\(\\mathbf{V'V} = \\mathbf{I}\\):\n\n\neigenvecs.transpose().dot(eigenvecs) # V'V\n\n\n\n\n\n\n\n\nv1\nv2\nv3\n\n\n\n\nv1\n1.000000e+00\n-5.311862e-16\n-7.332327e-17\n\n\nv2\n-5.311862e-16\n1.000000e+00\n6.231243e-17\n\n\nv3\n-7.332327e-17\n6.231243e-17\n1.000000e+00\n\n\n\n\n\n\n\n\n\nThe significance of this property is that \\(\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\) form an orthonormal basis."
  },
  {
    "objectID": "slides/week6-covariation.html#whats-a-basis",
    "href": "slides/week6-covariation.html#whats-a-basis",
    "title": "Covariance and correlation",
    "section": "What’s a basis?",
    "text": "What’s a basis?\nA basis in linear algebra is a set of vectors that span a linear space. Think of a basis as a set of axes.\n\nFor instance, the usual basis for \\(\\mathbb{R}^3\\) are the unit vectors: \\[\n\\mathbf{e}_1 = (1, 0, 0) \\quad \\mathbf{e}_2 = (0, 1, 0) \\quad \\mathbf{e}_3 = (0, 0, 1)\n\\]\nThe usual coordinates in \\(\\mathbb{R}^3\\) in fact refer to multiples of these basis vectors: \\[\n(1, 2, 1) = 1\\mathbf{e}_1 + 2\\mathbf{e}_2 + 1\\mathbf{e}_3\n\\]\n\nA basis is orthogonal if all basis vectors are at right angles to one another\nA basis is orthonormal if it is orthogonal and basis vectors are of unit length"
  },
  {
    "objectID": "slides/week6-covariation.html#nonstandard-bases",
    "href": "slides/week6-covariation.html#nonstandard-bases",
    "title": "Covariance and correlation",
    "section": "Nonstandard bases",
    "text": "Nonstandard bases\n\n\nTo an extent the usual choice of \\(\\mathbf{e}_1, \\mathbf{e}_2, \\mathbf{e}_3\\) is arbitrary.\nIt’s possible to choose any system based on three directions to represent the same point relative to the origin."
  },
  {
    "objectID": "slides/week6-covariation.html#change-of-basis",
    "href": "slides/week6-covariation.html#change-of-basis",
    "title": "Covariance and correlation",
    "section": "Change of basis",
    "text": "Change of basis\nDifferent bases are simply different coordinate systems.\nIt’s akin to the possibility of locating the distance to the corner store by different units and relative to different directions. For example:\n\nhalf a block to the right from the front door; or\nsixty steps diagonally in a straight line; or\nfifteen steps north, fifty steps east."
  },
  {
    "objectID": "slides/week6-covariation.html#basis-from-correlations",
    "href": "slides/week6-covariation.html#basis-from-correlations",
    "title": "Covariance and correlation",
    "section": "Basis from correlations",
    "text": "Basis from correlations\nDecomposing the correlation matrix yields a basis for \\(\\mathbb{R}^p\\) – when standardized data are represented on that basis, observations become uncorrelated.\n\nLet \\(\\mathbf{Y} = \\mathbf{ZV}\\); this is the representation of \\(\\mathbf{Z}\\) on the basis given by \\(\\mathbf{V}\\).\n\n\nThen: \\[\n\\mathbf{Y'Y} = \\mathbf{V'Z'ZV} =\\mathbf{V'V\\Lambda V' V} = \\Lambda\n\\]\n\n\nTogether with the observation that \\(\\bar{\\mathbf{Y}} = 0\\), this implies that \\(\\text{cov}(\\mathbf{y}_j, \\mathbf{y}_k) = 0\\) since \\(\\Lambda\\) is diagonal."
  },
  {
    "objectID": "slides/week6-covariation.html#decorrelating-data",
    "href": "slides/week6-covariation.html#decorrelating-data",
    "title": "Covariance and correlation",
    "section": "Decorrelating data",
    "text": "Decorrelating data\nWe can easily verify this property by computing \\(\\text{cor}(\\mathbf{ZV})\\):\n\nz_rotated = z_mx.dot(eigenvecs)\n\n\nThe columns are now orthogonal:\n\nz_rotated.T.dot(z_rotated)\n\n\n\n\n\n\n\n\nv1\nv2\nv3\n\n\n\n\nv1\n4.174294e+02\n-2.522427e-13\n1.714184e-13\n\n\nv2\n-2.522427e-13\n1.441470e+03\n2.087774e-13\n\n\nv3\n1.714184e-13\n2.087774e-13\n9.371010e+02\n\n\n\n\n\n\n\n\n\nAnd (since each column of \\(\\mathbf{Z}\\) has mean zero) also uncorrelated:\n\nz_rotated.corr()\n\n\n\n\n\n\n\n\nv1\nv2\nv3\n\n\n\n\nv1\n1.000000e+00\n-3.331954e-16\n2.703494e-16\n\n\nv2\n-3.331954e-16\n1.000000e+00\n2.067148e-16\n\n\nv3\n2.703494e-16\n2.067148e-16\n1.000000e+00"
  },
  {
    "objectID": "slides/week6-covariation.html#capturing-variation",
    "href": "slides/week6-covariation.html#capturing-variation",
    "title": "Covariance and correlation",
    "section": "Capturing variation",
    "text": "Capturing variation\nThe eigenbasis from the correlation matrix in a sense ‘captures’ the covariation in the data.\n\nprovides a coordinate system on which the (standardized) data are uncorrelated\ncoordinate axes are the ‘main’ directions of total variability (will demonstrate empirically)\n\n\nThose ‘main directions’ are known as principal components."
  },
  {
    "objectID": "slides/week6-covariation.html#other-decompositions",
    "href": "slides/week6-covariation.html#other-decompositions",
    "title": "Covariance and correlation",
    "section": "Other decompositions",
    "text": "Other decompositions\nThe eigenbasis from the correlation matrix can also be recovered from the singular value decomposition of \\(\\mathbf{Z}\\).\n\n\\[\n\\mathbf{Z} = \\mathbf{UDV'}\n\\quad\\Longrightarrow\\quad\n\\mathbf{Z'Z} = \\mathbf{V}\\underbrace{(\\mathbf{D'U'UD})}_{\\mathbf{\\Lambda}}\\mathbf{V'}\n\\]\n\n\nIn fact, most implementations of principal components use the SVD instead of eigendecomposition."
  },
  {
    "objectID": "slides/week2-tidy.html#announcements",
    "href": "slides/week2-tidy.html#announcements",
    "title": "Tidy data",
    "section": "",
    "text": "Complete Q1-Q4 (fruit_info section) of Lab 1 before section"
  },
  {
    "objectID": "slides/week2-tidy.html#this-week",
    "href": "slides/week2-tidy.html#this-week",
    "title": "Tidy data",
    "section": "This week",
    "text": "This week\n\nTabular data\n\nMany ways to structure a dataset\nFew organizational constraints ‘in the wild’\n\nPrinciples of tidy data: matching semantics with structure\n\nData semantics: observations and variables\nTabular structure: rows and columns\nThe tidy standard\nCommon messes\nTidying operations\n\nTransforming data frames\n\nSubsetting (slicing and filtering)\nDerived variables\nAggregation and summary statistics"
  },
  {
    "objectID": "slides/week2-tidy.html#tabular-data",
    "href": "slides/week2-tidy.html#tabular-data",
    "title": "Tidy data",
    "section": "Tabular data",
    "text": "Tabular data\n\nMany possible layouts for tabular data\n‘Real’ datasets have few organizational constraints\n\n. . .\nMost data are stored in tables, but there are always multiple possible tabular layouts for the same underlying data.\n. . .\nLet’s look at some examples."
  },
  {
    "objectID": "slides/week2-tidy.html#mammal-data-long-layouts",
    "href": "slides/week2-tidy.html#mammal-data-long-layouts",
    "title": "Tidy data",
    "section": "Mammal data: long layouts",
    "text": "Mammal data: long layouts\nBelow is the Allison 1976 mammal brain-body weight dataset from last time shown in two ‘long’ layouts:\n\n\n\n\n\n\n\n\n\nbody_wt\nbrain_wt\n\n\nspecies\n\n\n\n\n\n\nAfricanelephant\n6654.0\n5712.0\n\n\nAfricangiantpouchedrat\n1.0\n6.6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmeasurement\nweight\n\n\nspecies\n\n\n\n\n\n\nAfricanelephant\nbrain_wt\n5712.0\n\n\nAfricanelephant\nbody_wt\n6654.0\n\n\nAfricangiantpouchedrat\nbrain_wt\n6.6\n\n\nAfricangiantpouchedrat\nbody_wt\n1.0"
  },
  {
    "objectID": "slides/week2-tidy.html#mammal-data-wide-layout",
    "href": "slides/week2-tidy.html#mammal-data-wide-layout",
    "title": "Tidy data",
    "section": "Mammal data: wide layout",
    "text": "Mammal data: wide layout\nHere’s a third possible layout for the mammal brain-body weight data:\n\n\n\n\n\n\n\n\nspecies\nAfricanelephant\nAfricangiantpouchedrat\nArcticFox\nArcticgroundsquirrel\n\n\nmeasurement\n\n\n\n\n\n\n\n\nbody_wt\n6654.0\n1.0\n3.385\n0.92\n\n\nbrain_wt\n5712.0\n6.6\n44.500\n5.70"
  },
  {
    "objectID": "slides/week2-tidy.html#gdp-growth-data-wide-layout",
    "href": "slides/week2-tidy.html#gdp-growth-data-wide-layout",
    "title": "Tidy data",
    "section": "GDP growth data: wide layout",
    "text": "GDP growth data: wide layout\nHere’s another example: World Bank data on annual GDP growth for 264 countries from 1961 – 2019.\n\n\n\n\n\n\n\n\n\nCountry Name\nCountry Code\n2009\n2010\n2011\n\n\n\n\n0\nAruba\nABW\n-10.519749\n-3.685029\n3.446055\n\n\n1\nAfghanistan\nAFG\n21.390528\n14.362441\n0.426355\n\n\n2\nAngola\nAGO\n0.858713\n4.403933\n3.471976\n\n\n3\nAlbania\nALB\n3.350067\n3.706892\n2.545322\n\n\n4\nAndorra\nAND\n-5.302847\n-1.974958\n-0.008070"
  },
  {
    "objectID": "slides/week2-tidy.html#gdp-growth-data-long-layout",
    "href": "slides/week2-tidy.html#gdp-growth-data-long-layout",
    "title": "Tidy data",
    "section": "GDP growth data: long layout",
    "text": "GDP growth data: long layout\nHere’s an alternative layout for the annual GDP growth data:\n\n\n\n\n\n\n\n\n\nyear\ngrowth_pct\n\n\nCountry Name\n\n\n\n\n\n\nAfghanistan\n2009\n21.390528\n\n\nAruba\n2009\n-10.519749\n\n\nAfghanistan\n2010\n14.362441\n\n\nAruba\n2010\n-3.685029\n\n\nAfghanistan\n2011\n0.426355\n\n\nAruba\n2011\n3.446055"
  },
  {
    "objectID": "slides/week2-tidy.html#sb-weather-data-long-layouts",
    "href": "slides/week2-tidy.html#sb-weather-data-long-layouts",
    "title": "Tidy data",
    "section": "SB weather data: long layouts",
    "text": "SB weather data: long layouts\nA third example: daily minimum and maximum temperatures recorded at Santa Barbara Municipal Airport from January 2021 through March 2021.\n\n\n\n\n\n\n\n\n\nSTATION\nTMAX\nTMIN\nMONTH\nDAY\nYEAR\n\n\n\n\n0\nUSW00023190\n65\n37\n1\n1\n2021\n\n\n1\nUSW00023190\n62\n38\n1\n2\n2021\n\n\n2\nUSW00023190\n60\n42\n1\n3\n2021"
  },
  {
    "objectID": "slides/week2-tidy.html#sb-weather-data-wide-layout",
    "href": "slides/week2-tidy.html#sb-weather-data-wide-layout",
    "title": "Tidy data",
    "section": "SB weather data: wide layout",
    "text": "SB weather data: wide layout\nHere’s a wide layout for the SB weather data:\n\n\n\n\n\n\n\n\n\nDAY\n1\n2\n3\n4\n\n\nMONTH\ntype\n\n\n\n\n\n\n\n\n1\nTMAX\n65.0\n62.0\n60.0\n72.0\n\n\nTMIN\n37.0\n38.0\n42.0\n43.0\n\n\n2\nTMAX\n66.0\n67.0\n69.0\n63.0\n\n\nTMIN\n45.0\n40.0\n44.0\n37.0\n\n\n3\nTMAX\n68.0\n66.0\n59.0\n62.0"
  },
  {
    "objectID": "slides/week2-tidy.html#un-development-data-multiple-tables",
    "href": "slides/week2-tidy.html#un-development-data-multiple-tables",
    "title": "Tidy data",
    "section": "UN development data: multiple tables",
    "text": "UN development data: multiple tables\nA final example: United Nations country development data organized into different tables according to variable type.\nHere is a table of population measurements:\n\n\n\n\n\n\n\n\n\ntotal_pop\nurban_pct_pop\npop_under5\npop_15to64\npop_over65\n\n\ncountry\n\n\n\n\n\n\n\n\n\nAfghanistan\n38.0\n25.8\n5.6\n20.9\n1.0\n\n\nAlbania\n2.9\n61.2\n0.2\n2.0\n0.4\n\n\n\n\n\n\n\nAnd here is a table of a few gender-related variables:\n\n\n\n\n\n\n\n\n\ngender_inequality\nparliament_pct_women\nlabor_participation_women\nlabor_participation_men\n\n\ncountry\n\n\n\n\n\n\n\n\nNorway\n0.045\n40.8\n60.4\n67.2\n\n\nIreland\n0.093\n24.3\n56.0\n68.4"
  },
  {
    "objectID": "slides/week2-tidy.html#un-development-data-one-table",
    "href": "slides/week2-tidy.html#un-development-data-one-table",
    "title": "Tidy data",
    "section": "UN development data: one table",
    "text": "UN development data: one table\nHere are both tables merged by country:\n\n\n\n\n\n\n\n\n\ntotal_pop\nurban_pct_pop\npop_under5\npop_15to64\npop_over65\ngender_inequality\nparliament_pct_women\nlabor_participation_women\nlabor_participation_men\n\n\ncountry\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfghanistan\n38.0\n25.8\n5.6\n20.9\n1.0\n0.655\n27.2\n21.6\n74.7\n\n\nAlbania\n2.9\n61.2\n0.2\n2.0\n0.4\n0.181\n29.5\n46.7\n64.6\n\n\nAlgeria\n43.1\n73.2\n5.0\n27.1\n2.8\n0.429\n21.5\n14.6\n67.4"
  },
  {
    "objectID": "slides/week2-tidy.html#un-development-data-one-longer-table",
    "href": "slides/week2-tidy.html#un-development-data-one-longer-table",
    "title": "Tidy data",
    "section": "UN development data: one (longer) table",
    "text": "UN development data: one (longer) table\nAnd here is another arrangement of the merged table:\n\n\n\n\n\n\n\n\n\ngender_variable\ngender_value\npopulation_variable\npopulation_value\n\n\ncountry\n\n\n\n\n\n\n\n\nAfghanistan\ngender_inequality\n0.655\ntotal_pop\n38.0\n\n\nAlbania\ngender_inequality\n0.181\ntotal_pop\n2.9\n\n\nAlgeria\ngender_inequality\n0.429\ntotal_pop\n43.1\n\n\nAndorra\ngender_inequality\nNaN\ntotal_pop\n0.1\n\n\nAngola\ngender_inequality\n0.536\ntotal_pop\n31.8"
  },
  {
    "objectID": "slides/week2-tidy.html#what-are-the-differences",
    "href": "slides/week2-tidy.html#what-are-the-differences",
    "title": "Tidy data",
    "section": "What are the differences?",
    "text": "What are the differences?\nIn short, the alternate layouts differ in three respects:\n\nRows\nColumns\nNumber of tables"
  },
  {
    "objectID": "slides/week2-tidy.html#how-to-choose",
    "href": "slides/week2-tidy.html#how-to-choose",
    "title": "Tidy data",
    "section": "How to choose?",
    "text": "How to choose?\nReturn to one of the examples and review the different layouts with your neighbor.\n\nList a few advantages and disadvantages for each layout.\nWhich do you prefer and why?"
  },
  {
    "objectID": "slides/week2-tidy.html#few-organizational-constraints",
    "href": "slides/week2-tidy.html#few-organizational-constraints",
    "title": "Tidy data",
    "section": "Few organizational constraints",
    "text": "Few organizational constraints\nIt’s surprisingly difficult to articulate reasons why one layout might be preferable to another.\n\nUsually the choice of layout isn’t principled\nIdiosyncratic: two people are likely to make different choices\n\n. . .\nAs a result:\n\nFew widely used conventions\nLots of variability ‘in the wild’\nDatasets are often organized in bizarre ways"
  },
  {
    "objectID": "slides/week2-tidy.html#form-and-representation",
    "href": "slides/week2-tidy.html#form-and-representation",
    "title": "Tidy data",
    "section": "Form and representation",
    "text": "Form and representation\nBecause of the wide range of possible layouts for a dataset, and the variety of choices that are made about how to store data, data scientists are constantly faced with determining how best to reorganize datasets in a way that facilitates exploration and analysis.\n. . .\nBroadly, this involves two interdependent choices:\n\nChoice of representation: how to encode information.\n\nExample: parse dates as ‘MM/DD/YYYY’ (one variable) or ‘MM’, ‘DD’, ‘YYYY’ (three variables)?\nExample: use values 1, 2, 3 or ‘low’, ‘med’, ‘high’?\nExample: name variables ‘question1’, ‘question2’, …, or ‘age’, ‘income’, …?\n\nChoice of form: how to display information\n\nExample: wide table or long table?\nExample: one table or many?"
  },
  {
    "objectID": "slides/week2-tidy.html#tidy-data",
    "href": "slides/week2-tidy.html#tidy-data",
    "title": "Tidy data",
    "section": "Tidy data",
    "text": "Tidy data\nThe tidy data standard is a principled way of organizing tabular data. It has two main advantages:\n\nFacilitates workflow by establishing a consistent dataset structure.\nPrinciples are designed to make transformation, exploration, visualization, and modeling easy."
  },
  {
    "objectID": "slides/week2-tidy.html#semantics-and-structure",
    "href": "slides/week2-tidy.html#semantics-and-structure",
    "title": "Tidy data",
    "section": "Semantics and structure",
    "text": "Semantics and structure\n\n“Tidying your data means storing it in a consistent form that matches the semantics of the dataset with the way it is stored.” Wickham and Grolemund, R for Data Science, 2017.\n\n. . .\nA dataset is a collection of values.\n\nthe semantics of a dataset are the meanings of the values\nthe structure of a dataset is the arrangement of the values"
  },
  {
    "objectID": "slides/week2-tidy.html#data-semantics",
    "href": "slides/week2-tidy.html#data-semantics",
    "title": "Tidy data",
    "section": "Data semantics",
    "text": "Data semantics\nTo introduce some general vocabulary, each value in a dataset is\n\nan observation\nof a variable\ntaken on an observational unit."
  },
  {
    "objectID": "slides/week2-tidy.html#units-variables-and-observations",
    "href": "slides/week2-tidy.html#units-variables-and-observations",
    "title": "Tidy data",
    "section": "Units, variables, and observations",
    "text": "Units, variables, and observations\n\n\n\n\n\n\n\n\n\ntotal_pop\nurban_pct_pop\n\n\ncountry\n\n\n\n\n\n\nAfghanistan\n38.0\n25.8\n\n\nAlbania\n2.9\n61.2\n\n\n\n\n\n\n\n\nAn observational unit is the entity measured.\n\nAbove, country\n\nA variable is an attribute measured on each unit.\n\nAbove, total population and urban percentage\n\nAn observation is a collection of measurements taken on one unit.\n\nAbove, 38.0 and 25.8"
  },
  {
    "objectID": "slides/week2-tidy.html#identifying-units-variables-and-observations",
    "href": "slides/week2-tidy.html#identifying-units-variables-and-observations",
    "title": "Tidy data",
    "section": "Identifying units, variables, and observations",
    "text": "Identifying units, variables, and observations\nLet’s do an example. Here’s one record from the GDP growth data:\n. . .\n\n\n\n\n\n\n\n\n\nyear\ngrowth_pct\n\n\nCountry Name\n\n\n\n\n\n\nAfghanistan\n2010\n14.362441\n\n\n\n\n\n\n\n. . .\nAbove, the values -13.605441 and 1961 are observations of the variables GDP growth and year recorded for the observational unit Algeria."
  },
  {
    "objectID": "slides/week2-tidy.html#your-turn",
    "href": "slides/week2-tidy.html#your-turn",
    "title": "Tidy data",
    "section": "Your turn",
    "text": "Your turn\nWhat are the units, variables and observations?\n\n\n\n\n\n\n\n\n\nDAY\n1\n2\n3\n4\n\n\nMONTH\ntype\n\n\n\n\n\n\n\n\n1\nTMAX\n65.0\n62.0\n60.0\n72.0\n\n\nTMIN\n37.0\n38.0\n42.0\n43.0\n\n\n2\nTMAX\n66.0\n67.0\n69.0\n63.0\n\n\nTMIN\n45.0\n40.0\n44.0\n37.0\n\n\n\n\n\n\n\n. . .\nThink about it, then confer with your neighbor."
  },
  {
    "objectID": "slides/week2-tidy.html#data-structure",
    "href": "slides/week2-tidy.html#data-structure",
    "title": "Tidy data",
    "section": "Data structure",
    "text": "Data structure\nData structure refers to the form in which it is stored.\n. . .\nTabular data is arranged in rows and columns.\n. . .\nAs we saw, there are multiple structures – arrangements of rows and columns – available to represent any dataset."
  },
  {
    "objectID": "slides/week2-tidy.html#the-tidy-standard",
    "href": "slides/week2-tidy.html#the-tidy-standard",
    "title": "Tidy data",
    "section": "The tidy standard",
    "text": "The tidy standard\nThe tidy standard consists in matching semantics and structure. A dataset is tidy if:\n\nEach variable is a column.\nEach observation is a row.\nEach table contains measurements on only one type of observational unit.\n\n\n\n\nTidy data."
  },
  {
    "objectID": "slides/week2-tidy.html#tidy-or-messy",
    "href": "slides/week2-tidy.html#tidy-or-messy",
    "title": "Tidy data",
    "section": "Tidy or messy?",
    "text": "Tidy or messy?\nLet’s revisit some of our examples of multiple layouts.\n\n\n\n\n\n\n\n\n\nCountry Name\nCountry Code\n2009\n2010\n2011\n\n\n\n\n0\nAruba\nABW\n-10.519749\n-3.685029\n3.446055\n\n\n1\nAfghanistan\nAFG\n21.390528\n14.362441\n0.426355\n\n\n2\nAngola\nAGO\n0.858713\n4.403933\n3.471976\n\n\n\n\n\n\n\n. . .\nWe can compare the semantics and structure for alignment:\n\n\n\nSemantics\n\nStructure\n\n\n\n\n\nObservations\nAnnual records\nRows\nCountries\n\n\nVariables\nGDP growth and year\nColumns\nValue of year\n\n\nObservational units\nCountries\nTables\nJust one\n\n\n\n. . .\nRules 1 and 2 are violated, since column names are values (of year), not variables. Not tidy."
  },
  {
    "objectID": "slides/week2-tidy.html#tidy-or-messy-1",
    "href": "slides/week2-tidy.html#tidy-or-messy-1",
    "title": "Tidy data",
    "section": "Tidy or messy?",
    "text": "Tidy or messy?\n\n\n\n\n\n\n\n\n\nyear\ngrowth_pct\n\n\nCountry Name\n\n\n\n\n\n\nAfghanistan\n2009\n21.390528\n\n\nAruba\n2009\n-10.519749\n\n\nAfghanistan\n2010\n14.362441\n\n\nAruba\n2010\n-3.685029\n\n\n\n\n\n\n\n. . .\nComparison of semantics and structure:\n\n\n\nSemantics\n\nStructure\n\n\n\n\n\nObservations\nAnnual records\nRows\nAnnual records\n\n\nVariables\nGDP growth and year\nColumns\nGDP growth and year\n\n\nObservational units\nCountries\nTables\nJust one\n\n\n\n. . .\nAll three rules are met: rows are observations, columns are variables, and there’s one unit type and one table. Tidy."
  },
  {
    "objectID": "slides/week2-tidy.html#tidy-or-messy-2",
    "href": "slides/week2-tidy.html#tidy-or-messy-2",
    "title": "Tidy data",
    "section": "Tidy or messy?",
    "text": "Tidy or messy?\n\n\n\n\n\n\n\n\n\nSTATION\nTMAX\nTMIN\nMONTH\nDAY\nYEAR\n\n\n\n\n0\nUSW00023190\n65\n37\n1\n1\n2021\n\n\n1\nUSW00023190\n62\n38\n1\n2\n2021\n\n\n2\nUSW00023190\n60\n42\n1\n3\n2021\n\n\n\n\n\n\n\nTry this one on your own. Then compare with your neighbor.\n\nIdentify the observations and variables\nWhat are the observational units?"
  },
  {
    "objectID": "slides/week2-tidy.html#tidy-or-messy-3",
    "href": "slides/week2-tidy.html#tidy-or-messy-3",
    "title": "Tidy data",
    "section": "Tidy or messy?",
    "text": "Tidy or messy?\nIn undev1 and undev2:\n\n\n\n\n\n\n\n\n\ntotal_pop\nurban_pct_pop\npop_under5\npop_15to64\npop_over65\n\n\ncountry\n\n\n\n\n\n\n\n\n\nAfghanistan\n38.0\n25.8\n5.6\n20.9\n1.0\n\n\nAlbania\n2.9\n61.2\n0.2\n2.0\n0.4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngender_inequality\nparliament_pct_women\nlabor_participation_women\nlabor_participation_men\n\n\ncountry\n\n\n\n\n\n\n\n\nNorway\n0.045\n40.8\n60.4\n67.2\n\n\nIreland\n0.093\n24.3\n56.0\n68.4\n\n\n\n\n\n\n\n. . .\nHere there are multiple tables. To discuss:\n\nAre the observational units the same or different?\nBased on your answer above, is the data tidy or not?"
  },
  {
    "objectID": "slides/week2-tidy.html#common-messes",
    "href": "slides/week2-tidy.html#common-messes",
    "title": "Tidy data",
    "section": "Common messes",
    "text": "Common messes\n\n“Well, here’s another nice mess you’ve gotten me into” – Oliver Hardy\n\nThese examples illustrate some common messes:\n\nColumns are values, not variables\n\nGDP data: columns are 1961, 1962, …\n\nMultiple variables are stored in one column\n\nMammal data: weight column contains both body and brain weights\n\nVariables or values are stored in rows and columns\n\nWeather data: date values are stored in rows and columns, each column contains both min and max temperatures\n\nMeasurements on one type of observational unit are divided into multiple tables.\n\nUN development data: one table for population statistics and a separate table for gender statistics."
  },
  {
    "objectID": "slides/week2-tidy.html#tidying-operations",
    "href": "slides/week2-tidy.html#tidying-operations",
    "title": "Tidy data",
    "section": "Tidying operations",
    "text": "Tidying operations\nThese common messes can be cleaned up by some simple operations:\n\nmelt\n\nreshape a dataframe from wide to long format\n\npivot\n\nreshape a dataframe from long to wide format\n\nmerge\n\ncombine two dataframes row-wise by matching the values of certain columns"
  },
  {
    "objectID": "slides/week2-tidy.html#melt",
    "href": "slides/week2-tidy.html#melt",
    "title": "Tidy data",
    "section": "Melt",
    "text": "Melt\nMelting resolves the problem of having values stored as columns (common mess 1)."
  },
  {
    "objectID": "slides/week2-tidy.html#melt-1",
    "href": "slides/week2-tidy.html#melt-1",
    "title": "Tidy data",
    "section": "Melt",
    "text": "Melt\n\n\n\n\n\n\n\n\n\nCountry Name\nCountry Code\n1961\n1962\n1963\n1964\n1965\n1966\n1967\n1968\n...\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n\n\n\n\n0\nAruba\nABW\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n-3.685029\n3.446055\n-1.369863\n4.198232\n0.300000\n5.700001\n2.100000\n1.999999\nNaN\nNaN\n\n\n1\nAfghanistan\nAFG\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n14.362441\n0.426355\n12.752287\n5.600745\n2.724543\n1.451315\n2.260314\n2.647003\n1.189228\n3.911603\n\n\n\n\n2 rows × 61 columns\n\n\n\n\n# in pandas\ngdp1.melt(\n    id_vars = ['Country Name', 'Country Code'], # which variables do you want to retain for each row? .\n    var_name = 'Year', # what do you want to name the variable that will contain the column names?\n    value_name = 'GDP Growth', # what do you want to name the variable that will contain the values?\n).head(2)\n\n\n\n\n\n\n\n\nCountry Name\nCountry Code\nYear\nGDP Growth\n\n\n\n\n0\nAruba\nABW\n1961\nNaN\n\n\n1\nAfghanistan\nAFG\n1961\nNaN"
  },
  {
    "objectID": "slides/week2-tidy.html#pivot",
    "href": "slides/week2-tidy.html#pivot",
    "title": "Tidy data",
    "section": "Pivot",
    "text": "Pivot\nPivoting resolves the issue of having multiple variables stored in one column (common mess 2). It’s the inverse operation of melting."
  },
  {
    "objectID": "slides/week2-tidy.html#pivot-1",
    "href": "slides/week2-tidy.html#pivot-1",
    "title": "Tidy data",
    "section": "Pivot",
    "text": "Pivot\n\n\n\n\n\n\n\n\n\nmeasurement\nweight\n\n\nspecies\n\n\n\n\n\n\nAfricanelephant\nbrain_wt\n5712.0\n\n\nAfricanelephant\nbody_wt\n6654.0\n\n\nAfricangiantpouchedrat\nbrain_wt\n6.6\n\n\nAfricangiantpouchedrat\nbody_wt\n1.0\n\n\n\n\n\n\n\n\n# in pandas\nmammal2.pivot(\n    columns = 'measurement', # which variable(s) do you want to send to new column names?\n    values = 'weight' # which variable(s) do you want to use to populate the new columns?\n).head(2)\n\n\n\n\n\n\n\nmeasurement\nbody_wt\nbrain_wt\n\n\nspecies\n\n\n\n\n\n\nAfricanelephant\n6654.0\n5712.0\n\n\nAfricangiantpouchedrat\n1.0\n6.6"
  },
  {
    "objectID": "slides/week2-tidy.html#pivot-and-melt",
    "href": "slides/week2-tidy.html#pivot-and-melt",
    "title": "Tidy data",
    "section": "Pivot and melt",
    "text": "Pivot and melt\nCommon mess 3 is a combination of messes 1 and 2: values or variables are stored in both rows and columns. Pivoting and melting in sequence can usually fix this.\n\n\n\n\n\n\n\n\n\nDAY\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n...\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\nMONTH\ntype\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nTMAX\n65.0\n62.0\n60.0\n72.0\n61.0\n71.0\n73.0\n79.0\n71.0\n67.0\n...\n61.0\n59.0\n65.0\n55.0\n57.0\n54.0\n55.0\n55.0\n58.0\n63.0\n\n\nTMIN\n37.0\n38.0\n42.0\n43.0\n40.0\n39.0\n38.0\n36.0\n39.0\n37.0\n...\n41.0\n40.0\n38.0\n44.0\n40.0\n48.0\n49.0\n42.0\n37.0\n37.0\n\n\n2\nTMAX\n66.0\n67.0\n69.0\n63.0\n66.0\n68.0\n60.0\n57.0\n59.0\n61.0\n...\n75.0\n75.0\n70.0\n66.0\n69.0\n76.0\n68.0\nNaN\nNaN\nNaN\n\n\nTMIN\n45.0\n40.0\n44.0\n37.0\n38.0\n38.0\n38.0\n49.0\n49.0\n41.0\n...\n37.0\n39.0\n41.0\n39.0\n36.0\n43.0\n38.0\nNaN\nNaN\nNaN\n\n\n3\nTMAX\n68.0\n66.0\n59.0\n62.0\n67.0\n69.0\n60.0\n69.0\n65.0\n58.0\n...\n71.0\n72.0\n67.0\n65.0\n63.0\n72.0\n73.0\n77.0\nNaN\nNaN\n\n\nTMIN\n37.0\n36.0\n36.0\n37.0\n39.0\n43.0\n47.0\n47.0\n47.0\n43.0\n...\n50.0\n49.0\n41.0\n44.0\n40.0\n41.0\n41.0\n42.0\nNaN\nNaN\n\n\n\n\n6 rows × 31 columns"
  },
  {
    "objectID": "slides/week2-tidy.html#pivot-and-melt-1",
    "href": "slides/week2-tidy.html#pivot-and-melt-1",
    "title": "Tidy data",
    "section": "Pivot and melt",
    "text": "Pivot and melt\n\nFirst, meltThen, pivot\n\n\n\nweather3.melt(\n    ignore_index = False,\n    var_name = 'day',\n    value_name = 'temp'\n).head()\n\n\n\n\n\n\n\n\n\nday\ntemp\n\n\nMONTH\ntype\n\n\n\n\n\n\n1\nTMAX\n1\n65.0\n\n\nTMIN\n1\n37.0\n\n\n2\nTMAX\n1\n66.0\n\n\nTMIN\n1\n45.0\n\n\n3\nTMAX\n1\n68.0\n\n\n\n\n\n\n\n\n\n\nweather3.melt(\n    ignore_index = False,\n    var_name = 'day',\n    value_name = 'temp'\n).reset_index().pivot(\n    index = ['MONTH', 'day'],\n    columns = 'type',\n    values = 'temp'\n).reset_index().rename_axis(columns = {'type': ''}).head()\n\n\n\n\n\n\n\n\nMONTH\nday\nTMAX\nTMIN\n\n\n\n\n0\n1\n1\n65.0\n37.0\n\n\n1\n1\n2\n62.0\n38.0\n\n\n2\n1\n3\n60.0\n42.0\n\n\n3\n1\n4\n72.0\n43.0\n\n\n4\n1\n5\n61.0\n40.0"
  },
  {
    "objectID": "slides/week2-tidy.html#merge",
    "href": "slides/week2-tidy.html#merge",
    "title": "Tidy data",
    "section": "Merge",
    "text": "Merge\nMerging resolves the issue of storing observations or variables on one unit type in multiple tables (mess 4). The basic idea is to combine by matching rows."
  },
  {
    "objectID": "slides/week2-tidy.html#merge-1",
    "href": "slides/week2-tidy.html#merge-1",
    "title": "Tidy data",
    "section": "Merge",
    "text": "Merge\nThe code below combines columns in each table by matching rows based on country.\n\npd.merge(undev1, undev2, on = 'country').head(4)\n\n\n\n\n\n\n\n\ntotal_pop\nurban_pct_pop\npop_under5\npop_15to64\npop_over65\ngender_inequality\nparliament_pct_women\nlabor_participation_women\nlabor_participation_men\n\n\ncountry\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfghanistan\n38.0\n25.8\n5.6\n20.9\n1.0\n0.655\n27.2\n21.6\n74.7\n\n\nAlbania\n2.9\n61.2\n0.2\n2.0\n0.4\n0.181\n29.5\n46.7\n64.6\n\n\nAlgeria\n43.1\n73.2\n5.0\n27.1\n2.8\n0.429\n21.5\n14.6\n67.4\n\n\nAndorra\n0.1\n88.0\nNaN\nNaN\nNaN\nNaN\n46.4\nNaN\nNaN"
  },
  {
    "objectID": "slides/week2-tidy.html#merge-2",
    "href": "slides/week2-tidy.html#merge-2",
    "title": "Tidy data",
    "section": "Merge",
    "text": "Merge\nThere are various rules for exactly how to merge, but the general syntactical procedure to merge dataframes df1 and df2 is this.\n\nSpecify an order: merge(df1, df2) or merge(df2, df1).\nSpecify keys: the shared columns to use for matching rows of df1 with rows of df2.\n\nfor example, merging on date will align rows in df2 with rows of df1 that have the same value for date\n\nSpecify a rule for which rows to return after merging\n\nkeep all rows with key entries in df1, drop non-matching rows in df2 (‘left’ join)\nkeep all rows with key entries in df2 drop non-matching rows in df1 (‘right’ join)\nkeep all rows with key entries in either df1 or df2, inducing missing values (‘outer’ join)\nkeep all rows with key entries in both df1 and df2 (‘inner’ join)"
  },
  {
    "objectID": "slides/week2-tidy.html#next-time",
    "href": "slides/week2-tidy.html#next-time",
    "title": "Tidy data",
    "section": "Next time",
    "text": "Next time\nTransformations of tabular data\n\nSlicing and filtering\nDefining new variables\nVectorized operatioons\nAggregation and grouping"
  },
  {
    "objectID": "slides/week5-smoothing.html#outline-for-today",
    "href": "slides/week5-smoothing.html#outline-for-today",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Outline for today",
    "text": "Outline for today\n\nMore on density estimation\n\nnon-Gaussian kernels\nmultivariate KDE\nmixture models\n\nScatterplot smoothing\n\nKernel smoothing\nLOESS"
  },
  {
    "objectID": "slides/week5-smoothing.html#sustainability-data",
    "href": "slides/week5-smoothing.html#sustainability-data",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Sustainability data",
    "text": "Sustainability data\nWe’ll work with sustainability index data for US cities to explore density estimation further.\n\n\nCode\nsust = pd.read_csv('data/sustainability.csv')\nsust.iloc[:, 1:5].head(2)\n\n\n\n\n\n\n\n\n\nName\nEcon_Domain\nSocial_Domain\nEnv_Domain\n\n\n\n\n0\nAberdeen, SD Micro Area\n0.565264\n0.591259\n0.444472\n\n\n1\nAberdeen, WA Micro Area\n0.427671\n0.520744\n0.429274\n\n\n\n\n\n\n\n\n933 distinct cities\nindices for sustainability in enviornmental, social, and eonomic domains"
  },
  {
    "objectID": "slides/week5-smoothing.html#environmental-sustainability-index",
    "href": "slides/week5-smoothing.html#environmental-sustainability-index",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Environmental sustainability index",
    "text": "Environmental sustainability index\nLet’s use the environmental sustainability index (Env_Domain) initially. The distribution of environmental sustainability across cities is shown below.\n\n\nCode\nhist = alt.Chart(\n    sust\n).transform_bin(\n    'esi',\n    field = 'Env_Domain',\n    bin = alt.Bin(step = 0.02)\n).transform_aggregate(\n    count = 'count()',\n    groupby = ['esi']\n).transform_calculate(\n    Density = 'datum.count/(0.02*933)',\n    ESI = 'datum.esi + 0.01'\n).mark_bar(size = 8).encode(\n    x = 'ESI:Q',\n    y = 'Density:Q'\n)\n\nhist.configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-smoothing.html#kde-bandwidth-selection",
    "href": "slides/week5-smoothing.html#kde-bandwidth-selection",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "KDE bandwidth selection",
    "text": "KDE bandwidth selection\nA common choice for Gaussian KDE bandwidth is Scott’s rule:\n\\[\nb = 1.06 \\times s \\times n^{-1/5}\n\\]\n\n\nCode\n# bandwitdth parameter\nn, p = sust.shape\nsigma_hat = sust.Env_Domain.std()\nbw_scott = 1.06*sigma_hat*n**(-1/5)\n\n# plot\nsmooth = alt.Chart(\n    sust\n).transform_density(\n  'Env_Domain',\n  as_ = ['Environmental sustainability index', 'Density'],\n  extent = [0.1, 0.8],\n  bandwidth = bw_scott\n).mark_line(color = 'black').encode(\n    x = 'Environmental sustainability index:Q',\n    y = 'Density:Q'\n)\n\n(hist + smooth).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-smoothing.html#kdes-with-statsmodels",
    "href": "slides/week5-smoothing.html#kdes-with-statsmodels",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "KDEs with statsmodels",
    "text": "KDEs with statsmodels\nThe statsmodels package has KDE implementations with finer control. This will allow us to experiment with other kernels.\n\n# compute the gaussian KDE using statsmodels\nkde = sm.nonparametric.KDEUnivariate(sust.Env_Domain)\nkde.fit(bw = bw_scott)\n\n&lt;statsmodels.nonparametric.kde.KDEUnivariate at 0x213d79a8310&gt;\n\n\n\nThe object kde has .support (\\(x\\)) and .density (\\(\\hat{f}(x)\\)) attributes:\n\n\nCode\nkde_df = pd.DataFrame({\n    'Environmental sustainability index': kde.support, \n    'Density': kde.density\n    })\nkde_df.head(3)\n\n\n\n\n\n\n\n\n\nEnvironmental sustainability index\nDensity\n\n\n\n\n0\n0.088614\n0.000609\n\n\n1\n0.089334\n0.000621\n\n\n2\n0.090054\n0.000645"
  },
  {
    "objectID": "slides/week5-smoothing.html#other-kernels",
    "href": "slides/week5-smoothing.html#other-kernels",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Other kernels",
    "text": "Other kernels\nNow let’s try varying the kernel. The ‘local histogram’ introduced last lecture is a KDE with a uniform kernel.\n\n\nCode\n# compute density estimate\nkde = sm.nonparametric.KDEUnivariate(sust.Env_Domain)\nkde.fit(kernel = 'uni', fft = False, bw = 0.02)\n\n# arrange as dataframe\nkde_df = pd.DataFrame({'Environmental sustainability index': kde.support, 'Density': kde.density})\n\n# plot\nsmooth2 = alt.Chart(\n    kde_df\n).mark_line(color = 'black').encode(\n    x = 'Environmental sustainability index:Q',\n    y = alt.Y('Density:Q', scale = alt.Scale(domain = [0, 12]))\n)\n\n(hist + smooth2 + smooth2.mark_area(opacity = 0.3)).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-smoothing.html#other-kernels-1",
    "href": "slides/week5-smoothing.html#other-kernels-1",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Other kernels",
    "text": "Other kernels\n\n\nTitles indicate statsmodels.nonparametric.KDEUnivariate abbreviations\nNote different axis scales – not as similar as they look!"
  },
  {
    "objectID": "slides/week5-smoothing.html#explore",
    "href": "slides/week5-smoothing.html#explore",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Explore",
    "text": "Explore\nTake 5-6 minutes with your neighbor and use the activity notebook to experiment and answer the following.\n\nHow does the KDE differ if a parabolic (epa) kernel is used in place of a Gaussian (gau) kernel while the bandwidth is held constant?\nWhat effect does a triangular kernel (tri) have on how local peaks appear?\nPick two kernels. What will happen to the KDE for large bandwidths?\nWhich kernel seems to do the best job at capturing the shape closely without under-smoothing?"
  },
  {
    "objectID": "slides/week5-smoothing.html#multivariate-kde",
    "href": "slides/week5-smoothing.html#multivariate-kde",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Multivariate KDE",
    "text": "Multivariate KDE\nNow let’s estimate the joint distribution of environmental and economic sustainability indices. Here are the values:\n\n\nCode\nalt.Chart(\n    sust\n).mark_point().encode(\n    x = alt.X('Env_Domain', title = 'Environmental sustainability'),\n    y = alt.Y('Econ_Domain', title = 'Economic sustainability')\n).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-smoothing.html#multivariate-histograms",
    "href": "slides/week5-smoothing.html#multivariate-histograms",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Multivariate histograms",
    "text": "Multivariate histograms\nThere are a few options for displaying a 2-D histogram. One is to bin and plot a heatmap, as we saw before:\n\n\nCode\nalt.Chart(\n    sust\n).mark_rect().encode(\n    x = alt.X('Env_Domain', bin = alt.Bin(maxbins = 40), title = 'Environmental sustainability'),\n    y = alt.Y('Econ_Domain', bin = alt.Bin(maxbins = 40), title = 'Economic sustainability'),\n    color = alt.Color('count()', scale = alt.Scale(scheme = 'bluepurple'), title = 'Number of U.S. cities')\n).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n).configure_legend(\n    labelFontSize = 14, \n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-smoothing.html#multivariate-histograms-1",
    "href": "slides/week5-smoothing.html#multivariate-histograms-1",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Multivariate histograms",
    "text": "Multivariate histograms\nAnother option is to make a bubble chart with the size of the bubble proportional to the count of observations in the corresponding bin:\n\n\nCode\nalt.Chart(\n    sust\n).mark_circle().encode(\n    x = alt.X('Env_Domain', bin = alt.Bin(maxbins = 40), title = 'Environmental sustainability'),\n    y = alt.Y('Econ_Domain', bin = alt.Bin(maxbins = 40), title = 'Economic sustainability'),\n    size = alt.Size('count()', scale = alt.Scale(scheme = 'bluepurple'))\n).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n).configure_legend(\n    labelFontSize = 14, \n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-smoothing.html#multivariate-kde-1",
    "href": "slides/week5-smoothing.html#multivariate-kde-1",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Multivariate KDE",
    "text": "Multivariate KDE\nThe following computes a multivariate Gaussian KDE\n\n# retrieve data as 2-d array (num observations, num variables)\nfit_ary = sust.loc[:, ['Env_Domain', 'Econ_Domain']].values\n\n# compute KDE\nkde = sm.nonparametric.KDEMultivariate(data = fit_ary, var_type = 'cc')\nkde\n\nKDE instance\nNumber of variables: k_vars = 2\nNumber of samples:   nobs = 933\nVariable types:      cc\nBW selection method: normal_reference\n\n\n\ninput: 2-D array \\((\\text{observations} \\times \\text{variables})\\)\nvariable type specification, one per variate: continuous, discrete, etc.\nmethod of bandwidth selection (not shown)"
  },
  {
    "objectID": "slides/week5-smoothing.html#prediction-grids",
    "href": "slides/week5-smoothing.html#prediction-grids",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Prediction grids",
    "text": "Prediction grids\nA common visualization strategy is to generate a prediction grid: a mesh of values spanning a domain of interest, for the purpose of computing a function at each grid point.\n\nHere is a 20 x 20 mesh:\n\n\nCode\n# resolution of grid (number of points to use along each axis)\ngrid_res = 20\n\n# find grid point coordinates along each axis\nx1 = np.linspace(start = sust.Env_Domain.min(), stop = sust.Env_Domain.max(), num = grid_res)\nx2 = np.linspace(start = sust.Econ_Domain.min(), stop = sust.Econ_Domain.max(), num = grid_res)\n\n# generate a mesh from the coordinates\ngrid1, grid2 = np.meshgrid(x1, x2, indexing = 'ij')\ngrid_ary = np.array([grid1.ravel(), grid2.ravel()]).T\n\n# plot grid points\nalt.Chart(\n    pd.DataFrame(grid_ary).rename(columns = {0: 'env', 1: 'econ'})\n).mark_point(color = 'black').encode(\n    x = alt.X('env', scale = alt.Scale(zero = False)),\n    y = alt.Y('econ', scale = alt.Scale(zero = False))\n).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-smoothing.html#prediction-grids-1",
    "href": "slides/week5-smoothing.html#prediction-grids-1",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Prediction grids",
    "text": "Prediction grids\nWe’ll use a 100 x 100 mesh and kde.pdf() to compute the estimated density at each grid point:\n\n\nCode\n# resolution of grid (number of points to use along each axis)\ngrid_res = 100\n\n# find grid point coordinates along each axis\nx1 = np.linspace(start = sust.Env_Domain.min(), stop = sust.Env_Domain.max(), num = grid_res)\nx2 = np.linspace(start = sust.Econ_Domain.min(), stop = sust.Econ_Domain.max(), num = grid_res)\n\n# generate a mesh from the coordinates\ngrid1, grid2 = np.meshgrid(x1, x2, indexing = 'ij')\ngrid_ary = np.array([grid1.ravel(), grid2.ravel()]).T\n\n# compute the density at each grid point\nf_hat = kde.pdf(grid_ary)\n\n# rearrange as a dataframe\ngrid_df = pd.DataFrame({'env': grid1.reshape(grid_res**2), \n                        'econ': grid2.reshape(grid_res**2),\n                        'density': f_hat})\n\n# preview, for understanding\ngrid_df.head()\n\n\n\n\n\n\n\n\n\nenv\necon\ndensity\n\n\n\n\n0\n0.132467\n0.143811\n1.325548e-17\n\n\n1\n0.132467\n0.150278\n6.367762e-17\n\n\n2\n0.132467\n0.156745\n2.929104e-16\n\n\n3\n0.132467\n0.163212\n1.290785e-15\n\n\n4\n0.132467\n0.169679\n5.451826e-15"
  },
  {
    "objectID": "slides/week5-smoothing.html#multivariate-kde-heatmap",
    "href": "slides/week5-smoothing.html#multivariate-kde-heatmap",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Multivariate KDE heatmap",
    "text": "Multivariate KDE heatmap\n\n\nCode\n# kde\nkde_smooth = alt.Chart(\n    grid_df, title = 'Gaussian KDE'\n).mark_rect(opacity = 0.8).encode(\n    x = alt.X('env', bin = alt.Bin(maxbins = grid_res), title = 'Environmental sustainability'),\n    y = alt.Y('econ', bin = alt.Bin(maxbins = grid_res), title = 'Economic sustainability'),\n    color = alt.Color('mean(density)', # a bit hacky, but works\n                      scale = alt.Scale(scheme = 'bluepurple', type = 'sqrt'),\n                      title = 'Density')\n)\n\n# histogram\nbubble = alt.Chart(\n    sust\n).mark_circle().encode(\n    x = alt.X('Env_Domain', bin = alt.Bin(maxbins = 40), title = 'Environmental sustainability'),\n    y = alt.Y('Econ_Domain', bin = alt.Bin(maxbins = 40), title = 'Economic sustainability'),\n    size = alt.Size('count()', scale = alt.Scale(scheme = 'bluepurple'), title = 'Cities')\n)\n\n# layer\n(bubble + kde_smooth).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n).configure_legend(\n    labelFontSize = 14, \n    titleFontSize = 16\n)\n\n\n\n\n\n\n\n\n\nDoes the KDE seem like a good estimate?\nWhat, if anything, does the graphic convey about the sustainability of cities?"
  },
  {
    "objectID": "slides/week5-smoothing.html#mixture-models",
    "href": "slides/week5-smoothing.html#mixture-models",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Mixture models",
    "text": "Mixture models\nKDE is a nonparametric technique: it involves no population parameters.\n\nMixture models are a parametric alternative for density estimation. The Gaussian mixture model is: \\[\nf(x) = a_1 \\varphi(x; \\mu_1, \\sigma_1) + \\cdots + a_n \\varphi(x; \\mu_n, \\sigma_n)\n\\]\n\n\\(f(x)\\) is the distribution of interest\n\\(\\varphi(\\cdot; \\mu, \\sigma)\\) denotes a Gaussian density with mean \\(\\mu\\) and standard deviation \\(\\sigma\\)\nthe model has \\(n\\) components\n\\(a_1, \\dots, a_n\\) are the mixing parameters\nfitted using the EM algorithm"
  },
  {
    "objectID": "slides/week5-smoothing.html#bivariate-mixture-model",
    "href": "slides/week5-smoothing.html#bivariate-mixture-model",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Bivariate mixture model",
    "text": "Bivariate mixture model\nLet’s fit a mixture to the joint distribution of environmental and economic sustainability indices:\n\n# configure and fit mixture model\ngmm = mixture.GaussianMixture(n_components = 2, covariance_type = 'full')\ngmm.fit(fit_ary)\n\n\nWe can inspect the estimated components’ centers (means):\n\n\nCode\ncenters = pd.DataFrame(gmm.means_).rename(columns = {0: 'env', 1: 'econ'})\ncenters\n\n\n\n\n\n\n\n\n\nenv\necon\n\n\n\n\n0\n0.440006\n0.461773\n\n\n1\n0.381889\n0.478218\n\n\n\n\n\n\n\n\n\nAnd the mixing parameters:\n\n\nCode\nprint('mixing parameters', gmm.weights_)\n\n\nmixing parameters [0.7203574 0.2796426]"
  },
  {
    "objectID": "slides/week5-smoothing.html#bivariate-mixture-model-1",
    "href": "slides/week5-smoothing.html#bivariate-mixture-model-1",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Bivariate mixture model",
    "text": "Bivariate mixture model\n\n\nCode\n# evaluate log-likelihood\ngrid_df['gmm'] = np.exp(gmm.score_samples(grid_ary))\n\n# gmm\ngmm_smooth = alt.Chart(\n    grid_df, title = 'GMM'\n).mark_rect(opacity = 0.8).encode(\n    x = alt.X('env', bin = alt.Bin(maxbins = grid_res), title = 'Environmental sustainability'),\n    y = alt.Y('econ', bin = alt.Bin(maxbins = grid_res), title = 'Economic sustainability'),\n    color = alt.Color('mean(gmm)', # a bit hacky, but works\n                      scale = alt.Scale(scheme = 'bluepurple', type = 'sqrt'),\n                      title = 'Density')\n)\n\n# centers of mixture components\nctr = alt.Chart(centers).mark_point(color = 'black', shape = 'triangle').encode(x = 'env', y = 'econ')\n\n((bubble + gmm_smooth + ctr) | (bubble + kde_smooth)).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n).configure_legend(\n    labelFontSize = 14, \n    titleFontSize = 16\n)\n\n\n\n\n\n\n\n\n\nWhat differences do you observe?\nWhich do you prefer and why?"
  },
  {
    "objectID": "slides/week5-smoothing.html#pros-and-cons",
    "href": "slides/week5-smoothing.html#pros-and-cons",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Pros and Cons",
    "text": "Pros and Cons\nKDE is a nonparametric method and is sometimes called a memory-based procedure: it uses all available data every time an estimated value is calculated.\n\nAdvantages: minimal assumptions, highly flexible\nDisadvantages: not parsimonious, computationally intensive\n\n\nGMM’s are parametric models.\n\nAdvantages: closed-form structure, good for multimodal distributions, various kinds of inference and prediction are possible\nDisadvantages: estimation is less straightforward, may over-smooth, possible identifiability issues"
  },
  {
    "objectID": "slides/week5-smoothing.html#gmms-for-capturing-subpopulations",
    "href": "slides/week5-smoothing.html#gmms-for-capturing-subpopulations",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "GMMs for capturing subpopulations",
    "text": "GMMs for capturing subpopulations\nThe GMM is especially useful if there are latent subpopulations in the data. Recall the simulated population of hawks:\n\n\nCode\n# for reproducibility\nnp.random.seed(40721)\n\n# simulate hypothetical population\nfemale_hawks = pd.DataFrame(\n    data = {'length': np.random.normal(loc = 57.5, scale = 3, size = 3000),\n            'sex': np.repeat('female', 3000)}\n)\n\nmale_hawks = pd.DataFrame(\n    data = {'length': np.random.normal(loc = 50.5, scale = 3, size = 2000),\n            'sex': np.repeat('male', 2000)}\n)\n\npopulation_hawks = pd.concat([female_hawks, male_hawks], axis = 0)\n\npopulation_hawks.groupby('sex').head(1)\n\n\n\n\n\n\n\n\n\nlength\nsex\n\n\n\n\n0\n53.975230\nfemale\n\n\n0\n53.076663\nmale\n\n\n\n\n\n\n\n\nLet’s imagine we have a sample but without recorded sexes.\n\n\nCode\nnp.random.seed(50223)\nsamp = population_hawks.sample(n = 500).drop(columns = 'sex')\nsamp.head(3)\n\n\n\n\n\n\n\n\n\nlength\n\n\n\n\n840\n62.513960\n\n\n2747\n52.490258\n\n\n2698\n56.879861"
  },
  {
    "objectID": "slides/week5-smoothing.html#hawks",
    "href": "slides/week5-smoothing.html#hawks",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Hawks",
    "text": "Hawks\nThe histogram is not obviously bimodal, but we can suppose we’re aware of the sex differences in length and just don’t have that information.\n\n\nCode\nhist_hawks = alt.Chart(samp).transform_bin(\n    'length',\n    field = 'length',\n    bin = alt.Bin(step = 2)\n).transform_aggregate(\n    count = 'count()',\n    groupby = ['length']\n).transform_calculate(\n    density = 'datum.count/1000',\n    length = 'datum.length + 1'\n).mark_bar(size = 20).encode(\n    x = 'length:Q', \n    y = 'density:Q'\n)\n\nhist_hawks.configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-smoothing.html#recovering-subpopulations",
    "href": "slides/week5-smoothing.html#recovering-subpopulations",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Recovering subpopulations",
    "text": "Recovering subpopulations\nIf we fit a mixture model with two components, amazingly, the mixture components accurately recover the distributions for each sex, even though this was a latent (unobserved) variable:\n\n# configure and fit mixture model\ngmm_hawks = mixture.GaussianMixture(n_components = 2)\ngmm_hawks.fit(samp.length.values.reshape(-1, 1))\n\n# compare components with subpopulations\nprint('gmm component means: ', gmm_hawks.means_.ravel())\nprint('population means by sex: ', np.sort(population_hawks.groupby('sex').mean().values.ravel()))\n\nprint('gmm component variances: ', gmm_hawks.covariances_.ravel())\nprint('population variances by sex: ', np.sort(population_hawks.groupby('sex').var().values.ravel()))\n\ngmm component means:  [50.70543698 57.33071086]\npopulation means by sex:  [50.48194081 57.5749014 ]\ngmm component variances:  [ 8.60692088 10.12549367]\npopulation variances by sex:  [8.522606   9.39645762]\n\n\n\nNote that the means and variances are estimated from the sample but compared with the population values above."
  },
  {
    "objectID": "slides/week5-smoothing.html#gmm-density-estimate",
    "href": "slides/week5-smoothing.html#gmm-density-estimate",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "GMM density estimate",
    "text": "GMM density estimate\nFurther, the density estimate fits reasonably well:\n\n\nCode\n# compute a grid of lengths\ngrid_hawks = np.linspace(population_hawks.length.min(), population_hawks.length.max(), num = 500)\ndens = np.exp(gmm_hawks.score_samples(grid_hawks.reshape(-1, 1)))\n\ngmm_smooth_hawks = alt.Chart(\n    pd.DataFrame({'length': grid_hawks, 'density': dens})\n).mark_line(color = 'black').encode(\n    x = 'length',\n    y = 'density'\n)\n\n(hist_hawks + gmm_smooth_hawks).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-smoothing.html#explore-1",
    "href": "slides/week5-smoothing.html#explore-1",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Explore",
    "text": "Explore\nTake 2 minutes with your neighbor and use the activity notebook to explore the following questions.\n\nWhat happens if you fit the GMM with different numbers of components?\nDoes the solution change if the GMM is re-fitted?"
  },
  {
    "objectID": "slides/week5-smoothing.html#kernel-smoothing",
    "href": "slides/week5-smoothing.html#kernel-smoothing",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Kernel smoothing",
    "text": "Kernel smoothing\nKernel smoothing is a technique similar to KDE used to visualize and estimate relationships (rather than distributions).\n\nWe’ll use the GDP and life expectancy data from lab 3 to illustrate.\n\n\nCode\n# read in data and subset\nlife = pd.read_csv('data/life-gdp.csv')\nlife = life[life.Year == 2000].loc[:, ['Country Name', 'All', 'GDP per capita']]\nlife['GDP per capita'] = np.log(life['GDP per capita'])\nlife.rename(columns = {'GDP per capita': 'log_gdp', 'All': 'life_exp', 'Country Name': 'country'}, inplace=True)\n\n# scatterplot\nscatter = alt.Chart(\n    life\n).mark_point().encode(\n    x = alt.X('log_gdp', scale = alt.Scale(zero = False), title = 'log(GDP/capita)'),\n    y = alt.Y('life_exp', scale = alt.Scale(zero = False), title = 'Life expectancy')\n)\n\nscatter.configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-smoothing.html#the-method",
    "href": "slides/week5-smoothing.html#the-method",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "The method",
    "text": "The method\nThe technique consists in estimating trend by local weighted averaging; a kernel function is used to determine the exact weights.\n\\[\n\\hat{y}(x) = \\frac{\\sum_i K_b (x_i - x) y_i}{\\sum_i K_b(x_i - x)}\n\\]\n\n\n\n\nIllustration of kernel smoothing"
  },
  {
    "objectID": "slides/week5-smoothing.html#example",
    "href": "slides/week5-smoothing.html#example",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Example",
    "text": "Example\n\n\nCode\n# calculate kernel smoother\nkernreg = sm.nonparametric.KernelReg(endog = life.life_exp.values,\n                                     exog = life.log_gdp.values,\n                                     reg_type = 'lc', \n                                     var_type = 'c',\n                                     ckertype = 'gaussian')\n\n# grid of gdp values\ngdp_grid = np.linspace(life.log_gdp.min(), life.log_gdp.max(), num = 100)\n\n# calculate kernel smooth at each value\nfitted_values = kernreg.fit(gdp_grid)[0]\n\n# arrange as data frame\npred_df = pd.DataFrame({'log_gdp': gdp_grid, 'life_exp': fitted_values})\n\n# plot\nkernel_smooth = alt.Chart(\n    pred_df\n).mark_line(\n    color = 'black'\n).encode(\n    x = 'log_gdp', \n    y = alt.Y('life_exp', scale = alt.Scale(zero = False))\n)\n\n(scatter + kernel_smooth).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)\n\n\n\n\n\n\n\n\nNotice that the kernel smooth trails off a bit near the boundary – this is because fewer points are being averaged once the smoothing window begins to extend beyond the range of the data."
  },
  {
    "objectID": "slides/week5-smoothing.html#loess",
    "href": "slides/week5-smoothing.html#loess",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "LOESS",
    "text": "LOESS\nLocally weighted scatterplot smoothing (LOESS or LOWESS) largely corrects this issue by stitching together lines (or low-order polynomials) fitted to local subsets of data; this way, near the boundary, the estimate still takes account of any trend present in the data."
  },
  {
    "objectID": "slides/week5-smoothing.html#loess-example",
    "href": "slides/week5-smoothing.html#loess-example",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "LOESS example",
    "text": "LOESS example\n\n\nCode\n# fit loess smooth\nloess = sm.nonparametric.lowess(endog = life.life_exp.values,\n                                exog = life.log_gdp.values, \n                                frac = 0.3,\n                                xvals = gdp_grid)\n\n# store as data frame\nloess_df = pd.DataFrame({'log_gdp': gdp_grid, 'life_exp': loess})\n\n# plot\nloess_smooth = alt.Chart(\n    loess_df\n).mark_line(\n    color = 'blue',\n    strokeDash = [8, 8]\n).encode(\n    x = 'log_gdp', \n    y = alt.Y('life_exp', scale = alt.Scale(zero = False))\n)\n\n(scatter + loess_smooth).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-smoothing.html#comparison",
    "href": "slides/week5-smoothing.html#comparison",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Comparison",
    "text": "Comparison\nIf we compare the LOESS curve with the kernel smoother, the different behavior on the boundary is evident:\n\n\nCode\n(scatter + loess_smooth + kernel_smooth).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-smoothing.html#explore-2",
    "href": "slides/week5-smoothing.html#explore-2",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Explore",
    "text": "Explore\nTake 2-3 minutes with your neighbor and use the activity notebook to answer the following questions.\n\nAre there any bandwidths that give you a straight-line fit?\nWhat seems to be the minimum bandwidth?\nWhich bandwidth best captures the pattern of scatter?"
  },
  {
    "objectID": "slides/week5-density.html#announcements",
    "href": "slides/week5-density.html#announcements",
    "title": "Exploratory analysis and density estimation",
    "section": "Announcements",
    "text": "Announcements\n\nHw3 was posted this week: Due in two weeks (Nov 13)\nNo lab next week"
  },
  {
    "objectID": "slides/week5-density.html#this-week-eda-and-smoothing",
    "href": "slides/week5-density.html#this-week-eda-and-smoothing",
    "title": "Exploratory analysis and density estimation",
    "section": "This week: EDA and smoothing",
    "text": "This week: EDA and smoothing\n\nWhat is exploratory data analysis (EDA)?\n\nThe role of data: information or evidence?\nExploratory vs. confirmatory analysis\nEssential exploratory questions: variation and co-variation\n\nSmoothing\n\nKernel density estimation (KDE)\nLOESS"
  },
  {
    "objectID": "slides/week5-density.html#eda",
    "href": "slides/week5-density.html#eda",
    "title": "Exploratory analysis and density estimation",
    "section": "EDA",
    "text": "EDA\nThe term and spirit of exploratory data analysis (EDA) is attributed to John Tukey, whose philosophically-leaning work in statistics in the 1960’s and 1970’s stressed the need for more data-driven methods.\n\n\nFor a long time I have thought I was a statistician, interested in inferences from the particular to the general … All in all, I have come to feel that my central interest is in data analysis [which] is a larger and more varied field than inference. (Tukey, 1962)\n\n\n\nEDA is an initial stage of non-inferential and largely model-free analysis aiming at understanding the structure, patterns, and particularities present in a dataset."
  },
  {
    "objectID": "slides/week5-density.html#data-as-evidence",
    "href": "slides/week5-density.html#data-as-evidence",
    "title": "Exploratory analysis and density estimation",
    "section": "Data as evidence",
    "text": "Data as evidence\nExperimental data usually serve the role of evidence for or against prespecified hypotheses.\n\n\\[\n\\text{hypothesis} \\longrightarrow \\text{data} \\longrightarrow \\text{inference}\n\\]\n\n\nFor example, in vaccine efficacy trials, trial data are collected precisely to affirm or refute the hypothesis of no effect:\n\n\n\nVaccine group\nPlacebo group\n\n\n\n\n11 cases\n185 cases\n\n\n\n\\(\\hat{P}(\\text{case is in the vaccine group}) = 0.056 \\quad\\Longrightarrow\\quad \\text{evidence of effect}\\)"
  },
  {
    "objectID": "slides/week5-density.html#data-as-information",
    "href": "slides/week5-density.html#data-as-information",
    "title": "Exploratory analysis and density estimation",
    "section": "Data as information",
    "text": "Data as information\nBy contrast, observational data more often serve the role of information about some phenomenon.\n\nFor example, a secondary trial target might is to assess safety by gathering observational data on side effects; for this there is no hypothesis."
  },
  {
    "objectID": "slides/week5-density.html#eda-then-cda",
    "href": "slides/week5-density.html#eda-then-cda",
    "title": "Exploratory analysis and density estimation",
    "section": "EDA then CDA",
    "text": "EDA then CDA\nThe picture that most practitioners have of modern data science is that EDA precedes confirmatory data analysis (CDA).\n\nEDA is used to generate hypotheses or formulate a model based on patterns in the data\nCDA, consisting of model specification and estimation, is used for inference and/or prediction\n\n\nAside: Historically, statistics has focused on CDA – and therefore a lot of your PSTAT coursework does, too."
  },
  {
    "objectID": "slides/week5-density.html#essential-exploratory-questions",
    "href": "slides/week5-density.html#essential-exploratory-questions",
    "title": "Exploratory analysis and density estimation",
    "section": "Essential exploratory questions",
    "text": "Essential exploratory questions\n\nWhen you ask a question, the question focuses your attention on a specific part of your dataset and helps you decide which graphs, models, or transformations to make.\n\n\nThere are two basic kinds of questions that are always useful:\n\nWhat type of variation occurs within variables?\nWhat type of covariation occurs between variables?"
  },
  {
    "objectID": "slides/week5-density.html#variation-in-one-variable",
    "href": "slides/week5-density.html#variation-in-one-variable",
    "title": "Exploratory analysis and density estimation",
    "section": "Variation in one variable",
    "text": "Variation in one variable\nVariation in data is the tendency of values to change from measurement to measurement.\n\nFor example, the following observations from your mini project data are around 8 \\(\\mu g/m^3\\), but each one is different.\n\n\n\n\n\n\n\n\n\nPM25_mean\nCity\nState\nYear\n\n\n\n\n0\n8.6\nAberdeen\nSD\n2000\n\n\n1\n8.6\nAberdeen\nSD\n2001\n\n\n2\n7.9\nAberdeen\nSD\n2002\n\n\n3\n8.4\nAberdeen\nSD\n2003\n\n\n\n\n\n\n\n\n\nWhat does it mean to ask what ‘type’ of variation there is in this data?"
  },
  {
    "objectID": "slides/week5-density.html#questions-about-variation",
    "href": "slides/week5-density.html#questions-about-variation",
    "title": "Exploratory analysis and density estimation",
    "section": "Questions about variation",
    "text": "Questions about variation\nThere aren’t exact types of variation, but here are some useful questions:\n\n(Common) Which values are most common?\n(Rare) Which values are rare?\n(Spread) How spread out are the values and how are they spread out?\n(Shape) Are values spread out evenly or irregularly?\n\n\nThese questions often lead the way to more focused ones."
  },
  {
    "objectID": "slides/week5-density.html#air-quality",
    "href": "slides/week5-density.html#air-quality",
    "title": "Exploratory analysis and density estimation",
    "section": "Air quality",
    "text": "Air quality\nThe following histogram shows the distribution of PM 2.5 concentrations across all 200 cities and 20 years.\n\n\n\n\n\n\n\n\nIt shows several statistical properties of the data related to variation:\n\nThe common values have the highest bars – values between roughly 6 and 14.\nValues under 4 and over 18 are rare, accounting for under 5% of the data.\nValues are concentrated between 4 and 18, but are spread from 2 to 52.\nThe shape is pretty even but a little more spread out to the right (“right skew”)."
  },
  {
    "objectID": "slides/week5-density.html#question-refinement",
    "href": "slides/week5-density.html#question-refinement",
    "title": "Exploratory analysis and density estimation",
    "section": "Question refinement",
    "text": "Question refinement\nNew question: The national standard is 12 micrograms per cubic meter. Over 1,000 measurements exceeded this. Was it just a few cities, or more widespread?\n\n\n\n\n\n\n\n\n\n\nMany cities exceeded the standard at some point in time: over 70% of the cities in the dataset. So it was more widespread, but these were the worst:\n\n\n\n\nCity                 State\nHanford-Corcoran      CA      20\nVisalia-Porterville   CA      20\nFresno                CA      19\nBakersfield           CA      18\nName: Years exceeding standard, dtype: int64"
  },
  {
    "objectID": "slides/week5-density.html#further-questions",
    "href": "slides/week5-density.html#further-questions",
    "title": "Exploratory analysis and density estimation",
    "section": "Further questions",
    "text": "Further questions\nHow many cities exceed the benchmark each year? Does this change from year to year?\n\n\n\n\n\n\n\n\n\n\nThere are a lot of years and it’s hard to see anything with all the overlapping bars.\n\nRemember the rule? Don’t stack histograms.\nUse density plots instead."
  },
  {
    "objectID": "slides/week5-density.html#further-questions-1",
    "href": "slides/week5-density.html#further-questions-1",
    "title": "Exploratory analysis and density estimation",
    "section": "Further questions",
    "text": "Further questions\nVisually, it’s a lot easier to distinguish overlapping lines than overlapping bars. Smoothing out the histogram produces this:\n\n\n\n\n\n\n\n\n\n\nThis shows that both the variation in PM 2.5 and the typical values are diminishing over time.\n\nsuggests fewer cities exceed the EPA standard (12 \\(\\mu g/m^3\\)) over time\na few outliers in some early year\nnot the best presentation graphic, but useful exploratory graphic"
  },
  {
    "objectID": "slides/week5-density.html#further-questions-2",
    "href": "slides/week5-density.html#further-questions-2",
    "title": "Exploratory analysis and density estimation",
    "section": "Further questions",
    "text": "Further questions\n\n\nThis gets the message across better.\n\n\n\n\n\n\n\n\nAnd here are those outlying values:\n\n\n\n\n\n\n\n\n\nPM25_mean\nCity\nState\nYear\n\n\n\n\n1984\n51.2\nFairbanks\nAK\n2004\n\n\n1985\n31.3\nFairbanks\nAK\n2005"
  },
  {
    "objectID": "slides/week5-density.html#density-estimates",
    "href": "slides/week5-density.html#density-estimates",
    "title": "Exploratory analysis and density estimation",
    "section": "Density estimates",
    "text": "Density estimates\nAll of the above has amounted to exploration of the distribution of PM 2.5 values across years and cities.\n\nDensity estimates provide smooth approximations of distributions:\n\n\n\n\n\n\n\n\n\nThese are useful tools for answering questions about variation. Relative to the histogram:\n\nEasier to see the shape, spread, and typical values quickly.\nEasier to compare multiple distributions."
  },
  {
    "objectID": "slides/week5-density.html#histograms-and-probability-densities",
    "href": "slides/week5-density.html#histograms-and-probability-densities",
    "title": "Exploratory analysis and density estimation",
    "section": "Histograms and probability densities",
    "text": "Histograms and probability densities\nFrom 120A, a probability density/mass function has two properties:\n\nNonnegative: \\(f(x) \\geq 0\\) for every \\(x \\in \\mathbb{R}\\).\nSums/integrates to one: \\(\\int_\\mathbb{R} f(x) dx = 1\\) or \\(\\sum_{x \\in \\mathbb{R}} f(x) = 1\\)\n\n\nHistograms are almost proper density functions: they satisfy (1) but not (2)."
  },
  {
    "objectID": "slides/week5-density.html#a-preliminary-indicator-functions",
    "href": "slides/week5-density.html#a-preliminary-indicator-functions",
    "title": "Exploratory analysis and density estimation",
    "section": "A preliminary: indicator functions",
    "text": "A preliminary: indicator functions\nIn what follows we’re going to express the histogram mathematically as a function of data values.\n\nThis will require the use of indicator functions, which are simply functions that are 1 or 0 depending on a condition. They are denoted like this:\n\\[\n\\mathbf{1}\\{\\text{condition}\\} = \\begin{cases} 1 &\\text{ if condition is true} \\\\ 0 &\\text{ if condition is false} \\end{cases}\n\\]\n\n\nThe sum of an indicator gives a count of how many times the condition is met:\n\\[\n\\sum_i \\mathbf{1}\\{x_i &gt; 0\\} = \\#\\text{ of values that are positive}\n\\]"
  },
  {
    "objectID": "slides/week5-density.html#count-scale-histograms",
    "href": "slides/week5-density.html#count-scale-histograms",
    "title": "Exploratory analysis and density estimation",
    "section": "Count scale histograms",
    "text": "Count scale histograms\nWhen the bar height is a count of the number of observations in each bin, the histogram is on the count scale.\n\nMore precisely, if the values are \\(x_1, \\dots, x_n\\), then the height of the bar for the \\(j\\)th bin \\(B_j = (a_j, b_j]\\) is: \\[\n\\text{height}_j = \\sum_{i = 1}^n \\mathbf{1}\\{x_i \\in B_j\\}\n\\]\n\n\n\n\nBad for comparisons: the bar heights incomparable in scale whenever the bin widths and/or sample sizes differ."
  },
  {
    "objectID": "slides/week5-density.html#density-scale-histograms",
    "href": "slides/week5-density.html#density-scale-histograms",
    "title": "Exploratory analysis and density estimation",
    "section": "Density scale histograms",
    "text": "Density scale histograms\nA fix that ensures comparability of scale for any two histograms is to normalize heights by bin width \\(b\\) and sample size \\(n\\).\n\\[\n\\text{height}_j = \\color{red}{\\frac{1}{nb}} \\sum_{i = 1}^n \\mathbf{1}\\{x_i \\in B_j\\} \\quad\\text{where}\\quad b = b_j - a_j\n\\]\n\nNow the area under the histogram is \\(\\sum_j b \\times \\text{height}_j = 1\\), so we call this a density scale histogram, because it is a valid probability density."
  },
  {
    "objectID": "slides/week5-density.html#smoothing",
    "href": "slides/week5-density.html#smoothing",
    "title": "Exploratory analysis and density estimation",
    "section": "Smoothing",
    "text": "Smoothing\nKernel density estimates are local smoothings of the density scale histogram.\nThis can be seen by comparing the type of smooth curve we saw earlier with the density scale histogram."
  },
  {
    "objectID": "slides/week5-density.html#how-kde-works",
    "href": "slides/week5-density.html#how-kde-works",
    "title": "Exploratory analysis and density estimation",
    "section": "How KDE works",
    "text": "How KDE works\nTechnically, KDE is a convolution filtering. We can try to understand it in more intuitive terms by developing the idea constructively from the density histogram in two steps.\n\nDo locally adaptive binning\nReplace counting by weighted aggregation"
  },
  {
    "objectID": "slides/week5-density.html#the-histogram-as-a-step-function",
    "href": "slides/week5-density.html#the-histogram-as-a-step-function",
    "title": "Exploratory analysis and density estimation",
    "section": "The histogram as a step function",
    "text": "The histogram as a step function\nThe value (height) of the density scale histogram at an arbitrary point \\(\\color{red}{x}\\) is \\[\n\\text{hist}(\\color{red}{x}) = \\frac{1}{nb} \\sum_{i = 1}^n \\sum_{j} \\mathbf{1}\\{\\color{red}{x} \\in B_j\\} \\mathbf{1}\\{x_i \\in B_j\\}\n\\]\n\nHere’s what those indicators do:\n\\[\n\\mathbf{1}\\{\\color{red}{x} \\in B_j\\} \\quad \\text{finds the right bin}\\;,\\quad\n\\mathbf{1}\\{x_i \\in B_j\\} \\quad \\text{picks out the data points in the bin}\n\\]"
  },
  {
    "objectID": "slides/week5-density.html#a-local-histogram",
    "href": "slides/week5-density.html#a-local-histogram",
    "title": "Exploratory analysis and density estimation",
    "section": "A ‘local’ histogram",
    "text": "A ‘local’ histogram\nOne could do a ‘moving window’ binning by allowing the height at \\(\\color{red}{x}\\) to be a normalization of the count in a neighborhood of \\(x\\) of width \\(b\\) rather than in one of a fixed set of bins:\n\\[\n\\text{hist}_\\text{local}(\\color{red}{x}) = \\frac{1}{nb} \\sum_{i = 1}^n \\mathbf{1}\\left\\{|x_i - \\color{red}{x}| &lt; \\frac{b}{2}\\right\\}\n\\]\n\nLet’s call this a local histogram, because the height at any point \\(\\color{red}{x}\\) is determined relative to the exact location of \\(\\color{red}{x}\\)."
  },
  {
    "objectID": "slides/week5-density.html#drawing-a-local-histogram",
    "href": "slides/week5-density.html#drawing-a-local-histogram",
    "title": "Exploratory analysis and density estimation",
    "section": "Drawing a local histogram",
    "text": "Drawing a local histogram\n\\[\n\\text{hist}_\\text{local}(\\color{red}{x}) = \\frac{1}{nb} \\sum_{i = 1}^n \\mathbf{1}\\left\\{|x_i - \\color{red}{x}| &lt; \\frac{b}{2}\\right\\}\n\\]"
  },
  {
    "objectID": "slides/week5-density.html#pm-2.5-data",
    "href": "slides/week5-density.html#pm-2.5-data",
    "title": "Exploratory analysis and density estimation",
    "section": "PM 2.5 data",
    "text": "PM 2.5 data\nHere’s what that would look like with \\(b = 1\\) for the air quality data:"
  },
  {
    "objectID": "slides/week5-density.html#pm-2.5-data-1",
    "href": "slides/week5-density.html#pm-2.5-data-1",
    "title": "Exploratory analysis and density estimation",
    "section": "PM 2.5 data",
    "text": "PM 2.5 data\nZooming in reveals that this is still a step function:"
  },
  {
    "objectID": "slides/week5-density.html#the-kernel-function",
    "href": "slides/week5-density.html#the-kernel-function",
    "title": "Exploratory analysis and density estimation",
    "section": "The kernel function",
    "text": "The kernel function\nThe local histogram is in fact a density estimate with a uniform ‘kernel’: \\[\n\\text{hist}_\\text{local}(\\color{red}{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\underbrace{\\frac{1}{b}\\mathbf{1}\\left\\{|x_i - \\color{red}{x}| &lt; \\frac{b}{2}\\right\\}}_\\text{kernel function}\n\\]\n\nuniform because the kernel function is constant about \\(x\\)\nwhen \\(x_1, \\dots, x_n\\) are a random sample, this is an estimate of the population denisty"
  },
  {
    "objectID": "slides/week5-density.html#gaussian-kde",
    "href": "slides/week5-density.html#gaussian-kde",
    "title": "Exploratory analysis and density estimation",
    "section": "Gaussian KDE",
    "text": "Gaussian KDE\nReplacing the uniform kernel with a Gaussian kernel yields a smooth density estimate: \\[\n\\hat{f}(\\color{red}{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\frac{1}{b}\\varphi\\left(\\frac{x_i - \\color{red}{x}}{b}\\right)\n\\]\n\n\\(\\varphi\\) is the standard Gaussian density \\(\\varphi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{- \\frac{z^2}{2}\\right\\}\\)\n\\(b\\) is the smoothing ‘bandwidth’\n\n\nIn effect, the KDE curve at any point \\(\\color{red}{x}\\) is a weighted aggregation of all the data with weights proportional to their distance from \\(\\color{red}{x}\\)."
  },
  {
    "objectID": "slides/week5-density.html#smoothing-bandwidth",
    "href": "slides/week5-density.html#smoothing-bandwidth",
    "title": "Exploratory analysis and density estimation",
    "section": "Smoothing bandwidth",
    "text": "Smoothing bandwidth\nThe bandwidth parameter \\(b\\) controls how wiggly the KDE curve is.\n\n\n\n\n\n\n\n\nThe choice of smoothing bandwidth can change the visual impression.\n\ntoo much smoothing can obscure outliers and multiple modes\ntoo little smoothing can misleadingly overemphasize sample artefacts"
  },
  {
    "objectID": "slides/week5-density.html#other-kernels",
    "href": "slides/week5-density.html#other-kernels",
    "title": "Exploratory analysis and density estimation",
    "section": "Other kernels",
    "text": "Other kernels\nIn general, a KDE can be computed with any appropriately normalized nonnegative kernel function \\(K_b(\\cdot)\\).\n\\[\n\\hat{f}_{K_b}(\\color{red}{x}) = \\frac{1}{n} \\sum_{i = 1}^n K_b\\left(x_i - \\color{red}{x}\\right)\n\\]\nOther common kernel functions include:\n\ntriangular kernel \\(K(z) = 1 - |z|\\)\nparabolic kernel \\(K(z) \\propto 1 - z^2\\)\ncosine kernel \\(K(z) \\propto \\cos\\left(\\frac{\\pi z}{2}\\right)\\)\ncircular density \\(K(z) \\propto \\exp\\left\\{k\\cos(z)\\right\\}\\)\nany symmetric continuous probability density function"
  },
  {
    "objectID": "slides/week5-density.html#kde-in-higher-dimensions",
    "href": "slides/week5-density.html#kde-in-higher-dimensions",
    "title": "Exploratory analysis and density estimation",
    "section": "KDE in higher dimensions",
    "text": "KDE in higher dimensions\nUsually for multivariate data it’s easier to work with conditional distributions, but KDE can be generalized to estimating joint densities in \\(p\\) dimensions:\n\\[\n\\hat{f}_K (\\mathbf{x}) = \\frac{1}{n} \\sum_i |\\mathbf{B}|^{-1/2} K \\left(\\mathbf{B^{-1/2}}(\\mathbf{x} - \\mathbf{x}_i)\\right)\n\\]\n\n\\(\\mathbf{x}\\in\\mathbb{R}^p\\) is a \\(p\\)-dimensional vector\n\\(K:\\mathbb{R}^p \\rightarrow \\mathbb{R}\\) is a nonnegative kernel function\n\\(B\\) is a \\(p\\times p\\) matrix of bandwidth parameters\n\n\nThe usual approach is to decorrelate the variates and apply a product kernel \\(K(\\mathbf{z}) = K_1(z_1)K_2(z_2)\\cdots K_p(z_p)\\) with separate bandwidths for each dimension."
  },
  {
    "objectID": "slides/week5-density.html#bivariate-example",
    "href": "slides/week5-density.html#bivariate-example",
    "title": "Exploratory analysis and density estimation",
    "section": "Bivariate example",
    "text": "Bivariate example\n\n\n\n\n\n\nBivariate histogram shown as a raster plot\n\n\n\n\n\n\n\nContours of density estimate\n\n\n\n\n\nDoes race time seem correlated with runner’s age?"
  },
  {
    "objectID": "slides/week10-clustering.html#announcements",
    "href": "slides/week10-clustering.html#announcements",
    "title": "Clustering",
    "section": "Announcements",
    "text": "Announcements\n\nLab 7 optional, turn in by end of day Friday 6/9\nHW4 due Wednesday 6/7 with late submissions by Friday 6/9\nCourse project due Friday 6/16; groups preferred, max of 3\nExtra OH Tuesday 1-3pm (scheduled final time)?"
  },
  {
    "objectID": "slides/week10-clustering.html#last-time",
    "href": "slides/week10-clustering.html#last-time",
    "title": "Clustering",
    "section": "Last time",
    "text": "Last time\nWe fit the logistic regression model:\n\\[\n\\log\\left(\\frac{Pr(\\text{diabetic}_i)}{1 - Pr(\\text{diabetic}_i)}\\right) = \\beta_0 + \\beta_1\\text{age}_i +  \\beta_2\\text{male}_i+\\beta_3\\text{BMI}_i\n\\]\nAnd discussed:\n\nmodel specification\nparameter estimation\nparameter interpretation"
  },
  {
    "objectID": "slides/week10-clustering.html#last-time-1",
    "href": "slides/week10-clustering.html#last-time-1",
    "title": "Clustering",
    "section": "Last time",
    "text": "Last time\nThe logistic regression model can be employed as a classifier by articulating a rule:\n\\[\n\\text{classify as diabetic}\n\\quad\\Longleftrightarrow\\quad\n\\widehat{Pr}(\\text{diabetic}) &gt; c\n\\]\nwhere \\(\\widehat{Pr}(\\text{diabetic})\\) is computed from the fitted logistic regression model.\n\nAny classifier has some:\n\nsensitivity (true positive rate)\nspecificity (true negative rate)\n\n\n\nThese rates vary depending on \\(c\\)."
  },
  {
    "objectID": "slides/week10-clustering.html#roc-curves",
    "href": "slides/week10-clustering.html#roc-curves",
    "title": "Clustering",
    "section": "ROC Curves",
    "text": "ROC Curves\nA plot of sensitivity against specificity across all unique classification thresholds is known as a receiver operating characteristic (ROC) curve.\n\n\n\n\nCode\n# compute \nfrom sklearn import metrics\nfpr, tpr, thresh = metrics.roc_curve(y, fitted_probs)\n\nroc = pd.DataFrame({\n    'fpr': fpr,\n    'tpr': tpr,\n    'thresh': thresh\n})\n\nroc_opt_ix = [(roc.tpr - roc.fpr).argmax(), ((1 - roc.tpr)**2 + roc.fpr**2).argmin()]\nroc_opt = roc.loc[roc_opt_ix]\n\nroc_plot = alt.Chart(roc).mark_line().encode(\n    x = alt.X('fpr', title = '1 - specificity'),\n    y = alt.Y('tpr', title = 'sensitivity')\n)\n\nroc_pts = alt.Chart(roc_opt).mark_circle(\n    fill = 'red',\n    size = 100\n).encode(\n    x = 'fpr',\n    y = 'tpr'\n)\n\n(roc_plot + roc_pts).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)\n\n\n\n\n\n\n\n\n\n\ncloser to the upper left corner \\(\\longrightarrow\\) less trade-off \\(\\longrightarrow\\) better classifier\narea under the curve often used as an accuracy metric\ntwo common choices for \\(c\\) highlighted:\n\nthe point closest to the upper left corner\nthe point that maximizes sensitivity + specificity (sometimes called Youden’s J statistic)"
  },
  {
    "objectID": "slides/week10-clustering.html#probit-regression-model",
    "href": "slides/week10-clustering.html#probit-regression-model",
    "title": "Clustering",
    "section": "Probit regression model",
    "text": "Probit regression model\nLogistic regression is not the only regression model for binary data. One common alternative is probit regression, where:\n\\[\nP(Y = 1) = \\Phi\\left(x'\\beta\\right)\n\\]\n\n\\(\\Phi\\) is the standard normal CDF\nsimilar assumptions to logistic regression – independence and monotonicity"
  },
  {
    "objectID": "slides/week10-clustering.html#probit-v.-logit",
    "href": "slides/week10-clustering.html#probit-v.-logit",
    "title": "Clustering",
    "section": "Probit v. logit",
    "text": "Probit v. logit\nThe logistic function has heavier tails.\n\n\nCode\nfrom scipy.stats import norm\n# grid of values\nvals = pd.DataFrame({\n    'x': np.linspace(-5, 5, 200)\n})\n\n# compute probit and logit\nvals['logit'] = 1/(1 + np.exp(-vals.x))\nvals['probit'] = norm.cdf(vals.x)\nvals = vals.melt(id_vars = 'x', var_name = 'model', value_name = 'pr')\n\n# plot\nalt.Chart(vals).mark_line().encode(\n    x = alt.X('x', title = 'x*beta'),\n    y = alt.Y('pr', title = 'Pr(Y = 1)'),\n    color = 'model',\n    stroke = 'model'\n).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n).configure_legend(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week10-clustering.html#probit-fit-to-diabetes-data",
    "href": "slides/week10-clustering.html#probit-fit-to-diabetes-data",
    "title": "Clustering",
    "section": "Probit fit to diabetes data",
    "text": "Probit fit to diabetes data\nHere are the estimates from the probit model:\n\n\nCode\n# fit model\nmodel_probit = sm.Probit(endog = y, exog = x)\nfit_probit = model_probit.fit()\n\n# parameter estimates\ncoef_tbl_probit = pd.DataFrame({\n    'estimate': fit_probit.params,\n    'standard error': np.sqrt(fit_probit.cov_params().values.diagonal())},\n    index = x.columns\n)\ncoef_tbl_probit\n\n\nOptimization terminated successfully.\n         Current function value: 0.214265\n         Iterations 8\n\n\n\n\n\n\n\n\n\nestimate\nstandard error\n\n\n\n\nconst\n-4.215812\n0.172291\n\n\nmale\n0.116145\n0.061036\n\n\nAge\n0.025876\n0.001733\n\n\nBMI\n0.051143\n0.004201\n\n\n\n\n\n\n\n\nTrickier to interpret coefficients directly, since \\(\\Phi^{-1}(p)\\) is not a natural quantity"
  },
  {
    "objectID": "slides/week10-clustering.html#challenges-of-interpretation",
    "href": "slides/week10-clustering.html#challenges-of-interpretation",
    "title": "Clustering",
    "section": "Challenges of interpretation",
    "text": "Challenges of interpretation\nThe effect of incremental changes in explanatory variables on predicted probabilities depends on your starting point.\n\n# means of explanatory variables\nx1 = x.mean()\n\n# increment bmi twice\nx2 = x1 + np.array([0, 0, 0, 1])\nx3 = x2 + np.array([0, 0, 0, 1])\nx_pred = pd.concat([x1, x2, x3], axis = 1).T\nx_pred['male'] = np.array([0, 0, 0])\n\n# compute predictions and differences in probability\npreds = fit_probit.predict(x_pred)\npreds.diff()\n\n0         NaN\n1    0.003587\n2    0.003941\ndtype: float64\n\n\n\na one-unit increase in BMI from 26.45 (sample mean) for a 37.6 year old woman is associated with an estimated change in probability of diabetes of 0.0036\na one-unit increase in BMI from 27.45 (sample mean plus one) for a 37.6 year old woman is associated with an estimated change in probability of diabetes of 0.0039"
  },
  {
    "objectID": "slides/week10-clustering.html#centering-for-interpretability",
    "href": "slides/week10-clustering.html#centering-for-interpretability",
    "title": "Clustering",
    "section": "Centering for interpretability",
    "text": "Centering for interpretability\nIf explanatory variables are centered, then the change in estimated probability associated with a 1-unit change from the mean (and reference level(s)) is:\n\\[\n\\Phi\\left(\\hat{\\beta}_0 + \\hat{\\beta}_j\\right) - \\Phi\\left(\\hat{\\beta}_0\\right)\n\\]\n\nRefitting the model after centering age and BMI and computing the above yields:\n\n\nCode\n# center explanatory variables\nx_vars_ctr = x_vars - x.mean()\nx_ctr = pd.concat([x.loc[:, ['const', 'male']], x_vars_ctr.loc[:, ['Age', 'BMI']]], axis = 1)\n\n# fit model\nmodel_probit_ctr = sm.Probit(endog = y, exog = x_ctr)\nfit_probit_ctr = model_probit_ctr.fit()\n\n# baseline\nprobit_baseline = norm.cdf(fit_probit_ctr.params[0])\n\n# changes in estimated probabilities associated with one-unit change from mean, keeping other variables at mean/reference\nprob_diffs = norm.cdf(fit_probit_ctr.params[1:4] + fit_probit_ctr.params[0]) - probit_baseline\n\n# print\npd.DataFrame({'change in probability': prob_diffs}, index = np.array(['male', 'age', 'BMI']))\n\n\nOptimization terminated successfully.\n         Current function value: 0.214265\n         Iterations 8\n\n\n\n\n\n\n\n\n\nchange in probability\n\n\n\n\nmale\n0.008659\n\n\nage\n0.001772\n\n\nBMI\n0.003587"
  },
  {
    "objectID": "slides/week10-clustering.html#centering-for-interpretation",
    "href": "slides/week10-clustering.html#centering-for-interpretation",
    "title": "Clustering",
    "section": "Centering for interpretation",
    "text": "Centering for interpretation\nNow the coefficent interpretations are:\n\nthe estimated probability that a woman of average age and BMI has diabetes is 0.029\n\n\\(\\Phi(\\hat{\\beta}_0)\\)\n\namong people of average age and BMI, men are more likely than women to be diabetic with an estimated difference in probability of 0.009\n\n\\(0.008659 = \\Phi(\\hat{\\beta}_0 + \\hat{\\beta}_1) - \\Phi(\\hat{\\beta}_0)\\)\n\na one-year increase from the average age is associated with a change in the estimated probability that a woman of average BMI is diabetic of 0.002\n\n\\(0.001772 = \\Phi(\\hat{\\beta}_0 + \\hat{\\beta}_2) - \\Phi(\\hat{\\beta}_0)\\)\n\na one-unit increase in BMI from the average is associated with a change in the estimated probability that a woman of average age is diabetic of 0.004\n\n\\(0.003587 = \\Phi(\\hat{\\beta}_0 + \\hat{\\beta}_3) - \\Phi(\\hat{\\beta}_0)\\)"
  },
  {
    "objectID": "slides/week10-clustering.html#unsupervised-problems",
    "href": "slides/week10-clustering.html#unsupervised-problems",
    "title": "Clustering",
    "section": "Un/Supervised problems",
    "text": "Un/Supervised problems\nRegression and classification are known as ‘supervised’ problems:\n\nthe response variable/outcome is observed\nthe modeling of data is guided by observation\n\n\nBy contrast, in ‘unsupervised’ problems:\n\nthe response variable/outcome is not observed\nno ground truth to guide/supervise the modeling process"
  },
  {
    "objectID": "slides/week10-clustering.html#clustering",
    "href": "slides/week10-clustering.html#clustering",
    "title": "Clustering",
    "section": "Clustering",
    "text": "Clustering\nClustering is the unsupervised version of classification:\n\nCan we classify observations into two or more groups based on \\(p\\) variables without knowing the true grouping structure?\n\n\ncan think of this as modeling an unobserved response\nhowever, not necessary that there exist subpopulations in the data – often a useful exploratory technique for exploring multimodal distributions"
  },
  {
    "objectID": "slides/week10-clustering.html#voting-records-116th-house",
    "href": "slides/week10-clustering.html#voting-records-116th-house",
    "title": "Clustering",
    "section": "Voting records, 116th House",
    "text": "Voting records, 116th House\nRoll call votes of the 116th House of Representatives on bills and resolutions:\n\n\nCode\nmembers = pd.read_csv('data/members.csv').set_index('name_id')\nvotes = pd.read_csv('data/votes-clean.csv').set_index('name_id')\nvote_info = pd.read_csv('data/votes-info.csv').set_index('rollcall_id')\n\nvotes.head(3)\n\n\n\n\n\n\n\n\n\n2019:118\n2019:217\n2019:240\n2019:134\n2019:099\n2019:184\n2019:011\n2019:067\n2019:168\n2019:557\n...\n2019:610\n2019:535\n2019:627\n2019:538\n2019:569\n2019:570\n2019:583\n2019:626\n2019:592\n2019:624\n\n\nname_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA000374\n-1\n-1\n-1\n-1\n-1\n0\n-1\n1\n0\n1\n...\n1\n0\n1\n0\n1\n1\n-1\n1\n-1\n-1\n\n\nA000370\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nA000055\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n1\n-1\n1\n1\n-1\n\n\n\n\n3 rows × 144 columns\n\n\n\n\neach column is a roll call (\\(p = 144\\) total)\neach row is a representative (\\(n = 430\\) total)\n1 is a “yes” vote; 0 is an abstention; -1 is a “no” vote"
  },
  {
    "objectID": "slides/week10-clustering.html#clustering-voting-data",
    "href": "slides/week10-clustering.html#clustering-voting-data",
    "title": "Clustering",
    "section": "Clustering voting data",
    "text": "Clustering voting data\n\nQuestion: Can we identify groups of representatives that voted similarly?\n\n\nCan cluster the representatives according to roll call votes\nBut how many clusters to expect?"
  },
  {
    "objectID": "slides/week10-clustering.html#eda-with-pca",
    "href": "slides/week10-clustering.html#eda-with-pca",
    "title": "Clustering",
    "section": "EDA with PCA",
    "text": "EDA with PCA\nProjecting the data onto the first few principal components provides a way to visualize the data:\n\n\n\n\nCode\npca = sm.PCA(votes)\nalt.Chart(pca.scores).mark_circle(opacity = 0.5).encode(\n    x = alt.X('comp_000', title = 'PC1'),\n    y = alt.Y('comp_001', title = 'PC2')\n).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n).configure_legend(\n    labelFontSize = 14,\n    titleFontSize = 16\n)\n\n\n\n\n\n\n\n\n\n\ncan see at least two clusters"
  },
  {
    "objectID": "slides/week10-clustering.html#clustering-with-k-means",
    "href": "slides/week10-clustering.html#clustering-with-k-means",
    "title": "Clustering",
    "section": "Clustering with \\(K\\)-means",
    "text": "Clustering with \\(K\\)-means\nThe most widely used clustering method is known as \\(K\\)-means.\n\n\n\n\n\ncluster labels are based on shortest Euclidean distance to one of \\(K\\) centers\ncenters are found by minimizing the variance within each cluster"
  },
  {
    "objectID": "slides/week10-clustering.html#clustering-with-k-means-1",
    "href": "slides/week10-clustering.html#clustering-with-k-means-1",
    "title": "Clustering",
    "section": "Clustering with \\(K\\)-means",
    "text": "Clustering with \\(K\\)-means\nTechnically, given \\(n\\) observations of \\(p\\) variables \\(\\mathbf{X} = \\{x_{ij}\\}\\), the \\(k\\)-means problem is:\n\\[\n\\text{minimize}_{C_1, \\dots, C_K} \\left\\{\n    \\sum_{k = 1}^K |C_k|^{-1} \\sum_{i, i' \\in C_k} \\sum_{j = 1}^p (x_{ij} - x_{i'j})^2\n\\right\\}\n\\]\n\nA local solution is found by starting with random cluster assignments and then interatively:\n\nUpdate the cluster centers\nReassign the cluster labels"
  },
  {
    "objectID": "slides/week10-clustering.html#clustering-voting-data-1",
    "href": "slides/week10-clustering.html#clustering-voting-data-1",
    "title": "Clustering",
    "section": "Clustering voting data",
    "text": "Clustering voting data\nThe method is very easy to implement:\n\nfrom sklearn.cluster import KMeans\nnp.random.seed(60623)\n\nclust = KMeans(n_clusters = 2)\nclust.fit(votes)\nclust_labels = clust.predict(votes)\n\n\nCluster labels will be returned in the same order as the rows input to .predict()\n\n\nInitialization is random, so solutions may differ from run to run (usually just permutes labels)."
  },
  {
    "objectID": "slides/week10-clustering.html#clustering-voting-data-2",
    "href": "slides/week10-clustering.html#clustering-voting-data-2",
    "title": "Clustering",
    "section": "Clustering voting data",
    "text": "Clustering voting data\nWe could again visualize the clusters using PCA:\n\n\nCode\nplot_df = pca.scores.iloc[:, 0:2].copy()\nplot_df['cluster'] = clust_labels\n\nalt.Chart(plot_df).mark_circle(opacity = 0.5).encode(\n    x = alt.X('comp_000', title = 'PC1'),\n    y = alt.Y('comp_001', title = 'PC2'),\n    color = 'cluster:N'\n).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n).configure_legend(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week10-clustering.html#cluster-composition-by-party",
    "href": "slides/week10-clustering.html#cluster-composition-by-party",
    "title": "Clustering",
    "section": "Cluster composition by party",
    "text": "Cluster composition by party\nWe could also cross-tabulate the cluster labels with party affiliations:\n\n\nCode\nlabel_df = pd.DataFrame({'cluster': clust_labels}, index = votes.index)\n\npd.merge(members, label_df, left_index = True, right_index = True).groupby(['current_party', 'cluster']).size().reset_index().pivot(columns = 'current_party', index = 'cluster')\n\n\n\n\n\n\n\n\n\n0\n\n\ncurrent_party\nDemocratic\nIndependent\nRepublican\n\n\ncluster\n\n\n\n\n\n\n\n0\n232.0\nNaN\n3.0\n\n\n1\nNaN\n1.0\n194.0"
  },
  {
    "objectID": "slides/week10-clustering.html#cluster-composition-by-party-1",
    "href": "slides/week10-clustering.html#cluster-composition-by-party-1",
    "title": "Clustering",
    "section": "Cluster composition by party",
    "text": "Cluster composition by party\nWho are those three representatives that vote with the democrats?\n\n\nCode\nmembers_labeled = pd.merge(members, label_df, left_index = True, right_index = True)\n\nmembers_labeled[(members_labeled.cluster == 0) & (members_labeled.current_party == 'Republican')]\n\n\n\n\n\n\n\n\n\nname\nstate\nurl\nchamber\ncurrent_party\ncommittee_assignments\ncluster\n\n\nname_id\n\n\n\n\n\n\n\n\n\n\n\nF000466\nbrian-fitzpatrick\nPennsylvania\nhttps://www.congress.gov/member/brian-fitzpatr...\nHouse\nRepublican\n['Foreign Affairs', 'Transportation and Infras...\n0\n\n\nK000386\njohn-katko\nNew York\nhttps://www.congress.gov/member/john-katko/K00...\nHouse\nRepublican\n['Homeland Security', 'Transportation and Infr...\n0\n\n\nS000522\nchristopher-smith\nNew Jersey\nhttps://www.congress.gov/member/christopher-sm...\nHouse\nRepublican\n['Foreign Affairs']\n0"
  },
  {
    "objectID": "slides/week10-clustering.html#further-possible-questions",
    "href": "slides/week10-clustering.html#further-possible-questions",
    "title": "Clustering",
    "section": "Further possible questions",
    "text": "Further possible questions\nWe could use the same technique to explore a variety of additional questions:\n\nidentify voting blocs by issue or policy area (use a subset of columns)\nfind within-party voting blocs (increase \\(K\\))\nidentify representatives that don’t tend to vote together with others (assign a score based on how ‘quickly’ a representative is isolated)"
  },
  {
    "objectID": "slides/week9-classification.html#announcements",
    "href": "slides/week9-classification.html#announcements",
    "title": "Classification",
    "section": "Announcements",
    "text": "Announcements\n\nMP2 due today; late deadline is Friday\nHW4 due next Wednesday (not Monday); late deadline is Friday\nNo OH today\nlecture cancelled Monday 6/5 for data science event"
  },
  {
    "objectID": "slides/week9-classification.html#data-science-event-65",
    "href": "slides/week9-classification.html#data-science-event-65",
    "title": "Classification",
    "section": "Data science event 6/5",
    "text": "Data science event 6/5\nLoma Pelona Center 1pm – 4pm\n\n1pm: Graduate school presentation and panel discussion with recent students\n2pm: capstone project showcase\n3pm: Careers in data science panel discussion"
  },
  {
    "objectID": "slides/week9-classification.html#from-last-time",
    "href": "slides/week9-classification.html#from-last-time",
    "title": "Classification",
    "section": "From last time",
    "text": "From last time\nWe fit this model to the tree cover data:\n\\[\n\\log(\\text{cover}_i)\n    = \\beta_0 +\n        \\beta_1 \\log(\\text{income}_i) +\n        \\beta_2 \\text{low}_i +\n        \\beta_3 \\text{med}_i +\n        \\beta_4 \\text{high}_i +\n        \\epsilon_i\n\\]\n\nEach level of population density has its own intercept:\n\\[\n\\begin{align*}\n\\text{very low density:}\\quad &\\mathbb{E}\\log(\\text{cover}) = \\beta_0 + \\beta_1 \\log(\\text{income}) \\\\\n\\text{low density:}\\quad &\\mathbb{E}\\log(\\text{cover}) = (\\beta_0 + \\beta_2) + \\beta_1 \\log(\\text{income}) \\\\\n\\text{medium density:}\\quad &\\mathbb{E}\\log(\\text{cover}) = (\\beta_0 + \\beta_3) + \\beta_1 \\log(\\text{income}) \\\\\n\\text{high density:}\\quad &\\mathbb{E}\\log(\\text{cover}) = (\\beta_0 + \\beta_4) + \\beta_1 \\log(\\text{income})\n\\end{align*}\n\\]\n\n\n\\(\\beta_2, \\beta_3, \\beta_4\\) represent the difference in expected log cover between very low density and low, medium, high density after accounting for income"
  },
  {
    "objectID": "slides/week9-classification.html#interpreting-estimates",
    "href": "slides/week9-classification.html#interpreting-estimates",
    "title": "Classification",
    "section": "Interpreting estimates",
    "text": "Interpreting estimates\n\n\n\n\n\n\n\n\n\nestimate\nstandard error\n\n\n\n\nconst\n-4.751870\n0.621673\n\n\nlog_income\n0.631469\n0.058290\n\n\nlow\n-0.414486\n0.070483\n\n\nmedium\n-0.675626\n0.079552\n\n\nhigh\n-0.606980\n0.101708\n\n\nerror variance\n1.552578\nNaN\n\n\n\n\n\n\n\n\neach doubling of mean income is associated with an estimated 55% increase in median tree cover, after accounting for population density\ncensus blocks with higher population densities are estimated as having a median tree canopy up to 50% lower than census blocks with very low population densities, after accounting for mean income"
  },
  {
    "objectID": "slides/week9-classification.html#on-log-transforming-the-response",
    "href": "slides/week9-classification.html#on-log-transforming-the-response",
    "title": "Classification",
    "section": "On log-transforming the response",
    "text": "On log-transforming the response\nThe model is \\(\\log (y) \\sim N(x\\beta, \\sigma^2)\\); so \\(y\\) is what’s known as a lognormal random variable.\n\nFrom the properties of the lognormal distribution:\n\\[\ne^{x\\beta} = \\text{median}(y)\n\\]\n\n\nSo when parameters are back-transformed, they should be interpreted in terms of the median response."
  },
  {
    "objectID": "slides/week9-classification.html#interpretations-again",
    "href": "slides/week9-classification.html#interpretations-again",
    "title": "Classification",
    "section": "Interpretations, again",
    "text": "Interpretations, again\n\\[\n55\\% \\text{ increase in median tree cover}:\ne^{\\hat{\\beta}_1\\log(2)} = 1.549\n\\]\nMedian cover increases by a factor of 1.549, i.e., increases by 54.9%:\n\ndoubling income increments log income by \\(\\log(2)\\)\n\\(\\hat{\\beta}_1\\log(2)\\) gives the associated change in mean log cover\nexponentiating the change in mean log cover gives the multiplicative change in median cover"
  },
  {
    "objectID": "slides/week9-classification.html#prediction",
    "href": "slides/week9-classification.html#prediction",
    "title": "Classification",
    "section": "Prediction",
    "text": "Prediction\n\nx_new = np.array([1, np.log(115000), 0, 1, 0])\npred = rslt.get_prediction(x_new)\nnp.exp(pred.summary_frame())\n\n\n\n\n\n\n\n\nmean\nmean_se\nmean_ci_lower\nmean_ci_upper\nobs_ci_lower\nobs_ci_upper\n\n\n\n\n0\n6.895105\n1.102304\n5.696153\n8.346419\n0.594349\n79.990893\n\n\n\n\n\n\n\nFill in the blanks:\n\nthe median tree cover for a _______ density census block with mean income _______ is estimated to be between _______ and _______ percent\nthe tree cover for a _______ density census block with mean income _______ is estimated to be between _______ and _______ percent"
  },
  {
    "objectID": "slides/week9-classification.html#model-checking",
    "href": "slides/week9-classification.html#model-checking",
    "title": "Classification",
    "section": "Model checking",
    "text": "Model checking\nThe linearity and constant variance assumptions can be assessed by plotting residuals against fitted values:\n\n\n\n\nCode\nalt.Chart(pd.DataFrame({\n    'fitted': rslt.fittedvalues,\n    'resid': rslt.resid\n})).mark_circle().encode(\n    x = 'fitted',\n    y = 'resid'\n)\n\n\n\n\n\n\n\n\n\nShould see minimal pattern:\n\ncentered at zero\neven spread in either direction"
  },
  {
    "objectID": "slides/week9-classification.html#diabetes-data",
    "href": "slides/week9-classification.html#diabetes-data",
    "title": "Classification",
    "section": "Diabetes data",
    "text": "Diabetes data\n4831 responses from the 2011-2012 National Health and Nutrition Examination Survey (NHANES):\n\n\n\n\n\n\n\n\n\nGender\nAge\nBMI\nDiabetes\n\n\n\n\n0\nmale\n14\n17.3\nNo\n\n\n1\nfemale\n43\n33.3\nNo\n\n\n2\nmale\n80\n33.9\nNo\n\n\n3\nmale\n80\n33.9\nNo\n\n\n\n\n\n\n\n\nIs BMI a risk factor for diabetes after adjusting for age and sex?"
  },
  {
    "objectID": "slides/week9-classification.html#model-sketch",
    "href": "slides/week9-classification.html#model-sketch",
    "title": "Classification",
    "section": "Model sketch",
    "text": "Model sketch\nBroadly, we can answer the question by estimating the dependence of diabetes status on age, sex, and BMI.\n\nAn additive model might look something like this: \\[\n\\text{diabetes}_i \\longleftarrow \\beta_1\\text{age}_i\\;+\\; \\beta_2\\text{male}_i\\;+\\;\\beta_3\\text{BMI}_i\n\\]\n\n\nTo answer the question, fit the model and examine \\(\\beta_3\\)."
  },
  {
    "objectID": "slides/week9-classification.html#binary-response",
    "href": "slides/week9-classification.html#binary-response",
    "title": "Classification",
    "section": "Binary response",
    "text": "Binary response\nNote that the response variable – whether the respondent has diabetes – is categorical.\n\ndiabetes.Diabetes.unique()\n\narray(['No', 'Yes'], dtype=object)\n\n\n\nWe can encode this using an indicator variable, which results in a binary response:\n\ny = (diabetes.Diabetes == 'Yes').astype('int')\ny.unique()\n\narray([0, 1])\n\n\n\n\nRemember, a statistical model is a probability distribution, so we need to choose one that’s appropriate for binary outcomes. Ideas?"
  },
  {
    "objectID": "slides/week9-classification.html#what-not-to-do",
    "href": "slides/week9-classification.html#what-not-to-do",
    "title": "Classification",
    "section": "What not to do",
    "text": "What not to do\nOne might think:\n\\[\n\\text{diabetes}_i = \\beta_0 + \\beta_1\\text{age}_i +  \\beta_2\\text{male}_i+\\beta_3\\text{BMI}_i + \\epsilon_i\n\\]\n\nBut \\(\\text{diabetes}_i \\not\\sim N(x\\beta, \\sigma^2)\\)\n\ndiscrete, not continuous\nnormal model doesn’t make sense for a binary response"
  },
  {
    "objectID": "slides/week9-classification.html#what-not-to-do-1",
    "href": "slides/week9-classification.html#what-not-to-do-1",
    "title": "Classification",
    "section": "What not to do",
    "text": "What not to do\nNote that you can still fit this model.\n\n\nCode\n# explanatory variable matrix\ngender_indicator = pd.get_dummies(diabetes.Gender, drop_first = True)\nx_vars = pd.concat([gender_indicator, diabetes.loc[:, ['Age', 'BMI']]], axis = 1)\nx = sm.add_constant(x_vars)\ny = (diabetes.Diabetes == 'Yes').astype('int')\n\n# fit model\nmlr = sm.OLS(endog = y, exog = x)\nmlr.fit().params\n\n\nconst   -0.169002\nmale     0.010837\nAge      0.002425\nBMI      0.005602\ndtype: float64\n\n\nSo you have to discern that it isn’t appropriate. A few ways to tell:\n\nparameter interpretations won’t make sense – e.g. age is associated with a 0.0024 increase in diabetes presence\nmodel may yield predictions that are negative or greater than one\nplots will look odd"
  },
  {
    "objectID": "slides/week9-classification.html#what-not-to-do-2",
    "href": "slides/week9-classification.html#what-not-to-do-2",
    "title": "Classification",
    "section": "What not to do",
    "text": "What not to do\nAttempts at model visualization will look something like this:\n\n\nCode\nage_grid = x.Age.median()\nmale_grid = np.array([1])\nbmi_grid = np.linspace(x.BMI.min(), x.BMI.max(), 100)\ncx, ax, mx, bx = np.meshgrid(np.array([1]), age_grid, male_grid, bmi_grid)\ngrid = np.array([cx.ravel(), ax.ravel(), mx.ravel(), bx.ravel()]).T\ngrid_df = pd.DataFrame(grid, columns = ['const', 'age', 'male', 'bmi'])\npred_df = mlr.fit().get_prediction(grid_df).summary_frame()\npred_df = pd.concat([grid_df, pred_df], axis = 1)\ndiabetes['diabetes_indicator'] = y\n\npoints = alt.Chart(diabetes).mark_circle().encode(\n    x = alt.X('BMI', title = 'body mass index'),\n    y = alt.Y('diabetes_indicator:Q', title = 'diabetes')\n)\n\nline = alt.Chart(pred_df).mark_line().encode(\n    x = 'bmi',\n    y = 'mean'\n)\n\n(points + line).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week9-classification.html#regression-with-a-binary-response",
    "href": "slides/week9-classification.html#regression-with-a-binary-response",
    "title": "Classification",
    "section": "Regression with a binary response",
    "text": "Regression with a binary response\nFor a binary response \\(Y \\in \\{0, 1\\}\\), we model \\(P(Y = 1)\\) as a function of the explanatory variable(s) \\(x\\):\n\\[\nP(Y = 1) = f(x)\n\\]\n\nOf course, we don’t directly observe \\(P(Y = 1)\\) – but there are various ways around this."
  },
  {
    "objectID": "slides/week9-classification.html#logistic-regression-model",
    "href": "slides/week9-classification.html#logistic-regression-model",
    "title": "Classification",
    "section": "Logistic regression model",
    "text": "Logistic regression model\nThe most common approach to modeling binary responses is logistic regression:\n\\[\n\\log\\left(\\frac{p}{1 - p}\\right) = x'\\beta\n\\]\n\n\\(p = P(Y = 1)\\)\n\\(x\\) is a vector of explanatory variable(s)\n\\(\\beta\\) is a vector of parameters\n\n\n\nThis model holds that the log odds of the outcome of interest is a linear function of the explanatory variable(s)"
  },
  {
    "objectID": "slides/week9-classification.html#logistic-regression-model-1",
    "href": "slides/week9-classification.html#logistic-regression-model-1",
    "title": "Classification",
    "section": "Logistic regression model",
    "text": "Logistic regression model\nWhat does the model imply about the probability (rather than log-odds) of the outcome of interest? \\[\n\\log\\left(\\frac{p}{1 - p}\\right) = x'\\beta\n\\quad\\Longleftrightarrow\\quad\nP(Y = 1) = \\;??\n\\]"
  },
  {
    "objectID": "slides/week9-classification.html#logistic-regression-model-2",
    "href": "slides/week9-classification.html#logistic-regression-model-2",
    "title": "Classification",
    "section": "Logistic regression model",
    "text": "Logistic regression model\nThe logistic function looks like this:\n\n\nCode\nfrom scipy.stats import norm\nvals = pd.DataFrame({\n    'x': np.linspace(-5, 5, 200)\n})\n\nvals['pr'] = 1/(1 + np.exp(-vals.x))\n\nalt.Chart(vals).mark_line().encode(\n    x = alt.X('x', title = 'x*beta'),\n    y = alt.Y('pr', title = 'Pr(Y = 1)')\n).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week9-classification.html#assumptions",
    "href": "slides/week9-classification.html#assumptions",
    "title": "Classification",
    "section": "Assumptions",
    "text": "Assumptions\nThe model makes two key assumptions:\n\nthe probability of the outcome changes monotonically with each explanatory variable\nobservations are independent (used to obtain a joint distribution)"
  },
  {
    "objectID": "slides/week9-classification.html#estimation",
    "href": "slides/week9-classification.html#estimation",
    "title": "Classification",
    "section": "Estimation",
    "text": "Estimation\nThe model is fit by maximum likelihood: find the parameters for which the observed data are most likely. The likelihood (joint distribution) is constructed from the model and the Bernoulli distribution.\n\n# fit model\nmodel = sm.Logit(endog = y, exog = x)\nfit = model.fit()\n\n# parameter estimates\ncoef_tbl = pd.DataFrame({\n    'estimate': fit.params,\n    'standard error': np.sqrt(fit.cov_params().values.diagonal())},\n    index = x.columns\n)\ncoef_tbl\n\nOptimization terminated successfully.\n         Current function value: 0.213931\n         Iterations 8\n\n\n\n\n\n\n\n\n\nestimate\nstandard error\n\n\n\n\nconst\n-8.199231\n0.357565\n\n\nmale\n0.270378\n0.117799\n\n\nAge\n0.053200\n0.003512\n\n\nBMI\n0.100607\n0.007862"
  },
  {
    "objectID": "slides/week9-classification.html#parameter-interpretations",
    "href": "slides/week9-classification.html#parameter-interpretations",
    "title": "Classification",
    "section": "Parameter interpretations",
    "text": "Parameter interpretations\nSimilar to linear regression, coefficients give the change in log-odds associated with incremental changes in the explanatory variables.\n\nOn the scale of the linear predictor:\n\nA one-unit increase in BMI is associated with an estimated 0.1 increase in log odds of diabetes after adjusting for age and sex\n\n\n\nOn the scale of the odds:\n\nA one-unit increase in BMI is associated with an estimated 10% increase in the odds of diabetes after adjusting for age and sex\n\n\n\nOn the probability scale, the increase depends on the starting value of BMI."
  },
  {
    "objectID": "slides/week9-classification.html#confidence-intervals",
    "href": "slides/week9-classification.html#confidence-intervals",
    "title": "Classification",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nOne can also give confidence intervals. These are based on large-sample approximations.\n\n# confidence intervals\nci = fit.conf_int().rename(columns = {0:'lower',1:'upper'})\nnp.exp(ci.loc['BMI'])\n\nlower    1.088933\nupper    1.123013\nName: BMI, dtype: float64\n\n\n\n\nWith 95% confidence, each 1-unit increase in BMI is associated with an estimated increase in odds of diabetes between 8.9% and 12.3% after adjusting for age and sex"
  },
  {
    "objectID": "slides/week9-classification.html#fitted-values",
    "href": "slides/week9-classification.html#fitted-values",
    "title": "Classification",
    "section": "Fitted values",
    "text": "Fitted values\nThe fitted values for logistic regression are fitted probabilities (not outcomes).\n\\[\n\\hat{p}_i = \\frac{1}{1 + e^{-x_i'\\hat{\\beta}}}\n\\]\n\nstatsmodels returns estimated log odds instead of fitted probabilities.\n\n# log odds\nfit.fittedvalues\n\n0      -5.443558\n1      -2.561428\n2      -0.262283\n3      -0.262283\n4      -5.982723\n          ...   \n4826   -3.481417\n4827   -3.481417\n4828   -3.343464\n4829   -1.970167\n4830   -1.970167\nLength: 4831, dtype: float64"
  },
  {
    "objectID": "slides/week9-classification.html#fitted-values-1",
    "href": "slides/week9-classification.html#fitted-values-1",
    "title": "Classification",
    "section": "Fitted values",
    "text": "Fitted values\nTo obtain probabilities, one could manually back-transform:\n\n# compute fitted probabilities 'by hand'\nfitted_probs = 1/(1 + np.exp(-fit.fittedvalues))\nfitted_probs.head(5)\n\n0    0.004305\n1    0.071663\n2    0.434803\n3    0.434803\n4    0.002516\ndtype: float64\n\n\n\nOr use the .predict() method:\n\n# fitted probabilities\nfit.predict(x).head(5)\n\n0    0.004305\n1    0.071663\n2    0.434803\n3    0.434803\n4    0.002516\ndtype: float64"
  },
  {
    "objectID": "slides/week9-classification.html#classification",
    "href": "slides/week9-classification.html#classification",
    "title": "Classification",
    "section": "Classification",
    "text": "Classification\nFor each observation (or new observations), probabilities can be computed directly from the fitted model:\n\\[\n\\hat{p}_i = \\frac{1}{1 + e^{-x_i'\\hat{\\beta}}}\n\\]\n\nBut what if we want to classify a person as diabetic or not diabetic? Should we declare a case when…\n\nmore probable than not: \\(\\hat{p} &gt; 0.5\\)?\nhighly probable, say \\(\\hat{p} &gt; 0.8\\)?\nsomewhat probable, say \\(\\hat{p} &gt; 0.2\\)?"
  },
  {
    "objectID": "slides/week9-classification.html#sensitivity",
    "href": "slides/week9-classification.html#sensitivity",
    "title": "Classification",
    "section": "Sensitivity",
    "text": "Sensitivity\nIf we use a low threshold for classification, say:\n\\[\n\\hat{Y} = 1\n\\quad\\Longleftrightarrow\\quad\n\\hat{p} &gt; 0.1\n\\]\n\nThen the classifications will be more sensitive to cases – most cases of diabetes will be correctly classified."
  },
  {
    "objectID": "slides/week9-classification.html#specificity",
    "href": "slides/week9-classification.html#specificity",
    "title": "Classification",
    "section": "Specificity",
    "text": "Specificity\nIf we use a high threshold instead, say:\n\\[\n\\hat{Y} = 1\n\\quad\\Longleftrightarrow\\quad\n\\hat{p} &gt; 0.9\n\\]\n\nThen the classification will not be very sensitive to cases, but they will be fairly specific – classifications will be correct for most people without diabetes."
  },
  {
    "objectID": "slides/week9-classification.html#cross-tabulation",
    "href": "slides/week9-classification.html#cross-tabulation",
    "title": "Classification",
    "section": "Cross-tabulation",
    "text": "Cross-tabulation\nFor any given classification threshold, we can cross-tabulate the classifications with the observed outcomes:\n\n# confusion matrix\nfit.pred_table(0.5)\n\narray([[4446.,   19.],\n       [ 358.,    8.]])\n\n\n\nrows are observed outcomes\ncolumns are predicted outcomes\n\n\nUsing the more-likely-than-not criterion is very specific (high true negative rate) but not at all sensitive (low true positive rate)."
  },
  {
    "objectID": "slides/week9-classification.html#overall-accuracy-is-misleading",
    "href": "slides/week9-classification.html#overall-accuracy-is-misleading",
    "title": "Classification",
    "section": "Overall accuracy is misleading",
    "text": "Overall accuracy is misleading\nThe proportion of correctly classified observations is:\n\n# proportion of correctly classified observations\nfit.pred_table().diagonal().sum()/len(y)\n\n0.9219623266404471\n\n\n\nThis looks really good, but any method that classifies all or most observations as non-diabetic will achieve high accuracy because of the case imbalance in the data.\n\n# proportion of non-diabetic respondents\nnp.mean(y == 0)\n\n0.9242392879321052"
  },
  {
    "objectID": "slides/week9-classification.html#use-class-wise-error-rates",
    "href": "slides/week9-classification.html#use-class-wise-error-rates",
    "title": "Classification",
    "section": "Use class-wise error rates",
    "text": "Use class-wise error rates\nExamining class-wise error rates reveals how asymmetric the classifications are:\n\n\nCode\n(fit.pred_table(0.5).T/fit.pred_table(0.5).sum(axis = 1)).T\n\n\narray([[0.99574468, 0.00425532],\n       [0.97814208, 0.02185792]])\n\n\n\nsame layout as confusion matrix, but with entries divided by the total number of outcomes in each class\nnote 97.8% error rate among diabetes cases"
  },
  {
    "objectID": "slides/week9-classification.html#a-better-classifier",
    "href": "slides/week9-classification.html#a-better-classifier",
    "title": "Classification",
    "section": "A better classifier",
    "text": "A better classifier\nIn this case we can do better by choosing a low classification threshold \\(\\hat{p} &gt; 0.1\\):\n\n\nCode\nfit.pred_table(0.1)\n\n\narray([[3473.,  992.],\n       [ 105.,  261.]])\n\n\n\nhigher overall error rate \\(\\frac{1097}{4831} = 0.227\\)\nbut about 70% accurate within each class (diabetic and non-diabetic)\n\n\nClass-wise errors:\n\n\nCode\n(fit.pred_table(0.1).T/fit.pred_table(0.1).sum(axis = 1)).T\n\n\narray([[0.77782755, 0.22217245],\n       [0.28688525, 0.71311475]])"
  },
  {
    "objectID": "slides/week1-lifecycle.html#whats-data-science",
    "href": "slides/week1-lifecycle.html#whats-data-science",
    "title": "Data science lifecycle",
    "section": "What’s data science?",
    "text": "What’s data science?\nData science is a term of art encompassing a wide range of activities that involve uncovering insights from quantitative information.\n\nPeople that refer to themselves as data scientists typically combine specific interests (“domain knowledge”, e.g., biology) with computation, mathematics, and statistics and probability to contribute to knowledge in their communities.\n\nIntersectional in nature\nNo singular disciplinary background among practitioners"
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-science-lifecycle",
    "href": "slides/week1-lifecycle.html#data-science-lifecycle",
    "title": "Data science lifecycle",
    "section": "Data science lifecycle",
    "text": "Data science lifecycle\n\nData science lifecycle: an end-to-end process resulting in a data analysis product\n\n\nQuestion formulation\nData collection and cleaning\nExploration\nAnalysis\n\n\nThese form a cycle in the sense that the steps are iterated for question refinement and futher discovery."
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-science-lifecylce",
    "href": "slides/week1-lifecycle.html#data-science-lifecylce",
    "title": "Data science lifecycle",
    "section": "Data science lifecylce",
    "text": "Data science lifecylce\n\n\nThe point isn’t really the exact steps, but rather the notion of an iterative process."
  },
  {
    "objectID": "slides/week1-lifecycle.html#starting-with-a-question",
    "href": "slides/week1-lifecycle.html#starting-with-a-question",
    "title": "Data science lifecycle",
    "section": "Starting with a question",
    "text": "Starting with a question\nThe scaling of brains with bodies is thought to contain clues about evolutionary patterns pertaining to intelligence.\n\nThere are lots of datasets out there with brain and body weight measurements, so let’s consider the question:\n\nWhat is the relationship between an animal’s brain and body weight?"
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-acquisition",
    "href": "slides/week1-lifecycle.html#data-acquisition",
    "title": "Data science lifecycle",
    "section": "Data acquisition",
    "text": "Data acquisition\nFrom Allison et al. 1976, average body and brain weights for 62 mammals.\n\n\n\n\n\n\n\n\n\nspecies\nbody_wt\nbrain_wt\n\n\n\n\n0\nAfricanelephant\n6654.000\n5712.0\n\n\n1\nAfricangiantpouchedrat\n1.000\n6.6\n\n\n2\nArcticFox\n3.385\n44.5\n\n\n\n\n\n\n\nUnits of measurement\n\nbody weight in kilograms\nbrain weight in grams"
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-assessment",
    "href": "slides/week1-lifecycle.html#data-assessment",
    "title": "Data science lifecycle",
    "section": "Data assessment",
    "text": "Data assessment\nHow well-matched is the data to our question?\n\nMammals only (no birds, fish, reptiles, etc.)\nSpecies are those for which convenient specimens were available\nAverages across specimens are reported (‘aggregated’ data)\n\n\nWhat do you think? Take a moment to discuss with your neighbor."
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-assessment-1",
    "href": "slides/week1-lifecycle.html#data-assessment-1",
    "title": "Data science lifecycle",
    "section": "Data assessment",
    "text": "Data assessment\nBased on the great points you just made, we really only stand to learn something about this particular sample of animals.\n\nIn other words, no inference is possible.\n\n\n\nDo you think the data are still useful?"
  },
  {
    "objectID": "slides/week1-lifecycle.html#inpection",
    "href": "slides/week1-lifecycle.html#inpection",
    "title": "Data science lifecycle",
    "section": "Inpection",
    "text": "Inpection\nThis dataset is already impeccably neat: each row is an observation for some species of mammal, and the columns are the two variables (average weight).\nSo no tidying needed – we’ll just check the dimensions and see if any values are missing.\n\n# dimensions?\nbb_weights.shape\n\n(62, 3)\n\n\n\n# missing values?\nbb_weights.isna().sum(axis = 0)\n\nspecies     0\nbody_wt     0\nbrain_wt    0\ndtype: int64"
  },
  {
    "objectID": "slides/week1-lifecycle.html#exploration",
    "href": "slides/week1-lifecycle.html#exploration",
    "title": "Data science lifecycle",
    "section": "Exploration",
    "text": "Exploration\nVisualization is usually a good starting point for exploring data.\n\n\n\n\n\n\n\nNotice the apparent density of points near \\((0, 0)\\) – that suggests we shouldn’t look for a relationship on the scale of kg/g."
  },
  {
    "objectID": "slides/week1-lifecycle.html#exploration-1",
    "href": "slides/week1-lifecycle.html#exploration-1",
    "title": "Data science lifecycle",
    "section": "Exploration",
    "text": "Exploration\nA simple transformation of the axes reveals a clearer pattern."
  },
  {
    "objectID": "slides/week1-lifecycle.html#analysis",
    "href": "slides/week1-lifecycle.html#analysis",
    "title": "Data science lifecycle",
    "section": "Analysis",
    "text": "Analysis\nThe plot shows us that there’s a roughly linear relationship on the log scale:\n\\[\\log(\\text{brain}) = \\alpha \\log(\\text{body}) + c\\]\n\nSo what does that mean in terms of brain and body weights? A little algebra and we have a “power law”:\n\\[(\\text{brain}) \\propto (\\text{body})^\\alpha\\]\n\n\nCheck your understanding: what’s the proportionality constant?"
  },
  {
    "objectID": "slides/week1-lifecycle.html#interpretation",
    "href": "slides/week1-lifecycle.html#interpretation",
    "title": "Data science lifecycle",
    "section": "Interpretation",
    "text": "Interpretation\nSo it appears that the brain-body scaling is well-described by a power law:\n\namong selected specimens of these 62 species of mammal, species average brain weight is approximately proportional to a power of species average body weight\n\n\nNotice that I did not say:\n\nanimals’ brains are proportional to a power of their bodies\namong these 62 mammals, average brain weight is approximately proportional to a power of average body weight"
  },
  {
    "objectID": "slides/week1-lifecycle.html#question-refinement",
    "href": "slides/week1-lifecycle.html#question-refinement",
    "title": "Data science lifecycle",
    "section": "Question refinement",
    "text": "Question refinement\nWe can now ask further, more specific questions:\n\nDo other types of animals exhibit the same power law relationship?\n\n\nTo investigate, we need richer data."
  },
  {
    "objectID": "slides/week1-lifecycle.html#more-data-acquisition",
    "href": "slides/week1-lifecycle.html#more-data-acquisition",
    "title": "Data science lifecycle",
    "section": "(More) data acquisition",
    "text": "(More) data acquisition\nA number of authors have compiled and published ‘meta-analysis’ datasets by combining the results of multiple studies.\nBelow we’ll import a few of these for three different animal classes.\n\n# import metaanalysis datasets\nreptiles = pd.read_csv('data/reptile_meta.csv')\nbirds = pd.read_csv('data/bird_meta.csv', encoding = 'latin1')\nmammals = pd.read_csv('data/mammal_meta.csv', encoding = 'latin1')"
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-assessment-2",
    "href": "slides/week1-lifecycle.html#data-assessment-2",
    "title": "Data science lifecycle",
    "section": "Data assessment",
    "text": "Data assessment\nWhere does this data come from? It’s kind of a convenience sample of scientific data:\n\nMultiple studies \\(\\rightarrow\\) possibly different sampling and measurement protocols\nCriteria for inclusion unknown \\(\\rightarrow\\) probably neither comprehensive nor representative of all such measurements taken\n\n\nSo these data, while richer, are still relatively narrow in terms of generalizability."
  },
  {
    "objectID": "slides/week1-lifecycle.html#a-comment-on-scope-of-inference",
    "href": "slides/week1-lifecycle.html#a-comment-on-scope-of-inference",
    "title": "Data science lifecycle",
    "section": "A comment on scope of inference",
    "text": "A comment on scope of inference\nThese data don’t support general inferences (e.g., to all animals, all mammals, etc.) because they weren’t collected for the purpose to which we’re putting them.\n\nUsually, if data are not collected for the explicit purpose of the question you’re trying to answer, they won’t constitute a representative sample."
  },
  {
    "objectID": "slides/week1-lifecycle.html#tidying",
    "href": "slides/week1-lifecycle.html#tidying",
    "title": "Data science lifecycle",
    "section": "Tidying",
    "text": "Tidying\nBack to the task at hand, in order to comine the datasets one must:\n\nSelect columns of interest;\nPut in consistent order;\nGive consistent names;\nConcatenate row-wise.\n\n\nWe’ll skip the details for now."
  },
  {
    "objectID": "slides/week1-lifecycle.html#inspection",
    "href": "slides/week1-lifecycle.html#inspection",
    "title": "Data science lifecycle",
    "section": "Inspection",
    "text": "Inspection\nThis dataset has quite a lot of missing brain weight measurements: many of the studies combined to form these datasets did not include that particular measurement.\n\n# missing values?\ndata.isna().mean(axis = 0)\n\nOrder      0.00000\nFamily     0.00000\nGenus      0.00000\nSpecies    0.00000\nSex        0.00000\nbody       0.00000\nbrain      0.57404\nclass      0.00000\ndtype: float64"
  },
  {
    "objectID": "slides/week1-lifecycle.html#exploration-2",
    "href": "slides/week1-lifecycle.html#exploration-2",
    "title": "Data science lifecycle",
    "section": "Exploration",
    "text": "Exploration\nFocusing on the nonmissing values, we see the same power law relationship but with different proportionality constants and exponents for the three classes of animals."
  },
  {
    "objectID": "slides/week1-lifecycle.html#analysis-1",
    "href": "slides/week1-lifecycle.html#analysis-1",
    "title": "Data science lifecycle",
    "section": "Analysis",
    "text": "Analysis\nSo we might hypothesize that:\n\\[\n(\\text{brain}) = \\beta_1(\\text{body})^{\\alpha_1} \\qquad \\text{(mammal)} \\\\\n(\\text{brain}) = \\beta_2(\\text{body})^{\\alpha_2} \\qquad \\text{(reptile)} \\\\\n(\\text{brain}) = \\beta_3(\\text{body})^{\\alpha_3} \\qquad \\text{(bird)} \\\\\n\\beta_i \\neq \\beta_j, \\alpha_i \\neq \\alpha_j \\quad \\text{for } i \\neq j\n\\]"
  },
  {
    "objectID": "slides/week1-lifecycle.html#interpretation-1",
    "href": "slides/week1-lifecycle.html#interpretation-1",
    "title": "Data science lifecycle",
    "section": "Interpretation",
    "text": "Interpretation\nIt seems that the average brain and body weights of the birds, mammals, and reptiles measured in these studies exhibit distinct power law relationships.\n\nWhat would you investigate next?\n\nCorrelates of body weight?\nAdjust for lifespan, habitat, predation, etc.?\nEstimate the \\(\\alpha_i\\)’s and \\(\\beta_i\\)’s?\nPredict brain weights for unobserved species?\nSomething else?"
  },
  {
    "objectID": "slides/week1-lifecycle.html#a-comment",
    "href": "slides/week1-lifecycle.html#a-comment",
    "title": "Data science lifecycle",
    "section": "A comment",
    "text": "A comment\nNotice that I did not mention the word ‘model’ anywhere!\n\nThis was intentional – it is a common misconception that analyzing data always involves fitting models.\n\nModels are not not always necessary or appropriate\nYou can learn a lot from exploratory techniques\nModels approximate specific kinds of relationships in data\nExploratory analysis can reveal unexpected structure"
  },
  {
    "objectID": "slides/week1-lifecycle.html#but-if-we-did-want-to-fit-a-model",
    "href": "slides/week1-lifecycle.html#but-if-we-did-want-to-fit-a-model",
    "title": "Data science lifecycle",
    "section": "But if we did want to fit a model…",
    "text": "But if we did want to fit a model…\n\\((\\text{brain}) = \\beta_j(\\text{body})^{\\alpha_j} \\quad \\text{animal class } j = 1, 2, 3\\)\n\nFigureEstimates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nBird\n-1.9574\n0.040\n-49.118\n0.000\n-2.036\n-1.879\n\n\nMammal\n-2.9391\n0.029\n-100.061\n0.000\n-2.997\n-2.882\n\n\nReptile\n-4.0335\n0.083\n-48.577\n0.000\n-4.196\n-3.871\n\n\nlog.body.bird\n0.5653\n0.008\n66.566\n0.000\n0.549\n0.582\n\n\nlog.body.mammal\n0.7651\n0.004\n191.544\n0.000\n0.757\n0.773\n\n\nlog.body.reptile\n0.5293\n0.017\n31.375\n0.000\n0.496\n0.562"
  },
  {
    "objectID": "slides/week1-lifecycle.html#model-limitations",
    "href": "slides/week1-lifecycle.html#model-limitations",
    "title": "Data science lifecycle",
    "section": "Model limitations",
    "text": "Model limitations\nBack to the issue of representativeness:\n\nshouldn’t use this model for inferences\nmight not be reliable for prediction either\nbut does capture/convey some suggestive comparisons\n\n\nSo, just be careful with interpretation of results:\n\n“For this particular collection of specimens, we estimated…”"
  },
  {
    "objectID": "slides/week1-lifecycle.html#zooming-out",
    "href": "slides/week1-lifecycle.html#zooming-out",
    "title": "Data science lifecycle",
    "section": "Zooming out",
    "text": "Zooming out\nThis example illustrates the aspects of the lifecylce we’ll cover in this class:\n\ndata retrieval and import\ntidying and transformation\nvisualization\nexploratory analysis\nmodeling\n\n\nWe’ll address these topics in sequence."
  },
  {
    "objectID": "slides/week1-lifecycle.html#next-week",
    "href": "slides/week1-lifecycle.html#next-week",
    "title": "Data science lifecycle",
    "section": "Next week",
    "text": "Next week\n\nTabular data structure\nData semantics\nTidy data\nTransformations of tabular data\nAggregation and grouping"
  },
  {
    "objectID": "slides/week6-pca.html#from-last-time",
    "href": "slides/week6-pca.html#from-last-time",
    "title": "Principal components",
    "section": "From last time",
    "text": "From last time\nFacts:\n\ncorrelation is a measure of (linear) dependence\ncorrelation matrices – arrays of pairwise correlations – are a common summary for multivariate data\na basis can be thought of as an alternative coordinate system\nthe eigendecomposition of a \\(p \\times p\\) correlation matrix yields a basis for \\(\\mathbb{R}^p\\) on which the standardized data are uncorrelated"
  },
  {
    "objectID": "slides/week6-pca.html#principal-components",
    "href": "slides/week6-pca.html#principal-components",
    "title": "Principal components",
    "section": "Principal components",
    "text": "Principal components\nLet \\(\\mathbf{X}\\) be an \\(n \\times p\\) data matrix and \\(\\mathbf{V}\\) be the matrix of eigenvectors of \\(\\text{corr}(\\mathbf{X})\\).\n\nThe principal components (of \\(\\mathbf{X}\\)) are the coordinates of each observation on the eigenbasis \\(\\mathbf{V}\\):\n\\[\n\\text{principal components of }\\mathbf{X} = \\mathbf{XV}\n\\]\n\n\nThe name comes from the observation that \\(\\mathbf{V}\\) gives the ‘main directions’ of variability in the data."
  },
  {
    "objectID": "slides/week6-pca.html#a-remark",
    "href": "slides/week6-pca.html#a-remark",
    "title": "Principal components",
    "section": "A remark",
    "text": "A remark\nThe eigenbasis from the correlation matrix can also be recovered from the singular value decomposition of \\(\\mathbf{Z}\\).\n\n\\[\n\\mathbf{Z} = \\mathbf{UDV'}\n\\quad\\Longrightarrow\\quad\n\\mathbf{Z'Z} = \\mathbf{V}\\underbrace{(\\mathbf{D'U'UD})}_{\\mathbf{\\Lambda}}\\mathbf{V'}\n\\]\n\n\nMost implementations use SVD, so don’t be surprised if you see this more often than eigendecomposition."
  },
  {
    "objectID": "slides/week6-pca.html#pca-in-the-low-dimensional-setting",
    "href": "slides/week6-pca.html#pca-in-the-low-dimensional-setting",
    "title": "Principal components",
    "section": "PCA in the low-dimensional setting",
    "text": "PCA in the low-dimensional setting\nLet’s consider finding the principal components for \\(p = 2\\) variables. Consider: \\[\n\\mathbf{X} = [\\mathbf{x}_1 \\; \\mathbf{x}_2] \\qquad\\text{where}\\qquad \\mathbf{x}_1 = \\text{social index} \\;\\text{and}\\; \\mathbf{x}_2 = \\text{economic index}\n\\]\n\nTo get the correlation matrix, first compute \\(\\mathbf{Z} = \\left\\{\\frac{x_i - \\bar{x}}{s_x}\\right\\}\\).\n\n\n\n\nCode\n# scatterplot of unscaled data\nraw = alt.Chart(x_mx).mark_point(opacity = 0.1, color = 'black').encode(\n    x = alt.X('Social_Domain', scale = alt.Scale(domain = [0.1, 0.8])),\n    y = alt.Y('Econ_Domain', scale = alt.Scale(domain = [0.1, 0.8]))\n).properties(width = 200, height = 200, title = 'original data (X)')\n\n# mean vector\nmean = alt.Chart(\n    pd.DataFrame(x_mx.mean()).transpose()\n).mark_circle(color = 'red', size = 100).encode(\n    x = alt.X('Social_Domain'),\n    y = alt.Y('Econ_Domain')\n)\n\n# scatterplot of centered and scaled data\ncentered = alt.Chart(z_mx).mark_point(opacity = 0.1, color = 'black').encode(\n    x = alt.X('Social_Domain'),\n    y = alt.Y('Econ_Domain')\n).properties(width = 200, height = 200, title = 'centered and scaled (Z)')\n\n# mean vector\nmean_ctr = alt.Chart(\n    pd.DataFrame(z_mx.mean()).transpose()\n).mark_circle(color = 'red', size = 100).encode(\n    x = alt.X('Social_Domain'),\n    y = alt.Y('Econ_Domain')\n)\n\n# lines at zero\naxbase = alt.Chart(\n    pd.DataFrame({'Social_Domain': 0, 'Econ_Domain': 0}, index = [0])\n)\nax1 = axbase.mark_rule().encode(x = 'Social_Domain')\nax2 = axbase.mark_rule().encode(y = 'Econ_Domain')\n\n#layer\nfig1 = (raw + mean) | (centered + mean_ctr + ax1 + ax2)\n\nfig1.configure_axis(\n    domain = False,\n    labelFontSize = 14,\n    titleFontSize = 16\n    ).configure_title(\n        fontSize = 16\n    )"
  },
  {
    "objectID": "slides/week6-pca.html#pca-in-the-low-dimensional-setting-1",
    "href": "slides/week6-pca.html#pca-in-the-low-dimensional-setting-1",
    "title": "Principal components",
    "section": "PCA in the low-dimensional setting",
    "text": "PCA in the low-dimensional setting\nNow we’ll compute the eigendecomposition.\n\n\n\nCode\nx_mx = x_mx.iloc[:, 0:2]\nz_mx = z_mx.iloc[:, 0:2]\n\n# compute correlation mx\nr_mx = z_mx.transpose().dot(z_mx)/(len(z_mx) - 1)\n\n# eigendecomposition\nr_eig = linalg.eig(r_mx.values)\n\n# show PC directions\ndirections = pd.DataFrame(r_eig[1], columns = ['PC1_Direction', 'PC2_Direction'], index = r_mx.columns)\ndirections\n\n\n\n\n\n\n\n\n\nPC1_Direction\nPC2_Direction\n\n\n\n\nEcon_Domain\n0.707107\n-0.707107\n\n\nSocial_Domain\n0.707107\n0.707107\n\n\n\n\n\n\n\n\n\nThe ‘principal component directions’ are simply the eigenvectors."
  },
  {
    "objectID": "slides/week6-pca.html#pca-in-the-low-dimensional-setting-2",
    "href": "slides/week6-pca.html#pca-in-the-low-dimensional-setting-2",
    "title": "Principal components",
    "section": "PCA in the low-dimensional setting",
    "text": "PCA in the low-dimensional setting\nLet’s plot the principal component directions on the centered and scaled data.\n\n\nCode\n# store directions as dataframe for plotting\ndirection_df = pd.DataFrame(\n    np.vstack([np.zeros(2), r_eig[1][:, 0], np.zeros(2), r_eig[1][:, 1]]),\n    columns = ['Econ_Domain', 'Social_Domain']\n).join(\n    pd.Series(np.repeat(['PC1', 'PC2'], 2), name = 'PC direction')\n)\n\n# plot directions as vectors\neigenbasis = alt.Chart(direction_df).mark_line(order = False).encode(\n    x = 'Social_Domain', \n    y = 'Econ_Domain', \n    color = alt.Color('PC direction', scale = alt.Scale(scheme = 'dark2'))\n)\n\n# combine with scatter\ncentered_plot = (centered.properties(width = 300, height = 300) + mean_ctr + ax1 + ax2)\n\n(centered_plot + eigenbasis).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n).configure_title(\n    fontSize = 16\n).configure_legend(\n    labelFontSize = 14,\n    titleFontSize = 16\n)\n\n\n\n\n\n\n\n\nWhere do these directions point relative to the scatter?"
  },
  {
    "objectID": "slides/week6-pca.html#geometry-of-pca",
    "href": "slides/week6-pca.html#geometry-of-pca",
    "title": "Principal components",
    "section": "Geometry of PCA",
    "text": "Geometry of PCA\nNow scale the directions by the corresponding eigenvalues (plotting \\(\\lambda_1\\mathbf{v}_1\\) and \\(\\lambda_2\\mathbf{v}_2\\) instead of \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\)).\n\n\n\nCode\n# scale directions by eigenvalues\ndirection_df = pd.DataFrame(\n    np.vstack([np.zeros(2), r_eig[1][:, 0]*r_eig[0][0].real, np.zeros(2), r_eig[1][:, 1]*r_eig[0][1].real]),\n    columns = ['Econ_Domain', 'Social_Domain']\n).join(\n    pd.Series(np.repeat(['PC1', 'PC2'], 2), name = 'PC direction')\n)\n\n# repeat plot\neigenbasis = alt.Chart(direction_df).mark_line(order = False).encode(\n    x = 'Social_Domain', \n    y = 'Econ_Domain', \n    color = alt.Color('PC direction', scale = alt.Scale(scheme = 'dark2'))\n)\n\n(centered_plot + eigenbasis).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n).configure_title(\n    fontSize = 16\n).configure_legend(\n    labelFontSize = 14,\n    titleFontSize = 16\n)\n\n\n\n\n\n\n\n\n\nThe principal component directions are the axes along which the data vary most, and the eigenvalues give the magnitude of that variation."
  },
  {
    "objectID": "slides/week6-pca.html#projection",
    "href": "slides/week6-pca.html#projection",
    "title": "Principal components",
    "section": "Projection",
    "text": "Projection\nSo if we wanted to look at just one quantity that captures variability in both dimensions we could:\n\nproject the data onto the first principal component direction\ntreat the projected values as a new, derived variable\n\n\n\n\n\n\nCode\n# project data onto pc directions\npcdata = z_mx.dot(r_eig[1]).rename(columns = {0: 'PC1', 1: 'PC2'})\n\n# scatterplot\nprojection = alt.Chart(pcdata).mark_point(opacity = 0.1, color = 'black').encode(\n    x = alt.X('PC1', title = '', axis = None),\n    y = 'PC2'\n).properties(\n    width = 300, \n    height = 200,\n    title = 'Projected data'\n)\n\n# layer with univariate tick plot of pc1 values\npc1 = alt.Chart(pcdata).mark_tick(color = '#1B9E77').encode(x = 'PC1').properties(width = 300)\n\n(projection & pc1).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n).configure_title(fontSize = 16)\n\n\n\n\n\n\n\n\nIs it a problem to just drop PC2?"
  },
  {
    "objectID": "slides/week6-pca.html#quantifying-variation-captureloss",
    "href": "slides/week6-pca.html#quantifying-variation-captureloss",
    "title": "Principal components",
    "section": "Quantifying variation capture/loss",
    "text": "Quantifying variation capture/loss\nTo figure out how much variation/covariation is captured and lost, we need to know how much is available in the first place.\n\nOne measure of the total variation in \\(\\mathbf{X}\\) is given by the determinant of the correlation matrix \\(\\mathbf{R}\\):\n\\[\n\\text{total variation} = \\text{det}\\left(\\mathbf{R}\\right) = \\sum_{i = 1}^p \\lambda_i\n\\]\n\n\nNow let \\(\\mathbf{y}_j = \\mathbf{Zv}_j\\) be the \\(j\\)th principal component. Its variance is:\n\\[\n\\frac{\\mathbf{y}_j'\\mathbf{y}}{n - 1} = \\frac{\\mathbf{v}_j'\\mathbf{Z'Zv}_j}{n - 1} = \\mathbf{v}_j'\\mathbf{V'\\Lambda Vv}_j = \\mathbf{e}_j'\\mathbf{\\Lambda e}_j = \\lambda_j\n\\]"
  },
  {
    "objectID": "slides/week6-pca.html#quantifying-variation-captureloss-1",
    "href": "slides/week6-pca.html#quantifying-variation-captureloss-1",
    "title": "Principal components",
    "section": "Quantifying variation capture/loss",
    "text": "Quantifying variation capture/loss\nSo the total variation is the sum of the eigenvalues, and the variance of each PC is the corresponding eigenvalue.\n\nWe can therefore define the proportion of total variation explained by the \\(j\\)th principal component as:\n\\[\n\\frac{\\lambda_j}{\\sum_{j = 1}^p \\lambda_j}\n\\]\n\n\nThis is sometimes also called the variance ratio for the \\(j\\)th principal component."
  },
  {
    "objectID": "slides/week6-pca.html#quantifying-variation-captureloss-2",
    "href": "slides/week6-pca.html#quantifying-variation-captureloss-2",
    "title": "Principal components",
    "section": "Quantifying variation capture/loss",
    "text": "Quantifying variation capture/loss\nSo in our example, the variance ratios are:\n\n# compute correlation mx\nr_mx = z_mx.transpose().dot(z_mx)/(len(z_mx) - 1)\n\n# eigendecomposition\nr_eig = linalg.eig(r_mx.values)\n\n# store eigenvalues as real array\neigenvalues = r_eig[0].real \n\n# variance ratios\neigenvalues/eigenvalues.sum() \n\narray([0.76574532, 0.23425468])\n\n\n\nthe first principal component captures 77% of total variation\nthe second captures 23% of total variation"
  },
  {
    "objectID": "slides/week6-pca.html#interpreting-principal-components",
    "href": "slides/week6-pca.html#interpreting-principal-components",
    "title": "Principal components",
    "section": "Interpreting principal components",
    "text": "Interpreting principal components\nSo we have obtained a single derived variable that captures 3/4 of total variation.\n\nBut what is the meaning of the derived variable? What does it represent in the context of the data?\n\n\nThe values of the first principal component (green ticks) are given by: \\[\n\\text{PC1}_i = \\mathbf{x}_i'\\mathbf{v}_1 = 0.7071 \\times\\text{economic}_i + 0.7071 \\times\\text{social}_i\n\\]\n\n\nSo the principal component is a linear combination of the underlying variables.\n\nthe coefficients \\((0.7071, 0.7071)\\) are known as loadings\nthe values are known as principal component scores\n\n\n\n\nIn this case, the PC1 loadings are equal; so this principal component is simply the average of the social and economic indices."
  },
  {
    "objectID": "slides/week6-pca.html#interpreting-principal-components-1",
    "href": "slides/week6-pca.html#interpreting-principal-components-1",
    "title": "Principal components",
    "section": "Interpreting principal components",
    "text": "Interpreting principal components\nThe loadings for the second component are:\n\nr_eig[1][:, 0]\n\narray([0.70710678, 0.70710678])\n\n\n\n\\[\n\\text{PC2}_i = \\mathbf{x}_i'\\mathbf{v}_1 = 0.7071 \\times\\text{social}_i - 0.7071 \\times\\text{economic}_i\n\\]\n\n\nHow would you interpret this quantity?"
  },
  {
    "objectID": "slides/week6-pca.html#packages-for-pca",
    "href": "slides/week6-pca.html#packages-for-pca",
    "title": "Principal components",
    "section": "Packages for PCA",
    "text": "Packages for PCA\nIn scikit-learn, one must preprocess the data by centering and scaling:\n\nfrom sklearn.decomposition import PCA\n\n# center and scale data\nz_mx = (x_mx - x_mx.mean())/x_mx.std()\n\n# compute principal components\npca = PCA(n_components = 2)\npca.fit(z_mx)\n\n\nCheck that the results are the same:\n\n# loadings\npca.components_\n\narray([[-0.70710678, -0.70710678],\n       [ 0.70710678, -0.70710678]])\n\n\n\n\nThe variance ratios are stored as an attribute:\n\n# variance ratios\npca.explained_variance_ratio_\n\narray([0.76574532, 0.23425468])"
  },
  {
    "objectID": "slides/week6-pca.html#packages-for-pca-1",
    "href": "slides/week6-pca.html#packages-for-pca-1",
    "title": "Principal components",
    "section": "Packages for PCA",
    "text": "Packages for PCA\nIn scikit-learn, to retrieve the scores – the coordinates on the principal axes – one must ‘transform’ input data:\n\n# get scores\npca.transform(z_mx)\n\narray([[-1.29591927,  0.32681298],\n       [ 0.98357823,  0.34852919],\n       [ 0.93478265,  1.17632266],\n       ...,\n       [ 0.61009199, -0.73354446],\n       [-0.13641772, -0.34803313],\n       [ 3.44303576, -0.86984466]])\n\n\n\nuseful if you want to compute PC’s from one dataset and then apply the transformation to a new collection of observations\ninconvenient if all you want are the scores of the input data"
  },
  {
    "objectID": "slides/week6-pca.html#packages-for-pca-2",
    "href": "slides/week6-pca.html#packages-for-pca-2",
    "title": "Principal components",
    "section": "Packages for PCA",
    "text": "Packages for PCA\nIn statsmodels, the implementation is a bit more streamlined:\n\nfrom statsmodels.multivariate.pca import PCA\npca = PCA(x_mx, normalize = False, standardize = True)\npca.loadings\n\n\n\n\n\n\n\n\ncomp_0\ncomp_1\n\n\n\n\nEcon_Domain\n-0.707107\n-0.707107\n\n\nSocial_Domain\n-0.707107\n0.707107\n\n\n\n\n\n\n\n\ninput here is x_mx – no need to preprocess\nstandardize = True will center and scale the input data x_mx\nnormalize = True rescales the scores (not the input data)\nsee documentation for additional control arguments"
  },
  {
    "objectID": "slides/week6-pca.html#packages-for-pca-3",
    "href": "slides/week6-pca.html#packages-for-pca-3",
    "title": "Principal components",
    "section": "Packages for PCA",
    "text": "Packages for PCA\nFor this implementation the scores of the input data are retained as an attribute .scores:\n\npca.scores.head(3)\n\n\n\n\n\n\n\n\ncomp_0\ncomp_1\n\n\n\n\n0\n-1.296614\n-0.326988\n\n\n1\n0.984106\n-0.348716\n\n\n2\n0.935284\n-1.176954\n\n\n\n\n\n\n\n\nThe variance ratios, however, must be calculated manually:\n\npca.eigenvals/pca.eigenvals.sum()\n\n0    0.765745\n1    0.234255\nName: eigenvals, dtype: float64"
  },
  {
    "objectID": "slides/week6-pca.html#reconstruction-and-approximation",
    "href": "slides/week6-pca.html#reconstruction-and-approximation",
    "title": "Principal components",
    "section": "Reconstruction and approximation",
    "text": "Reconstruction and approximation\nInterestingly, statsmodels has a .project() method, which will compute a reconstruction of the original data from a specified number of PC’s.\n\nIf we use both PC’s…\n\npca.project(ncomp = 2).head(3)\n\n\n\n\n\n\n\n\nEcon_Domain\nSocial_Domain\n\n\n\n\n0\n0.565264\n0.591259\n\n\n1\n0.427671\n0.520744\n\n\n2\n0.481092\n0.496874\n\n\n\n\n\n\n\nWe get the input data back again:\n\nx_mx.head(3)\n\n\n\n\n\n\n\n\nEcon_Domain\nSocial_Domain\n\n\n\n\n0\n0.565264\n0.591259\n\n\n1\n0.427671\n0.520744\n\n\n2\n0.481092\n0.496874"
  },
  {
    "objectID": "slides/week6-pca.html#reconstruction-and-approximation-1",
    "href": "slides/week6-pca.html#reconstruction-and-approximation-1",
    "title": "Principal components",
    "section": "Reconstruction and approximation",
    "text": "Reconstruction and approximation\nIf we use only one PC, we get a low-dimensional approximation:\n\npca.project(ncomp = 1).head(3)\n\n\n\n\n\n\n\n\nEcon_Domain\nSocial_Domain\n\n\n\n\n0\n0.545347\n0.601274\n\n\n1\n0.406431\n0.531424\n\n\n2\n0.409404\n0.532919\n\n\n\n\n\n\n\n\nCompare with input data:\n\nx_mx.head(3)\n\n\n\n\n\n\n\n\nEcon_Domain\nSocial_Domain\n\n\n\n\n0\n0.565264\n0.591259\n\n\n1\n0.427671\n0.520744\n\n\n2\n0.481092\n0.496874"
  },
  {
    "objectID": "slides/week6-pca.html#effect-of-standardization",
    "href": "slides/week6-pca.html#effect-of-standardization",
    "title": "Principal components",
    "section": "Effect of standardization",
    "text": "Effect of standardization\nIf the data are not standardized, then the covariance matrix is decomposed instead of the correlation matrix.\n\n# refit without standardization\npca_unscaled = PCA(x_mx, standardize = False, normalize = False)\n\n# examine\nprint('variance ratios: ', pca_unscaled.eigenvals.values/pca_unscaled.eigenvals.sum())\npca_unscaled.loadings\n\nvariance ratios:  [0.86663761 0.13336239]\n\n\n\n\n\n\n\n\n\ncomp_0\ncomp_1\n\n\n\n\nEcon_Domain\n-0.952189\n-0.305509\n\n\nSocial_Domain\n-0.305509\n0.952189\n\n\n\n\n\n\n\n\nThe economic index gets upweighted significantly\nThe first component captures more variance"
  },
  {
    "objectID": "slides/week6-pca.html#effect-of-standardization-1",
    "href": "slides/week6-pca.html#effect-of-standardization-1",
    "title": "Principal components",
    "section": "Effect of standardization",
    "text": "Effect of standardization\nWhen the data are not standardized, the method is susceptible to problems of scale.\nIn our example, the economic index became more ‘important’, but this is only because it has a larger variance:\n\nx_mx.cov()\n\n\n\n\n\n\n\n\nEcon_Domain\nSocial_Domain\n\n\n\n\nEcon_Domain\n0.007428\n0.001985\n\n\nSocial_Domain\n0.001985\n0.001878\n\n\n\n\n\n\n\n\nSo if the covariance (not correlation) matrix is decomposed, economic sustainability accounts for more of the total variation measure, because it dominates the variance-covariance matrix, but only because of scale."
  },
  {
    "objectID": "slides/week6-pca.html#effect-of-standardization-2",
    "href": "slides/week6-pca.html#effect-of-standardization-2",
    "title": "Principal components",
    "section": "Effect of standardization",
    "text": "Effect of standardization\nVisually, here is the difference.\n\n\nCode\ndirection_df = pd.DataFrame(\n    np.vstack([np.zeros(2), r_eig[1][:, 0]*r_eig[0][0].real, np.zeros(2), r_eig[1][:, 1]*r_eig[0][1].real]),\n    columns = ['Econ_Domain', 'Social_Domain']\n).join(\n    pd.Series(np.repeat(['PC1', 'PC2'], 2), name = 'PC direction')\n)\n\n# repeat plot\neigenbasis = alt.Chart(direction_df).mark_line(order = False).encode(\n    x = 'Social_Domain', \n    y = 'Econ_Domain', \n    color = alt.Color('PC direction', scale = alt.Scale(scheme = 'dark2'))\n)\n\nc_eig = linalg.eig(x_mx.cov())\nc_eigenval_std = c_eig[0].real/c_eig[0].real.sum()\n\ndirection_df_cov = pd.DataFrame(\n    30*np.vstack([np.zeros(2), c_eig[1][:, 0]*c_eig[0][0].real, np.zeros(2), c_eig[1][:, 1]*c_eig[0][1].real]) + x_mx.mean().values,\n    columns = ['Econ_Domain', 'Social_Domain']\n).join(\n    pd.Series(np.repeat(['PC1', 'PC2'], 2), name = 'PC direction')\n)\n\neigenbasis_cov = alt.Chart(direction_df_cov).mark_line(order = False).encode(\n    x = 'Social_Domain', \n    y = 'Econ_Domain', \n    color = alt.Color('PC direction', scale = alt.Scale(scheme = 'dark2'))\n)\n\n(raw + eigenbasis_cov | centered + eigenbasis).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n).configure_title(\n    fontSize = 16\n).configure_legend(\n    labelFontSize = 14,\n    titleFontSize = 16\n)\n\n\n\n\n\n\n\n\nPrincipal axes are sensitive to scale\nIf not standardized, the principal components will upweight variables with larger variances"
  },
  {
    "objectID": "slides/week6-pca.html#terminology",
    "href": "slides/week6-pca.html#terminology",
    "title": "Principal components",
    "section": "Terminology",
    "text": "Terminology\nA quick review of PCA terminology:\n\nthe principal components are the eigenbasis vectors; the principal axes or directions\nthe loadings are the eigenvectors; these correspond to the principal axes or principal directions\nthe scores are the data values projected onto the principal axes\n\nsome refer to scores as principal components instead\n\nthe variance ratios are the proportions of total variance explained by each component/direction"
  },
  {
    "objectID": "slides/week6-pca.html#pca-in-higher-dimensions",
    "href": "slides/week6-pca.html#pca-in-higher-dimensions",
    "title": "Principal components",
    "section": "PCA in higher dimensions",
    "text": "PCA in higher dimensions\nPCA is rarely used for bivariate data; it is a multivariate technique.\n\nThe technique is this:\n\ncompute all principal components\nexamine the variance ratios\nselect a subset of components that collectively explain ‘enough’ variance"
  },
  {
    "objectID": "slides/week6-pca.html#pca-in-higher-dimensions-1",
    "href": "slides/week6-pca.html#pca-in-higher-dimensions-1",
    "title": "Principal components",
    "section": "PCA in higher dimensions",
    "text": "PCA in higher dimensions\nNow that we’ve reviewed the basic technique, we can consider cases with a larger number of variables.\nThis is really where PCA is most useful.\n\nIt can help answer the question: which variables are driving variation in the data?\nIt can help reduce data dimensions by finding combinations of variables that preserve variation.\nIt can provide a means of visualizing high-dimensional data."
  },
  {
    "objectID": "slides/week6-pca.html#example-application",
    "href": "slides/week6-pca.html#example-application",
    "title": "Principal components",
    "section": "Example application",
    "text": "Example application\nSo, let’s look at a different example: world development indicators.\n\n\nCode\nwdi = pd.read_csv('data/wdi-data.csv').iloc[:, 2:].set_index('country')\nwdi.head(3)\n\n\n\n\n\n\n\n\n\ninequality_adjusted_life\ninequality_adjusted_education\nmaternal_mortality\nparliament_pct_women\nlabor_participation_women\nlabor_participation_men\ntotal_pop\nurban_pct_pop\npop_under5\npop_15to64\n...\ntotal_unemployment\nyouth_unemployment\nprison_5yr\ntrade\nforeign_investment\nnet_migration\nimmigrant_pop\ntourists_2018\ninternet_users_2018\nmobile_users_2018\n\n\ncountry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNorway\n0.931\n0.908\n2\n40.8\n60.4\n67.2\n5.4\n82.6\n0.055556\n0.648148\n...\n3.3\n9.3\n80\n72.1\n0.4\n5.3\n16.1\n5688\n96.5\n107.2\n\n\nIreland\n0.926\n0.892\n5\n24.3\n56.0\n68.4\n4.9\n63.4\n0.061224\n0.653061\n...\n4.9\n13.1\n82\n239.2\n-20.4\n4.9\n17.1\n10926\n84.5\n103.2\n\n\nSwitzerland\n0.947\n0.883\n5\n38.6\n62.9\n73.8\n8.6\n73.8\n0.046512\n0.662791\n...\n4.6\n7.4\n77\n119.4\n-2.6\n6.1\n29.9\n10362\n89.7\n129.6\n\n\n\n\n3 rows × 31 columns"
  },
  {
    "objectID": "slides/week6-pca.html#computation-via-sklearn",
    "href": "slides/week6-pca.html#computation-via-sklearn",
    "title": "Principal components",
    "section": "Computation via sklearn",
    "text": "Computation via sklearn\nRecall that this implementation requires centering and scaling the data first ‘by hand’.\n\nfrom sklearn.decomposition import PCA\n\n# center and scale\nwdi_ctr = (wdi - wdi.mean())/wdi.std()\n\n# compute principal components\npca = PCA(31)\npca.fit(wdi_ctr)"
  },
  {
    "objectID": "slides/week6-pca.html#variance-ratios",
    "href": "slides/week6-pca.html#variance-ratios",
    "title": "Principal components",
    "section": "Variance ratios",
    "text": "Variance ratios\nSelecting the number of principal components to use is somewhat subjective, but always based on the variance ratios and their cumulative sum:\n\n\n\nCode\n# store proportion of variance explained as a dataframe\npcvars = pd.DataFrame({'Proportion of variance explained': pca.explained_variance_ratio_})\n\n# add component number as a new column\npcvars['Component'] = np.arange(1, 32)\n\n# add cumulative variance explained as a new column\npcvars['Cumulative variance explained'] = pcvars.iloc[:, 0].cumsum(axis = 0)\n\n# encode component axis only as base layer\nbase = alt.Chart(pcvars).encode(\n    x = 'Component')\n\n# make a base layer for the proportion of variance explained\nprop_var_base = base.encode(\n    y = alt.Y('Proportion of variance explained',\n              axis = alt.Axis(titleColor = '#57A44C'))\n)\n\n# make a base layer for the cumulative variance explained\ncum_var_base = base.encode(\n    y = alt.Y('Cumulative variance explained', axis = alt.Axis(titleColor = '#5276A7'))\n)\n\n# add points and lines to each base layer\nline = alt.Chart(pd.DataFrame({'Component': [2.5]})).mark_rule(opacity = 0.3, color = 'red').encode(x = 'Component')\nprop_var = prop_var_base.mark_line(stroke = '#57A44C') + prop_var_base.mark_point(color = '#57A44C') + line\ncum_var = cum_var_base.mark_line() + cum_var_base.mark_point() + line\n\n# layer the layers\nvariance_plot = alt.layer(prop_var, cum_var).resolve_scale(y = 'independent') \n\nvariance_plot.properties(height = 200, width = 400).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)\n\n\n\n\n\n\n\n\n\nIn this case, the variance ratios drop off after 2 components. These first two capture about half of total variation in the data."
  },
  {
    "objectID": "slides/week6-pca.html#loading-plots",
    "href": "slides/week6-pca.html#loading-plots",
    "title": "Principal components",
    "section": "Loading plots",
    "text": "Loading plots\nExamining the loadings graphically can help to interpret the components.\n\n\n\nCode\n# store the loadings as a data frame with appropriate names\nloading_df = pd.DataFrame(pca.components_).transpose().rename(\n    columns = {0: 'PC1', 1: 'PC2'}\n).loc[:, ['PC1', 'PC2']]\n\n# add a column with the taxon names\nloading_df['indicator'] = wdi_ctr.columns.values\n\n# melt from wide to long\nloading_plot_df = loading_df.melt(\n    id_vars = 'indicator',\n    var_name = 'PC',\n    value_name = 'Loading'\n)\n\n# create base layer with encoding\nbase = alt.Chart(loading_plot_df).encode(\n    y = alt.X('indicator', title = ''),\n    x = 'Loading',\n    color = 'PC'\n)\n\n# store horizontal line at zero\nrule = alt.Chart(pd.DataFrame({'Loading': 0}, index = [0])).mark_rule().encode(x = 'Loading', size = alt.value(2))\n\n# layer points + lines + rule to construct loading plot\nloading_plot = base.mark_point() + base.mark_line() + rule\n\nloading_plot.properties(width = 300, height = 400).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week6-pca.html#visualization",
    "href": "slides/week6-pca.html#visualization",
    "title": "Principal components",
    "section": "Visualization",
    "text": "Visualization\nFinally, we might want to use the first two principal components to visualize the data.\n\n\n\nCode\n# project pcdata onto first two components; store as data frame\nprojected_data = pd.DataFrame(pca.fit_transform(wdi_ctr)).iloc[:, 0:2].rename(columns = {0: 'PC1', 1: 'PC2'})\n\n# add index and reset\nprojected_data.index = wdi_ctr.index\nprojected_data = projected_data.reset_index()\n\n# append one of the original variables\nprojected_data['gdppc'] = wdi.gdp_percapita.values\nprojected_data['pop'] = wdi.total_pop.values\n\n# base chart\nbase = alt.Chart(projected_data)\n\n# data scatter\nscatter = base.mark_point().encode(\n    x = alt.X('PC1:Q', title = 'Mortality'),\n    y = alt.Y('PC2:Q', title = 'Labor'),\n    color = alt.Color('gdppc', \n                      bin = alt.Bin(maxbins = 6), \n                      scale = alt.Scale(scheme = 'blues'), \n                      title = 'GDP per capita'),\n    size = alt.Size('pop',\n                   scale = alt.Scale(type = 'sqrt'),\n                   title = 'Population')\n).properties(width = 400, height = 400)\n\n# text labels\nlabel = projected_data.sort_values(by = 'gdppc', ascending = False).head(4)\n\ntext = alt.Chart(label).mark_text(align = 'left', dx = 3).encode(\n     x = alt.X('PC1:Q', title = 'Mortality'),\n    y = alt.Y('PC2:Q', title = 'Labor'),\n    text = 'country'\n)\n\n(scatter + text).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)\n\n\n\n\n\n\n\n\nOften it’s helpful to merge the principal components with the original data and apply visualization techniques you already know to search for interesting patterns."
  },
  {
    "objectID": "slides/week6-pca.html#perspectives-on-pca",
    "href": "slides/week6-pca.html#perspectives-on-pca",
    "title": "Principal components",
    "section": "Perspectives on PCA",
    "text": "Perspectives on PCA\nNow that you know that a subcollection of principal components is usually selected, you have a sense that PCA involves projecting multivariate data onto a subspace.\n\nThere are two main perspectives on the meaning of this:\n\nLow-rank approximation of the correlation structure\n\nThis is the perspective we’ve taken in this class\n\nLatent regression or optimization problem\n\nFind the axes that maximize variance of projected data"
  },
  {
    "objectID": "slides/week6-pca.html#other-applications-of-pca",
    "href": "slides/week6-pca.html#other-applications-of-pca",
    "title": "Principal components",
    "section": "Other applications of PCA",
    "text": "Other applications of PCA\nPCA can also be used as a filtering technique for image data.\n\n\n\n\nNoisy handwritten digits\n\n\n\n\n\n\n\nReconstruction from 12 PC’s computed from the pixel values."
  },
  {
    "objectID": "slides/week6-pca.html#other-applications-of-pca-1",
    "href": "slides/week6-pca.html#other-applications-of-pca-1",
    "title": "Principal components",
    "section": "Other applications of PCA",
    "text": "Other applications of PCA\nIn a similar vein, it can be used as a compression technique.\n\n\n\nFull-size images are roughly 3K pixels\nProjecting pixel values onto 150 PC’s and then reconstructing the data from this subset yields a heavily compressed image"
  },
  {
    "objectID": "slides/week8-prediction.html#from-last-week",
    "href": "slides/week8-prediction.html#from-last-week",
    "title": "Prediction",
    "section": "From last week",
    "text": "From last week\nSimple linear regression model: \\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\quad\n\\begin{cases}\ni = 1, \\dots, n \\\\\n\\epsilon_i \\sim N\\left(0,\\sigma^2\\right)\n\\end{cases}\n\\]\n\nestimation of \\(\\beta_0, \\beta_1, \\sigma^2\\)\nstandard errors for the coefficients\nparameter interpretations\n\n\nExample using the SEDA data:\n\\[\n\\left(\\text{math gap}\\right)_i = \\beta_0 + \\beta_1 \\log\\left(\\text{median income}\\right)_i + \\epsilon_i\n\\quad\n\\text{district } i = 1, \\dots, 625\n\\]\n\n\\(\\hat{\\beta_0} = -1.356, \\hat{\\beta_1} = 0.121, \\hat{\\sigma}^2 = 0.0132\\)\ncentering the explanatory variable helped with interpretability"
  },
  {
    "objectID": "slides/week8-prediction.html#where-we-left-off",
    "href": "slides/week8-prediction.html#where-we-left-off",
    "title": "Prediction",
    "section": "Where we left off",
    "text": "Where we left off\n\n\n\n\nUncertainty bands are a common visualization technique for conveying variability in estimates.\n\nconstructed using standard errors\nbut how?"
  },
  {
    "objectID": "slides/week8-prediction.html#predictions",
    "href": "slides/week8-prediction.html#predictions",
    "title": "Prediction",
    "section": "Predictions",
    "text": "Predictions\nThe line we are visualizing shows model predictions across the range of the data:\n\\[\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\n\\;,\\quad\nx \\in [x_\\text{min}, x_\\text{max}]\n\\]\n\nsame calculation as for the fitted values\nbut predictions because we didn’t fit the model to \\(x \\in [x_\\text{min}, x_\\text{max}]\\)"
  },
  {
    "objectID": "slides/week8-prediction.html#predictions-1",
    "href": "slides/week8-prediction.html#predictions-1",
    "title": "Prediction",
    "section": "Predictions",
    "text": "Predictions\nIf for example, we’d like to make a prediction for a district with a median income of 86K, we’d calculate: \\[\n\\hat{\\beta}_0 + \\hat{\\beta}_1\\log(86000)\n\\]\n\nIn statsmodels, there is a .predict method to perform this basic calculation:\n\n# prediction\nnewobs = np.array([1, np.log(86000)])\nslr.fit().predict(newobs)\n\narray([0.01929165])\n\n\n\n\nNote that this returns a numpy array. The uncertainty bands are intervals constructed around such predictions for a fine grid of values on the interval \\([x_\\text{min}, x_\\text{max}]\\)."
  },
  {
    "objectID": "slides/week8-prediction.html#two-types-of-prediction",
    "href": "slides/week8-prediction.html#two-types-of-prediction",
    "title": "Prediction",
    "section": "Two types of prediction",
    "text": "Two types of prediction\nThe prediction \\(\\hat{y}\\) is an estimate, but what of?\n\nTwo options:\n\nestimate of the mean at \\(x\\): \\[\n\\mathbb{E}y = \\beta_0 + \\beta_1 x\n\\]\nestimate of an observation at \\(x\\) \\[\ny = \\beta_0 + \\beta_1 x + \\epsilon\n\\]"
  },
  {
    "objectID": "slides/week8-prediction.html#standard-error-for-the-mean",
    "href": "slides/week8-prediction.html#standard-error-for-the-mean",
    "title": "Prediction",
    "section": "Standard error for the mean",
    "text": "Standard error for the mean\nIf we interpret \\(\\hat{y}\\) as a prediction for the mean, the standard error is based on: \\[\n\\text{var}\\left(\\hat{\\beta}_0 + \\hat{\\beta}_1 x\\right)\n= \\text{var}\\hat{\\beta}_0 + x^2 \\text{var}\\hat{\\beta}_1 + 2x\\text{cov}\\left(\\hat{\\beta}_0, \\hat{\\beta}_1\\right)\n\\]\n\nTo get the standard error (estimated standard deviation), substitute \\(\\hat{\\sigma}^2\\) for \\(\\sigma^2\\) and take the square root."
  },
  {
    "objectID": "slides/week8-prediction.html#standard-error-for-the-observation",
    "href": "slides/week8-prediction.html#standard-error-for-the-observation",
    "title": "Prediction",
    "section": "Standard error for the observation",
    "text": "Standard error for the observation\nIf instead we interpret \\(\\hat{y}\\) as a prediction for a new observation, the standard error is based on:\n\\[\n\\text{var}\\left(\\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\epsilon\\right)\n= \\text{var}\\hat{\\beta}_0 + x^2 \\text{var}\\hat{\\beta}_1 + 2x\\text{cov}\\left(\\hat{\\beta}_0, \\hat{\\beta}_1\\right) + \\text{var}\\epsilon\n\\]\nThe above holds because we assume the future random error is independent of current observations (and thus the estimates).\n\nTo get the standard error (estimated standard deviation), substitute \\(\\hat{\\sigma}^2\\) for \\(\\sigma^2\\) and take the square root."
  },
  {
    "objectID": "slides/week8-prediction.html#computing-standard-errors",
    "href": "slides/week8-prediction.html#computing-standard-errors",
    "title": "Prediction",
    "section": "Computing standard errors",
    "text": "Computing standard errors\nThe .get_prediction() returns a prediction object with several attributes, including standard errors. This is recommended over .predict() for most purposes.\n\npred = slr.fit().get_prediction(newobs)\npred\n\n&lt;statsmodels.regression._prediction.PredictionResults at 0x280b5af90&gt;\n\n\n\nThe value of the prediction is stored as .predicted_mean:\n\npred.predicted_mean\n\narray([0.01929165])"
  },
  {
    "objectID": "slides/week8-prediction.html#standard-errors-in-matrix-form",
    "href": "slides/week8-prediction.html#standard-errors-in-matrix-form",
    "title": "Prediction",
    "section": "Standard errors, in matrix form",
    "text": "Standard errors, in matrix form\nThe variance of the estimated mean at \\(\\mathbf{x}_{new}\\) is: \\[\n\\text{var}(\\widehat{\\mathbb{E}y_\\text{new}}) = \\mathbf{x}_\\text{new} \\;\n\\underbrace{\n    \\left[\\sigma^2 (\\mathbf{X'X})^{-1}\\right]\n    }_{\n    \\text{var}\\hat{\\beta}\n    }\\;\n\\mathbf{x}_\\text{new}'\n\\]\n\nSo the standard error is obtained by substituting \\(\\hat{\\sigma}\\) for \\(\\sigma\\): \\[\nSE\\left(\\widehat{\\mathbb{E}y_\\text{new}}\\right) = \\sqrt{\\hat{\\sigma}^2\\left(\\mathbf{x}_\\text{new}'(\\mathbf{X'X})^{-1}\\mathbf{x}_\\text{new}\\right)}\n\\]\n\n\nThis value is stored with predictions as the .se_mean attribute:\n\npred.se_mean\n\narray([0.00605441])"
  },
  {
    "objectID": "slides/week8-prediction.html#standard-errors-in-matrix-form-1",
    "href": "slides/week8-prediction.html#standard-errors-in-matrix-form-1",
    "title": "Prediction",
    "section": "Standard errors, in matrix form",
    "text": "Standard errors, in matrix form\nThe variance of a predicted observation at \\(\\mathbf{x}_{new}\\) is: \\[\n\\text{var}(y_\\text{new}) = \\underbrace{\n    \\mathbf{x}_\\text{new}\n    \\left[\\hat{\\sigma}^2 (\\mathbf{X'X})^{-1}\\right] \\mathbf{x}_\\text{new}'\n}_{SE^2\\left(\\widehat{\\mathbb{E}y_\\text{new}}\\right)}\n+ \\hat{\\sigma}^2\n\\]\n\nSo the standard error is obtained by substituting \\(\\hat{\\sigma}\\) for \\(\\sigma\\): \\[\nSE\\left(y_\\text{new}\\right) = \\sqrt{\\hat{\\sigma}^2\\left(1 + \\mathbf{x}_\\text{new}'(\\mathbf{X'X})^{-1}\\mathbf{x}_\\text{new}\\right)}\n\\]\n\n\nThis value is stored with predictions as the .se_obs attribute:\n\npred.se_obs\n\narray([0.11492531])"
  },
  {
    "objectID": "slides/week8-prediction.html#comparing-standard-errors",
    "href": "slides/week8-prediction.html#comparing-standard-errors",
    "title": "Prediction",
    "section": "Comparing standard errors",
    "text": "Comparing standard errors\nNote the substantial difference between the standard error for the mean and that for the observation:\n\nprint('standard error for mean: ', pred.se_mean)\nprint('standard error for observation: ', pred.se_obs)\n\nstandard error for mean:  [0.00605441]\nstandard error for observation:  [0.11492531]\n\n\nWhy is one so much larger??"
  },
  {
    "objectID": "slides/week8-prediction.html#sources-of-variability",
    "href": "slides/week8-prediction.html#sources-of-variability",
    "title": "Prediction",
    "section": "Sources of variability",
    "text": "Sources of variability\nThere is only one source of variability for a predicted mean:\n\nvariability of the estimates \\(\\hat{\\beta}_j\\)\n\n\nThere are two sources of variability for a predicted observation:\n\nvariability of the estimates \\(\\hat{\\beta}_j\\)\nvariability of the random error \\(\\epsilon\\)"
  },
  {
    "objectID": "slides/week8-prediction.html#interpreting-standard-errors",
    "href": "slides/week8-prediction.html#interpreting-standard-errors",
    "title": "Prediction",
    "section": "Interpreting standard errors",
    "text": "Interpreting standard errors\nThe standard error for the mean is smaller, because it’s easier to estimate the average value for a given \\(x\\) than the exact value:\n\n(less uncertainty) the typical achievement gap for a district with a given median income\n(more uncertainty) the precise achievement gap for such-and-such district\n\n\nThe two standard errors will differ substantially whenever there is a lot of unexplained variability (\\(low R^2\\))."
  },
  {
    "objectID": "slides/week8-prediction.html#uncertainty-bands",
    "href": "slides/week8-prediction.html#uncertainty-bands",
    "title": "Prediction",
    "section": "Uncertainty bands",
    "text": "Uncertainty bands\nThe uncertainty bands from before were \\(\\hat{y} \\pm 2SE(\\hat{y})\\)\n\nTechnically, a confidence interval; usually shown for the mean (not the observation).\n\n\nCode\n# compute predictions at observed values\npreds = slr.fit().get_prediction(x)\n\n# append fitted values to observations\nregdata['fitted'] = preds.predicted_mean\n\n# add interval bounds\nregdata['lwr_mean'] = preds.predicted_mean - 2*preds.se_mean\nregdata['upr_mean'] = preds.predicted_mean + 2*preds.se_mean\n\nscatter = alt.Chart(regdata).mark_circle().encode(\n    x = alt.X('log_income', \n        scale = alt.Scale(zero = False),\n        title = 'log median income'),\n    y = alt.Y('gap', title = 'math achievement gap')\n)\n\ntrend = alt.Chart(regdata).mark_line(\n    color = 'black'\n).encode(\n    x = 'log_income',\n    y = 'fitted'\n)\n\nband = alt.Chart(regdata).mark_area(\n    opacity = 0.3, color = 'black'\n).encode(\n    x = 'log_income',\n    y = 'upr_mean',\n    y2 = 'lwr_mean'\n)\n\n(scatter + trend + band).configure_axis(\n    labelFontSize = 16,\n    titleFontSize = 18\n)"
  },
  {
    "objectID": "slides/week8-prediction.html#prediction-bands",
    "href": "slides/week8-prediction.html#prediction-bands",
    "title": "Prediction",
    "section": "Prediction bands",
    "text": "Prediction bands\nThe same figure constructed using the standard error for observations is:\n\n\nCode\n# add interval bounds\nregdata['lwr_obs'] = preds.predicted_mean - 2*preds.se_obs\nregdata['upr_obs'] = preds.predicted_mean + 2*preds.se_obs\n\nscatter = alt.Chart(regdata).mark_circle().encode(\n    x = alt.X('log_income', \n        scale = alt.Scale(zero = False),\n        title = 'log median income'),\n    y = alt.Y('gap', title = 'math achievement gap')\n)\n\ntrend = alt.Chart(regdata).mark_line(\n    color = 'black'\n).encode(\n    x = 'log_income',\n    y = 'fitted'\n)\n\nband = alt.Chart(regdata).mark_area(\n    opacity = 0.3, color = 'black'\n).encode(\n    x = 'log_income',\n    y = 'upr_obs',\n    y2 = 'lwr_obs'\n)\n\n(scatter + trend + band).configure_axis(\n    labelFontSize = 16,\n    titleFontSize = 18\n)"
  },
  {
    "objectID": "slides/week8-prediction.html#confidence-interval-for-the-mean",
    "href": "slides/week8-prediction.html#confidence-interval-for-the-mean",
    "title": "Prediction",
    "section": "Confidence interval for the mean",
    "text": "Confidence interval for the mean\nPrediction objects come equipped with a method .conf_int() for computing confidence intervals.\n\nBy default, these will return 95% intervals for the mean. For the prediction at median income of 86K:\n\npred.conf_int()\n\narray([[0.00740212, 0.03118118]])\n\n\n\n\nInterpretation:\n\nWith 95% confidence, the mean achievement gap for a district with median income of 86K is estimated to favor boys by between 0.0074 and 0.0312 standard deviations from the national average."
  },
  {
    "objectID": "slides/week8-prediction.html#prediction-intervals",
    "href": "slides/week8-prediction.html#prediction-intervals",
    "title": "Prediction",
    "section": "Prediction intervals",
    "text": "Prediction intervals\nThe same method can return prediction intervals for observations. Set obs = True to use the standard error for observations:\n\npred.conf_int(obs = True)\n\narray([[-0.20639627,  0.24497957]])\n\n\n\nInterpretation:\n\nWith 95% confidence, the precise achievement gap for a district with median income of 86K is estimated to be between 0.206 standard deviations favoring girls and 0.245 standard deviations favoring boys."
  },
  {
    "objectID": "slides/week8-prediction.html#confidence-level",
    "href": "slides/week8-prediction.html#confidence-level",
    "title": "Prediction",
    "section": "Confidence level",
    "text": "Confidence level\nThe multiplier 2 gives an approximate 95% interval.\n\nWe can change that multiplier to obtain an interval for a different confidence level.\n\nprint('95% prediction interval: ', \n    pred.conf_int(alpha = 0.05, obs = True))\nprint('90% prediction interval: ', \n    pred.conf_int(alpha = 0.1, obs = True))\n\n95% prediction interval:  [[-0.20639627  0.24497957]]\n90% prediction interval:  [[-0.17002517  0.20860847]]\n\n\n\n\nThe confidence level is simply the percentage of the time that the interval covers the true value.\n\nthe 95% interval contains the observation of interest for 95% of samples\nthe 90% interval contains the observation of interest for 90% of samples"
  },
  {
    "objectID": "slides/week8-prediction.html#summary_frame",
    "href": "slides/week8-prediction.html#summary_frame",
    "title": "Prediction",
    "section": ".summary_frame()",
    "text": ".summary_frame()\nThe .summary_frame() method is handy for dealing with multiple predictions:\n\n# explanatory variable array for 4 new observations\nincome_new = np.linspace(10000, 100000, 4)\nx_new = sm.tools.add_constant(np.log(income_new))\n\n# compute predictions\npred = slr.fit().get_prediction(x_new)\n\n# generate point estimates and intervals\npred_df = pred.summary_frame(alpha = 0.1)\npred_df['log_income'] = np.log(income_new)\n\npred_df\n\n\n\n\n\n\n\n\nmean\nmean_se\nmean_ci_lower\nmean_ci_upper\nobs_ci_lower\nobs_ci_upper\nlog_income\n\n\n\n\n0\n-0.241194\n0.022020\n-0.277468\n-0.204920\n-0.433697\n-0.048692\n9.210340\n\n\n1\n-0.073374\n0.006875\n-0.084699\n-0.062048\n-0.262766\n0.116019\n10.596635\n\n\n2\n-0.005628\n0.004832\n-0.013589\n0.002332\n-0.194850\n0.183593\n11.156251\n\n\n3\n0.037550\n0.007345\n0.025450\n0.049649\n-0.151891\n0.226991\n11.512925\n\n\n\n\n\n\n\nA summary dataframe computed on a prediction grid returns the raw materials needed to plot uncertainty bands of different levels/types."
  },
  {
    "objectID": "slides/week8-prediction.html#check-your-understanding",
    "href": "slides/week8-prediction.html#check-your-understanding",
    "title": "Prediction",
    "section": "Check your understanding",
    "text": "Check your understanding\nWhich is the prediction band and which is the mean? Why?\n\n\n\n\nCode\n# construct prediction grid\nincome_grid = np.logspace(start = 10, stop = 12.5, num = 200, base = np.e)\nx_grid = sm.tools.add_constant(np.log(income_grid))\n\n# compute predictions\npreds = slr.fit().get_prediction(x_grid)\npred_df = preds.summary_frame(alpha = 0.05)\npred_df['log_income'] = np.log(income_grid)\n\n# visualization\ntrend = alt.Chart(pred_df).mark_line(\n    color = 'black'\n    ).encode(\n        x = 'log_income',\n        y = 'mean'\n    )\n\nband = alt.Chart(pred_df).mark_area(\n    opacity = 0.3, color = 'black'\n    ).encode(\n    x = 'log_income',\n    y = 'mean_ci_lower',\n    y2 = 'mean_ci_upper'\n)\n\n(scatter + trend + band).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# construct prediction grid\nincome_grid = np.logspace(start = 10, stop = 12.5, num = 200, base = np.e)\nx_grid = sm.tools.add_constant(np.log(income_grid))\n\n# compute predictions\npreds = slr.fit().get_prediction(x_grid)\npred_df = preds.summary_frame(alpha = 0.05)\npred_df['log_income'] = np.log(income_grid)\n\n# visualization\ntrend = alt.Chart(pred_df).mark_line(\n    color = 'black'\n    ).encode(\n        x = 'log_income',\n        y = 'mean'\n    )\n\nband = alt.Chart(pred_df).mark_area(\n    opacity = 0.3, color = 'black'\n    ).encode(\n    x = 'log_income',\n    y = 'obs_ci_lower',\n    y2 = 'obs_ci_upper'\n)\n\n(scatter + trend + band).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week8-prediction.html#check-your-understanding-1",
    "href": "slides/week8-prediction.html#check-your-understanding-1",
    "title": "Prediction",
    "section": "Check your understanding",
    "text": "Check your understanding\nWhich is the 99% band and which is the 85% band? Why?\n\n\n\n\nCode\n# construct prediction grid\nincome_grid = np.logspace(start = 10, stop = 12.5, num = 200, base = np.e)\nx_grid = sm.tools.add_constant(np.log(income_grid))\n\n# compute predictions\npreds = slr.fit().get_prediction(x_grid)\npred_df = preds.summary_frame(alpha = 0.01)\npred_df['log_income'] = np.log(income_grid)\n\n# visualization\ntrend = alt.Chart(pred_df).mark_line(\n    color = 'black'\n    ).encode(\n        x = 'log_income',\n        y = 'mean'\n    )\n\nband = alt.Chart(pred_df).mark_area(\n    opacity = 0.3, color = 'black'\n    ).encode(\n    x = 'log_income',\n    y = 'mean_ci_lower',\n    y2 = 'mean_ci_upper'\n)\n\n(scatter + trend + band).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# construct prediction grid\nincome_grid = np.logspace(start = 10, stop = 12.5, num = 200, base = np.e)\nx_grid = sm.tools.add_constant(np.log(income_grid))\n\n# compute predictions\npreds = slr.fit().get_prediction(x_grid)\npred_df = preds.summary_frame(alpha = 0.15)\npred_df['log_income'] = np.log(income_grid)\n\n# visualization\ntrend = alt.Chart(pred_df).mark_line(\n    color = 'black'\n    ).encode(\n        x = 'log_income',\n        y = 'mean'\n    )\n\nband = alt.Chart(pred_df).mark_area(\n    opacity = 0.3, color = 'black'\n    ).encode(\n    x = 'log_income',\n    y = 'mean_ci_lower',\n    y2 = 'mean_ci_upper'\n)\n\n(scatter + trend + band).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week8-prediction.html#predictive-accuracy",
    "href": "slides/week8-prediction.html#predictive-accuracy",
    "title": "Prediction",
    "section": "Predictive accuracy",
    "text": "Predictive accuracy\nHow good are the predictions?\nThe standard measure of accuracy is mean square error (MSE):\n\\[\nMSE(y, \\hat{y}) = \\frac{1}{n}\\sum_i (y_i - \\hat{y}_i)^2\n\\]\nSimilar to a variance, but measures the spread of observations about the predictions rather than their spread about the sample mean."
  },
  {
    "objectID": "slides/week8-prediction.html#expected-squared-error",
    "href": "slides/week8-prediction.html#expected-squared-error",
    "title": "Prediction",
    "section": "Expected squared error",
    "text": "Expected squared error\nMSE is an estimate of \\(\\mathbb{E}(y - \\hat{y})^2\\).\n\nFor genuine predictions, \\(y_i\\) and \\(\\hat{y}_i\\) are uncorrelated, because:\n\nnew observations are independent of past observations\nthe prediction depends only on past observations (through \\(\\hat{\\beta}\\))\n\n\n\nAs a result one can show: \\[\n\\mathbb{E}(y_i - \\hat{y}_i)^2 = \\text{var}\\epsilon_i + \\text{var}\\hat{y}_i\n\\]\n\nThe expected squared error is the variance of the observation plus the variance of the prediction.\nLarger expected prediction errors come from high prediction variance or high observation variance or both."
  },
  {
    "objectID": "slides/week8-prediction.html#data-partitioning",
    "href": "slides/week8-prediction.html#data-partitioning",
    "title": "Prediction",
    "section": "Data partitioning",
    "text": "Data partitioning\nIf fitted values are used to calculate MSE, \\(y_i\\) and \\(\\hat{y}_i\\) will be correlated, and MSE will be biased.\n\nAnother way of understanding: the model will have an advantage on the data that it was fit to.\n\n\nTo avoid this problem, it is common practice to partition data into nonoverlapping subsets:\n\none used to fit the model\nanother used to evaluate predictions"
  },
  {
    "objectID": "slides/week4-principles.html#principles-of-effective-design",
    "href": "slides/week4-principles.html#principles-of-effective-design",
    "title": "Figure design",
    "section": "Principles of effective design",
    "text": "Principles of effective design\nA good figure should:\n\nconvey a clear message or story\navoid excessive complexity\nlook nice\nbe well-labeled and appropriately sized\nstand alone with a short caption\n\n. . .\nHere we’ll mostly look at lots of examples."
  },
  {
    "objectID": "slides/week4-principles.html#on-color",
    "href": "slides/week4-principles.html#on-color",
    "title": "Figure design",
    "section": "On color",
    "text": "On color\nColor is one of the most frequently used aesthetics and is easy to misuse.\n\nchoice of color scale should match the data\nuse of color should take account of colorblindness\ncolor can only encode a limited amount of information"
  },
  {
    "objectID": "slides/week4-principles.html#color-scales",
    "href": "slides/week4-principles.html#color-scales",
    "title": "Figure design",
    "section": "Color scales",
    "text": "Color scales\nThere are three types of color scales.\n\nQualitative scales are non-monotonic sets of colors.\nSequential scales are monotonic sets of colors spanning a color gradient.\nDiverging scales are sequential scales centered at a neutral color."
  },
  {
    "objectID": "slides/week4-principles.html#qualitative-scales",
    "href": "slides/week4-principles.html#qualitative-scales",
    "title": "Figure design",
    "section": "Qualitative scales",
    "text": "Qualitative scales\nQualitative scales are non-monotonic sets of colors.\n\n. . .\nUseful for displaying categorical variables with few levels."
  },
  {
    "objectID": "slides/week4-principles.html#sequential-scales",
    "href": "slides/week4-principles.html#sequential-scales",
    "title": "Figure design",
    "section": "Sequential scales",
    "text": "Sequential scales\nSequential scales are monotonic sets of colors spanning a color gradient.\n\n. . .\nUseful for continuous variables."
  },
  {
    "objectID": "slides/week4-principles.html#sequential-scales-1",
    "href": "slides/week4-principles.html#sequential-scales-1",
    "title": "Figure design",
    "section": "Sequential scales",
    "text": "Sequential scales\n\n\n\nExample sequential color scale"
  },
  {
    "objectID": "slides/week4-principles.html#diverging-scales",
    "href": "slides/week4-principles.html#diverging-scales",
    "title": "Figure design",
    "section": "Diverging scales",
    "text": "Diverging scales\nDiverging scales are sequential scales centered at a neutral color.\n\n. . .\nUseful for continuous variables with a ‘natural’ center."
  },
  {
    "objectID": "slides/week4-principles.html#diverging-scales-1",
    "href": "slides/week4-principles.html#diverging-scales-1",
    "title": "Figure design",
    "section": "Diverging scales",
    "text": "Diverging scales"
  },
  {
    "objectID": "slides/week4-principles.html#use-of-color",
    "href": "slides/week4-principles.html#use-of-color",
    "title": "Figure design",
    "section": "Use of color",
    "text": "Use of color\nCommon mistakes:\n\nEncoding too much information\nPoor choice of scale\nNot accounting for colorblindness"
  },
  {
    "objectID": "slides/week4-principles.html#tmi",
    "href": "slides/week4-principles.html#tmi",
    "title": "Figure design",
    "section": "TMI",
    "text": "TMI"
  },
  {
    "objectID": "slides/week4-principles.html#better",
    "href": "slides/week4-principles.html#better",
    "title": "Figure design",
    "section": "Better",
    "text": "Better\n\n. . .\nAvoid encoding more than 5 categories using color"
  },
  {
    "objectID": "slides/week4-principles.html#inappropriate-scales",
    "href": "slides/week4-principles.html#inappropriate-scales",
    "title": "Figure design",
    "section": "Inappropriate scales",
    "text": "Inappropriate scales\nThe color scale doesn’t match the data well, since the rainbow scale emphasizes arbitrary data values. In addition, colors here are too intense."
  },
  {
    "objectID": "slides/week4-principles.html#better-1",
    "href": "slides/week4-principles.html#better-1",
    "title": "Figure design",
    "section": "Better",
    "text": "Better\n\n. . .\nA diverging scale is appropriate here because 50% is a natural midpoint in context."
  },
  {
    "objectID": "slides/week4-principles.html#color-blindness",
    "href": "slides/week4-principles.html#color-blindness",
    "title": "Figure design",
    "section": "Color blindness",
    "text": "Color blindness\nColor vision deficiency (CVD) or colorblindness refers to difficulty distinguishing specific colors.\n\nred-green CVD: protanomaly and deuteranomaly\nblue-yellow CVD: tritanomaly"
  },
  {
    "objectID": "slides/week4-principles.html#cvd-friendly-scales",
    "href": "slides/week4-principles.html#cvd-friendly-scales",
    "title": "Figure design",
    "section": "CVD-friendly scales",
    "text": "CVD-friendly scales\nSome color scales still retain visible contrast for different types of color vision deficiency (CVD).\nHere is a simulation (for those without CVD).\n\n\n\nColor scale shown for different types of colorblindness using CVD simulator"
  },
  {
    "objectID": "slides/week4-principles.html#cvd-unfriendly-scales",
    "href": "slides/week4-principles.html#cvd-unfriendly-scales",
    "title": "Figure design",
    "section": "CVD-unfriendly scales",
    "text": "CVD-unfriendly scales\nOther scales get muddled.\n\n. . .\nWhen in doubt, use a CVD simulator to check figures"
  },
  {
    "objectID": "slides/week4-principles.html#another-approach-redundancy",
    "href": "slides/week4-principles.html#another-approach-redundancy",
    "title": "Figure design",
    "section": "Another approach: redundancy",
    "text": "Another approach: redundancy\nWhen possible, use ‘redundant coding’ – map the same variable to color and one other aesthetic."
  },
  {
    "objectID": "slides/week4-principles.html#redundancy",
    "href": "slides/week4-principles.html#redundancy",
    "title": "Figure design",
    "section": "Redundancy",
    "text": "Redundancy\nWhen possible, use ‘redundant coding’ – map the same variable to color and one other aesthetic.\n\n\nC:\\Users\\lnbar\\AppData\\Local\\Temp\\ipykernel_25140\\1928961422.py:20: FutureWarning:\n\nThe default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n\nC:\\Users\\lnbar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\altair\\utils\\core.py:317: FutureWarning:\n\niteritems is deprecated and will be removed in a future version. Use .items instead.\n\n\n\n\n\n\n\n\n. . .\nRedundancy provides a failsafe against any circumstance that might compromise the effectiveness of color:\n\nprinters or black-and-white printing\nprojectors, displays, and lighting conditions\nCVD"
  },
  {
    "objectID": "slides/week4-principles.html#faceting",
    "href": "slides/week4-principles.html#faceting",
    "title": "Figure design",
    "section": "Faceting",
    "text": "Faceting\nYou’ve already made a faceted plot.\n\n. . .\nNotice the redundant use of color!"
  },
  {
    "objectID": "slides/week4-principles.html#faceting-1",
    "href": "slides/week4-principles.html#faceting-1",
    "title": "Figure design",
    "section": "Faceting",
    "text": "Faceting\nFacets are another way to encode categorical variables when side-by-side comparisons are of interest.\n. . .\nThe most common blunders with faceting are:\n\nFree axis scales are misleading\nFacet layout isn’t conducive to comparison of interest"
  },
  {
    "objectID": "slides/week4-principles.html#many-facets",
    "href": "slides/week4-principles.html#many-facets",
    "title": "Figure design",
    "section": "Many facets",
    "text": "Many facets\nOften a big panel of scatterplots can be a useful exploratory graphic.\n\n\n\n\n\nMovie ratings from IMDB\n\n\n\nThe figure shows a lot:\n\nTimespan of data 1906-2005\nMore observations (movies) in later years\nHigher vote counts in later years\nHigher rating variance among movies with fewer votes\nLong term reversal of voting/rating trend"
  },
  {
    "objectID": "slides/week4-principles.html#use-fixed-axis-scales",
    "href": "slides/week4-principles.html#use-fixed-axis-scales",
    "title": "Figure design",
    "section": "Use fixed axis scales",
    "text": "Use fixed axis scales\n\n\n\nExample of facets with different y axes\n\n\n. . .\nSuggests, misleadingly, that Education declined by the same amount as social science and history."
  },
  {
    "objectID": "slides/week4-principles.html#use-fixed-axis-scales-1",
    "href": "slides/week4-principles.html#use-fixed-axis-scales-1",
    "title": "Figure design",
    "section": "Use fixed axis scales",
    "text": "Use fixed axis scales\n\n\n\nSame as before, with common fixed axis scales."
  },
  {
    "objectID": "slides/week4-principles.html#what-about-this",
    "href": "slides/week4-principles.html#what-about-this",
    "title": "Figure design",
    "section": "What about this?",
    "text": "What about this?\nOne axis is fixed, one is free.\n\n\n\nA figure from HW2\n\n\n. . .\nThe variable of interest, Gap, is still comparable across facets. So only one axis needs to be fixed.\n. . .\nWhat would it look like if all axis scales were fixed? Would comparisons be easier or harder?"
  },
  {
    "objectID": "slides/week4-principles.html#labels-and-legends",
    "href": "slides/week4-principles.html#labels-and-legends",
    "title": "Figure design",
    "section": "Labels and legends",
    "text": "Labels and legends\nThe most common blunders with regard to labels are:\n\nUse of dataframe column names as labels\nObscure or uninterpretable labels\nToo small or too big\n\n. . .\nFor sizing, it’s important to pay attention to the balance of labels, whitespace, and graphical elements."
  },
  {
    "objectID": "slides/week4-principles.html#sizing",
    "href": "slides/week4-principles.html#sizing",
    "title": "Figure design",
    "section": "Sizing",
    "text": "Sizing\nUsually figure defaults look fine on your IDE but render too small when graphics are exported.\n\n\n\nThese will be illegible in slide presentations, reports, etc."
  },
  {
    "objectID": "slides/week4-principles.html#sizing-1",
    "href": "slides/week4-principles.html#sizing-1",
    "title": "Figure design",
    "section": "Sizing",
    "text": "Sizing\nThese labels are legible, but still too small – they take up a minimum of space in the figure.\n\n\n\nUnbalanced text/graphic/whitespace"
  },
  {
    "objectID": "slides/week4-principles.html#sizing-2",
    "href": "slides/week4-principles.html#sizing-2",
    "title": "Figure design",
    "section": "Sizing",
    "text": "Sizing\nUse larger labels than you think you’ll need.\n\n\n\nBalanced\n\n\n. . .\nNote also the mark size is increased a bit."
  },
  {
    "objectID": "slides/week4-principles.html#sizing-3",
    "href": "slides/week4-principles.html#sizing-3",
    "title": "Figure design",
    "section": "Sizing",
    "text": "Sizing\nDon’t overdo it.\n\n\n\nUnbalanced again"
  },
  {
    "objectID": "slides/week4-principles.html#sizing-4",
    "href": "slides/week4-principles.html#sizing-4",
    "title": "Figure design",
    "section": "Sizing",
    "text": "Sizing\nIf the figure will be reproduced in a scaled-down size, increase all sizes in proportion."
  },
  {
    "objectID": "slides/week4-principles.html#critiques",
    "href": "slides/week4-principles.html#critiques",
    "title": "Figure design",
    "section": "Critiques",
    "text": "Critiques\nSeries from NYC Life Expectancy Dropped 4.6 Years in 2020\n\n\n\n\nPositive:\n\neffective use of labels\neffective use of highlighting\nwell-proportioned\nclean axes\n\nNegative:\n\nCOVID spike looks minimal, contrary to story?\nthe most striking feature of the plot is the time trend and variance stabilization"
  },
  {
    "objectID": "slides/week4-principles.html#critiques-1",
    "href": "slides/week4-principles.html#critiques-1",
    "title": "Figure design",
    "section": "Critiques",
    "text": "Critiques\nSeries from NYC Life Expectancy Dropped 4.6 Years in 2020\n\n\n\n\nPositive:\n\nsame as before\n\nNegative:\n\ndoesn’t convey proportional change in decrease efficiently, but that’s what the caption emphasizes\n‘overall’ looks like a fourth group"
  },
  {
    "objectID": "slides/week4-principles.html#critiques-2",
    "href": "slides/week4-principles.html#critiques-2",
    "title": "Figure design",
    "section": "Critiques",
    "text": "Critiques\nSeries from NYC Life Expectancy Dropped 4.6 Years in 2020\n\n\n\n\nPositive:\n\nexemplary use of color scale/palette\nline shading shows missing data clearly\neffective use of labels\n\nNegative:\n\nno clear story\nlacking a baseline comparison"
  },
  {
    "objectID": "slides/week4-principles.html#critiques-3",
    "href": "slides/week4-principles.html#critiques-3",
    "title": "Figure design",
    "section": "Critiques",
    "text": "Critiques\nSeries from NYC Life Expectancy Dropped 4.6 Years in 2020\n\n\n\n\nPositive:\n\nclear story\n\nNegative:\n\nawkward/distracting to include time, since no history for COVID\nnot the most efficient display of the captioned message\n\nRemark:\n\nit would be more interesting to see the time courses after 2020"
  },
  {
    "objectID": "slides/week4-principles.html#more-critiques",
    "href": "slides/week4-principles.html#more-critiques",
    "title": "Figure design",
    "section": "More critiques",
    "text": "More critiques\n\n\n\n\nPositive:\n\nclear labels\nunambiguous\n\nNegative:\n\nbars take up all of the plot here\nmany words seem equivalent\n\nSuggestions:\n\nfind an alternative to the bar plot\nconsider emphasizing comparisons between word clusters rather than individual words"
  },
  {
    "objectID": "slides/week4-principles.html#tidy-graphics",
    "href": "slides/week4-principles.html#tidy-graphics",
    "title": "Figure design",
    "section": "Tidy graphics?",
    "text": "Tidy graphics?\nGraphics should avoid conflating data semantics.\n\nobservational units should be clearly distinguished\ndifferent types of observational units should be shown on different graphics\n\n. . .\nIn addition, they should avoid conflating observed from inferred quantities.\n\naggregated values should be clearly distinguished from individual observations\npredictions, inferred trends, or uncertainty should be shown using a different graphical element than observed data\nunless comparing estimates and observations is the point, make separate graphics"
  },
  {
    "objectID": "slides/week4-principles.html#an-untidy-plot",
    "href": "slides/week4-principles.html#an-untidy-plot",
    "title": "Figure design",
    "section": "An untidy plot",
    "text": "An untidy plot\nThe starting plot in lab 3 is actually a bad plot because all years are shown together – so observational units (countries) are not clearly distinguished."
  },
  {
    "objectID": "slides/week4-principles.html#a-tidy-plot",
    "href": "slides/week4-principles.html#a-tidy-plot",
    "title": "Figure design",
    "section": "A tidy plot",
    "text": "A tidy plot\n\nThis is tidy, because within facets:\n\neach bubble represents a country\nany two bubbles represent distinct countries"
  },
  {
    "objectID": "slides/week4-principles.html#exploration-or-presentation",
    "href": "slides/week4-principles.html#exploration-or-presentation",
    "title": "Figure design",
    "section": "Exploration or presentation?",
    "text": "Exploration or presentation?\nIn data exploration, it’s more important to generate lots of figures quickly than put a lot of care into details.\n\ndo not need to be scrupulous about labels, sizing, color scales, proportionality, etc.\ndo need to attend to axis scales and appropriate choice of graphical display (e.g., boxplots vs. densities)\nshould keep plots simple; don’t try to visualize too much information at once\n\n. . .\nIn developing presentation graphics, details matter.\n\nconsider all visualization principles, especially sizing, color, etc.\noptimize for communication"
  },
  {
    "objectID": "slides/week4-principles.html#presenting-graphics",
    "href": "slides/week4-principles.html#presenting-graphics",
    "title": "Figure design",
    "section": "Presenting graphics",
    "text": "Presenting graphics\nHere is my approach to presenting a graphic. I use this for both written and oral presentations.\n\nDescribe clearly each graphical element and what each represents.\n\nStart with the highest-level graphical elements (axes) and work progressively to the lowest-level elements (aesthetics)\n\nDescribe clearly what is shown visually in the graphic without interpreting any patterns.\nFinally, say what the graphic shows."
  },
  {
    "objectID": "slides/week4-principles.html#practice",
    "href": "slides/week4-principles.html#practice",
    "title": "Figure design",
    "section": "Practice",
    "text": "Practice"
  },
  {
    "objectID": "slides/week4-principles.html#practice-1",
    "href": "slides/week4-principles.html#practice-1",
    "title": "Figure design",
    "section": "Practice",
    "text": "Practice"
  },
  {
    "objectID": "slides/week4-principles.html#practice-2",
    "href": "slides/week4-principles.html#practice-2",
    "title": "Figure design",
    "section": "Practice",
    "text": "Practice"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Concepts and Analysis",
    "section": "",
    "text": "This is the course website for UCSB’s Data Science Concepts and Analysis class (PSTAT100). Content is directed towards currently enrolled students. Please ask permission before using course materials in any other capacity. Please do not post any materials obtained or derived from this website on third-party websites."
  },
  {
    "objectID": "projects/mp2/mp2-ncca-soln.html",
    "href": "projects/mp2/mp2-ncca-soln.html",
    "title": "Mini project 2: primary productivity in coastal waters",
    "section": "",
    "text": "In this project you’re again given a dataset and some questions. The data for this project come from the EPA’s National Aquatic Resource Surveys, and in particular the National Coastal Condition Assessment (NCCA); broadly, you’ll do an exploratory analysis of primary productivity in coastal waters.\nBy way of background, chlorophyll A is often used as a proxy for primary productivity in marine ecosystems; primary producers are important because they are at the base of the food web. Nitrogen and phosphorus are key nutrients that stimulate primary production.\nIn the data folder you’ll find water chemistry data, site information, and metadata files. It might be helpful to keep the metadata files open when tidying up the data for analysis. It might also be helpful to keep in mind that these datasets contain a considerable amount of information, not all of which is relevant to answering the questions of interest. Notice that the questions pertain somewhat narrowly to just a few variables. It’s recommended that you determine which variables might be useful and drop the rest.\nAs in the first mini project, there are accurate answers to each question that are mutually consistent with the data, but there aren’t uniquely correct answers. You will likely notice that you have even more latitude in this project than in the first, as the questions are slightly broader. Since we’ve been emphasizing visual and exploratory techniques in class, you are encouraged (but not required) to support your answers with graphics.\nThe broader goal of these mini projects is to cultivate your problem-solving ability in an unstructured setting. Your work will be evaluated based on the following: - choice of method(s) used to answer questions; - clarity of presentation; - code style and documentation.\nPlease write up your results separately from your codes; codes should be included at the end of the notebook."
  },
  {
    "objectID": "projects/mp2/mp2-ncca-soln.html#part-1-dataset",
    "href": "projects/mp2/mp2-ncca-soln.html#part-1-dataset",
    "title": "Mini project 2: primary productivity in coastal waters",
    "section": "Part 1: dataset",
    "text": "Part 1: dataset\nMerge the site information with the chemistry data and tidy it up. Determine which columns to keep based on what you use in answering the questions in part 2; then, print the first few rows here (but do not include your codes used in tidying the data) and write a brief description (1-2 paragraphs) of the dataset conveying what you take to be the key attributes. Direct your description to a reader unfamiliar with the data; ensure that in your data preview the columns are named intelligibly.\nSuggestion: export your cleaned data as a separate .csv file and read that directly in below, as in: pd.read_csv('YOUR DATA FILE').head().\n\n# show a few rows of clean data\n\nWrite your description here."
  },
  {
    "objectID": "projects/mp2/mp2-ncca-soln.html#part-2-exploratory-analysis",
    "href": "projects/mp2/mp2-ncca-soln.html#part-2-exploratory-analysis",
    "title": "Mini project 2: primary productivity in coastal waters",
    "section": "Part 2: exploratory analysis",
    "text": "Part 2: exploratory analysis\nAnswer each question below and provide a visualization supporting your answer. A description and interpretation of the visualization should be offered.\nComment: you can either designate your plots in the codes section with clear names and reference them in your answers; or you can export your plots as image files and display them in markdown cells.\n\nWhat is the apparent relationship between nutrient availability and productivity?\nComment: it’s fine to examine each nutrient – nitrogen and phosphorus – separately, but do consider whether they might be related to each other.\nWrite your answer here.\n\n\nAre there any notable differences in available nutrients among U.S. coastal regions?\nWrite your answer here.\n\n\nBased on the 2010 data, does productivity seem to vary geographically in some way?\nIf so, explain how; If not, explain what options you considered and ruled out.\nWrite your answer here.\n\n\nHow does primary productivity in California coastal waters change seasonally in 2010, if at all?\nDoes your result make intuitive sense?\nWrite your answer here.\n\n\nPose and answer one additional question.\nWrite your answer here."
  },
  {
    "objectID": "projects/mp1/mp1-airquality.html",
    "href": "projects/mp1/mp1-airquality.html",
    "title": "Mini project 1: air quality in U.S. cities",
    "section": "",
    "text": "In a way, this project is simple: you are given some data on air quality in U.S. metropolitan areas over time together with several questions of interest, and your objective is to answer the questions.\nHowever, unlike the homeworks and labs, there is no explicit instruction provided about how to answer the questions or where exactly to begin. Thus, you will need to discern for yourself how to manipulate and summarize the data in order to answer the questions of interest, and you will need to write your own codes from scratch to obtain results. It is recommended that you examine the data, consider the questions, and plan a rough approach before you begin doing any computations.\nYou have some latitude for creativity: although there are accurate answers to each question – namely, those that are consistent with the data – there is no singularly correct answer. Most students will perform similar operations and obtain similar answers, but there’s no specific result that must be considered to answer the questions accurately. As a result, your approaches and answers may differ from those of your classmates. If you choose to discuss your work with others, you may even find that disagreements prove to be fertile learning opportunities.\nThe questions can be answered using computing skills taught in class so far and basic internet searches for domain background; for this project, you may wish to refer to HW1 and Lab1 for code examples and the EPA website on PM pollution for background. However, you are also encouraged to refer to external resources (package documentation, vignettes, stackexchange, internet searches, etc.) as needed – this may be an especially good idea if you find yourself thinking, ‘it would be really handy to do X, but I haven’t seen that in class anywhere’.\nThe broader goal of these mini projects is to cultivate your problem-solving ability in an unstructured setting. Your work will be evaluated based on the following: - choice of method(s) used to answer questions; - clarity of presentation; - code style and documentation.\nPlease write up your results separately from your codes; codes should be included at the end of the notebook."
  },
  {
    "objectID": "projects/mp1/mp1-airquality.html#part-i",
    "href": "projects/mp1/mp1-airquality.html#part-i",
    "title": "Mini project 1: air quality in U.S. cities",
    "section": "Part I",
    "text": "Part I\nMerge the city information with the air quality data and tidy the dataset (see notes below). Write a one- to two-paragraph description of the data.\nIn your description, answer the following questions:\n\nWhat is a CBSA (the geographic unit of measurement)?\nHow many CBSA’s are included in the data?\nIn how many states and territories do the CBSA’s reside? (Hint: str.split())\nIn which years were data values recorded?\nHow many observations are recorded?\nHow many variables are measured?\nWhich variables are non-missing most of the time (i.e., in at least 50% of instances)?\nWhat is PM 2.5 and why is it important?\nWhat are the basic statistical properties of the variable(s) of interest?\n\nPlease write your description in narrative fashion; please do not list answers to the questions above one by one.\n\nAir quality data\nWrite your description here."
  },
  {
    "objectID": "projects/mp1/mp1-airquality.html#part-ii",
    "href": "projects/mp1/mp1-airquality.html#part-ii",
    "title": "Mini project 1: air quality in U.S. cities",
    "section": "Part II",
    "text": "Part II\nFocus on the PM2.5 measurements that are non-missing most of the time. Answer each of the following questions in a brief paragraph or two. Do not describe your analyses step-by-step for your answers; instead, report your findings. Your paragraph(s) should indicate both your answer to the question and a justification for your answer; please do not include codes with your answers.\n\nHas PM 2.5 air pollution improved in the U.S. on the whole since 2000?\nWrite your answer here.\n\n\nOver time, has PM 2.5 pollution become more variable, less variable, or about equally variable from city to city in the U.S.?\nWrite your answer here.\n\n\nWhich state has seen the greatest improvement in PM 2.5 pollution over time? Which city has seen the greatest improvement?\nWrite your answer here. Be sure to explain how you defined ‘best improvement’ in each case.\n\n\nChoose a location with some meaning to you (e.g. hometown, family lives there, took a vacation there, etc.). Was that location in compliance with EPA primary standards as of the most recent measurement?\nWrite your answer here."
  },
  {
    "objectID": "projects/mp1/mp1-airquality.html#imputation",
    "href": "projects/mp1/mp1-airquality.html#imputation",
    "title": "Mini project 1: air quality in U.S. cities",
    "section": "Imputation",
    "text": "Imputation\nOne strategy for filling in missing values (‘imputation’) is to use non-missing values to predict the missing ones; the success of this strategy depends in part on the strength of relationship between the variable(s) used as predictors of missing values.\nIdentify one other pollutant that might be a good candidate for imputation based on the PM 2.5 measurements and explain why you selected the variable you did. Can you envision any potential pitfalls to this technique?"
  },
  {
    "objectID": "projects/mp1/mp1-airquality.html#notes-on-merging-keep-at-bottom-of-notebook",
    "href": "projects/mp1/mp1-airquality.html#notes-on-merging-keep-at-bottom-of-notebook",
    "title": "Mini project 1: air quality in U.S. cities",
    "section": "Notes on merging (keep at bottom of notebook)",
    "text": "Notes on merging (keep at bottom of notebook)\nTo combine datasets based on shared information, you can use the pd.merge(A, B, how = ..., on = SHARED_COLS) function, which will match the rows of A and B based on the shared columns SHARED_COLS. If how = 'left', then only rows in A will be retained in the output (so B will be merged to A); conversely, if how = 'right', then only rows in B will be retained in the output (so A will be merged to B).\nA simple example of the use of pd.merge is illustrated below:\n\n# toy data frames\nA = pd.DataFrame(\n    {'shared_col': ['a', 'b', 'c'], \n    'x1': [1, 2, 3], \n    'x2': [4, 5, 6]}\n)\n\nB = pd.DataFrame(\n    {'shared_col': ['a', 'b'], \n    'y1': [7, 8]}\n)\n\n\nA\n\n\nB\n\nBelow, if A and B are merged retaining the rows in A, notice that a missing value is input because B has no row where the shared column (on which the merging is done) has value c. In other words, the third row of A has no match in B.\n\n# left join\npd.merge(A, B, how = 'left', on = 'shared_col')\n\nIf the direction of merging is reversed, and the row structure of B is dominant, then the third row of A is dropped altogether because it has no match in B.\n\n# right join\npd.merge(A, B, how = 'right', on = 'shared_col')"
  },
  {
    "objectID": "content.html",
    "href": "content.html",
    "title": "Materials",
    "section": "",
    "text": "Readings:\n\nLDS1 The Data Science Lifecycle\nLDS5 Case Study: Why is my Bus Always Late?\nPDSH2.1 Understanding data types in python\nPDSH2.2 The basics of numpy arrays\nPDSH2.4 Aggregations: min, max, and everything in between\n\nMonday: Course introduction [slides]\nLab sections: Orientation to Jupyter notebooks [html]\nWednesday: Data science lifecycle [slides]"
  },
  {
    "objectID": "content.html#week-1",
    "href": "content.html#week-1",
    "title": "Materials",
    "section": "",
    "text": "Readings:\n\nLDS1 The Data Science Lifecycle\nLDS5 Case Study: Why is my Bus Always Late?\nPDSH2.1 Understanding data types in python\nPDSH2.2 The basics of numpy arrays\nPDSH2.4 Aggregations: min, max, and everything in between\n\nMonday: Course introduction [slides]\nLab sections: Orientation to Jupyter notebooks [html]\nWednesday: Data science lifecycle [slides]"
  },
  {
    "objectID": "content.html#week-2",
    "href": "content.html#week-2",
    "title": "Materials",
    "section": "Week 2",
    "text": "Week 2\nReadings:\n\nWickham (2014). Tidy data. Journal of statistical software 59(10). [link to paper]\nPDSH3.1 Introducing pandas objects\nPDSH3.2 Data indexing and selection\nPDSH3.7 Merge and join\nPDSH3.8 Aggregation and grouping\n\nAssignments:\n\nHW1, BRFSS case study, due Monday, October 16 [html]\n\nMonday: Tidy data [slides]\nLab sections: Pandas [html]\nWednesday: Dataframe transformations [slides]"
  },
  {
    "objectID": "content.html#week-3",
    "href": "content.html#week-3",
    "title": "Materials",
    "section": "Week 3",
    "text": "Week 3\nReadings:\n\nLDS2.2 Population, frame, sample\nVan Buuren, Flexible Imputation of Missing Data, section 2.2 Concepts in incomplete data\nPDSH3.4 Handling missing data\n\nAssignments:\n\nMini project 1, due Monday, Oct 23 [html]\n\nMonday: Sampling, bias, and missingness [slides]\nLab sections: Exploring sampling bias through simulation [html]"
  },
  {
    "objectID": "content.html#week-4",
    "href": "content.html#week-4",
    "title": "Materials",
    "section": "Week 4",
    "text": "Week 4\nReadings:\n\nWilke, Fundamentals of Data Visualization Ch. 2-5\nLDS11.1 Choosing scale to reveal structure\n(Recommended) Cook, D., Lee, E. K., & Majumder, M. (2016). Data visualization and statistical graphics in big data analysis. Annual Review of Statistics and Its Application, 3, 133-159. [link to paper]\n(Recommended) Gelman, A., & Unwin, A. (2013). Infovis and statistical graphics: different goals, different looks. Journal of Computational and Graphical Statistics, 22(1), 2-28. [link to paper]\n(Recommended) Iliinsky, N. (2010). On beauty. Beautiful visualization: Looking at data through the eyes of experts, 1-13. [link to chapter]\n\nAssignments:\n\nHW2, SEDA case study, due Monday, Oct 30 [html]\n\nMonday: Statistical graphics [slides]\nLab sections: Data visualization [html]\nWednesday: Principles of figure design [slides]"
  },
  {
    "objectID": "content.html#week-5",
    "href": "content.html#week-5",
    "title": "Materials",
    "section": "Week 5",
    "text": "Week 5\nReadings:\n\nLDS 11.2 Smoothing and aggregating data\nScott, D.W. (2012). Multivariate Density Estimation and Visualization. In: Gentle, J., HC$rdle, W., Mori, Y. (eds) Handbook of Computational Statistics. [link to chapter]\n\nAssignments:\n\nHW3, Diatom paleoclimatology case study, due Monday, Nov 13 [html]\n\nMonday: Exploratory analysis and density estimation [slides]\nLab sections: Smoothing [html]\nWednesday: Multivariate KDE, mixture models, and scatterplot smoothing [slides][activity html]"
  },
  {
    "objectID": "content.html#week-6",
    "href": "content.html#week-6",
    "title": "Materials",
    "section": "Week 6",
    "text": "Week 6\nReadings:\n\nLDS 10.2-10.5 Exploratory data analysis\nPDSH5.9 Principal component andalysis\n\nAssignments:\nMonday: Covariance, correlation, and spectral decomposition [slides]\nLab sections: Principal components [html]\nWednesday: Principal components [slides]"
  },
  {
    "objectID": "content.html#week-7",
    "href": "content.html#week-7",
    "title": "Materials",
    "section": "Week 7",
    "text": "Week 7\nReadings:\n\nLDS 15.1-15.3 Simple linear models\n\nAssignments:\n\nMini project 2, due Tuesday, May 30 [html]\n\nMonday: Modeling concepts; least squares [slides]\nLab sections: Principal components [html]\nWednesday: The simple linear regression model [slides]"
  },
  {
    "objectID": "content.html#week-8",
    "href": "content.html#week-8",
    "title": "Materials",
    "section": "Week 8",
    "text": "Week 8\nReadings:\n\nLDS 17.5 Basics of prediction intervals\nLDS 15.4 Multiple regression\n\nAssignments:\n\nHW4, Discrimination in disability benefit allocation, due Wednesday, June 7 [html]\n\nMonday: Prediction [slides]\nLab sections Fitting regression models [html]\nWednesday: Multiple regression [slides]"
  },
  {
    "objectID": "content.html#week-9",
    "href": "content.html#week-9",
    "title": "Materials",
    "section": "Week 9",
    "text": "Week 9\nReadings:\n\nLDS 19.1 – 19.3 and 19.5 Classification\n\nAssignments:\n\nCourse project due Friday, June 16 [html]\n\nNo class or lab sections Monday\nWednesday: Classification [slides]"
  },
  {
    "objectID": "content.html#week-10",
    "href": "content.html#week-10",
    "title": "Materials",
    "section": "Week 10",
    "text": "Week 10\nNo readings or new assignments\nNo class Monday\nLab sections: Logistic regression (submission is optional) [html]\nWendesday: Clustering [slides]"
  },
  {
    "objectID": "hw/hw4-dds/hw4-dds-soln.html",
    "href": "hw/hw4-dds/hw4-dds-soln.html",
    "title": "Background: California Department of Developmental Services",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"hw4-dds.ipynb\")\n\n\nimport numpy as np\nimport pandas as pd\nimport altair as alt\nimport statsmodels.api as sm\n# disable row limit for plotting\nalt.data_transformers.disable_max_rows()\n# uncomment to ensure graphics display with pdf export\n# alt.renderers.enable('mimetype')\n\nDataTransformerRegistry.enable('default')\n\n\nFrom Taylor, S. A., & Mickel, A. E. (2014). Simpson’s Paradox: A Data Set and Discrimination Case Study Exercise. Journal of Statistics Education, 22(1):\n\nMost states in the USA provide services and support to individuals with developmental disabilities (e.g., intellectual disability, cerebral palsy, autism, etc.) and their families. The agency through which the State of California serves the developmentally-disabled population is the California Department of Developmental Services (DDS) … One of the responsibilities of DDS is to allocate funds that support over 250,000 developmentally-disabled residents. A number of years ago, an allegation of discrimination was made and supported by a univariate analysis that examined average annual expenditures on consumers by ethnicity. The analysis revealed that the average annual expenditures on Hispanic consumers was approximately one-third of the average expenditures on White non-Hispanic consumers. This finding was the catalyst for further investigation; subsequently, state legislators and department managers sought consulting services from a statistician.\n\nIn this assignment, you’ll analyze the deidentified DDS data published with this article to answer the question: is there evidence of ethnic or gender discrimination in allocation of DDS funds? This will involve practicing the following:\n\nexploratory data visualization\nregression analysis\nmodel visualization\n\nAside: The JSE article focuses on what’s known as Simpson’s paradox, an arithmetic phenomenon in which aggregate trends across multiple groups show the opposite of within-group trends. We won’t emphasize this topic, though the data does provide a nice illustration.\n\nDDS data\nThe data for this assignment are already tidy, so in this section you’ll just familiarize yourself with basic characteristics. The first few rows of the data are shown below:\n\ndds = pd.read_csv('data/california-dds.csv')\ndds.head()\n\n\n\n\n\n\n\n\nId\nAge Cohort\nAge\nGender\nExpenditures\nEthnicity\n\n\n\n\n0\n10210\n13 to 17\n17\nFemale\n2113\nWhite not Hispanic\n\n\n1\n10409\n22 to 50\n37\nMale\n41924\nWhite not Hispanic\n\n\n2\n10486\n0 to 5\n3\nMale\n1454\nHispanic\n\n\n3\n10538\n18 to 21\n19\nFemale\n6400\nHispanic\n\n\n4\n10568\n13 to 17\n13\nMale\n4412\nWhite not Hispanic\n\n\n\n\n\n\n\nTake a moment to open and read the data documentation (data &gt; california-dds-documentation.md).\n\n\nQuestion 1: Data description\nWrite a short paragraph answering the following questions based on the data documentation.\n\n\nWhy were the data collected? What is the purpose of this dataset?\n\n\nWhat are the observational units?\n\n\nWhat is the population of interest?\n\n\nHow was the sample obtained (e.g. random sampling, adminsitrative data, convenience sampling, etc.)?\n\n\nCan inferences about the population be drawn from the sample?\n\n\nIn addition, make a table summarizing the variables measured. Use the format below.\n\n\n\nName\nVariable description\nType\nUnits of measurement\n\n\n\n\nID\nUnique consumer identifier\nNumeric\nNone\n\n\n\nType your answer here, replacing this text.\nSOLUTION\nThe population of interest is all developmentally disabled residents in California recieving DDS support. A random sample of individuals were drawn from the population to ascertain whether there is systematic ethnic or other bias in allocating DDS benefits. Because the sample was drawn randomly, the data support inferences about the population. Variables measured are summarized below.\n\n\n\nName\nVariable description\nType\nUnits of measurement\n\n\n\n\nID\nUnique consumer identifier\nNumeric\nNone\n\n\nAge Cohort\nAge Range of Person\nString\nYears\n\n\nAge\nAge of Person\nNumeric\nYears\n\n\nGender\nGender of Person\nString\nNone\n\n\nExpenditures\nDDS Funds Allocated for Person\nNumeric\nUSD\n\n\nEthnicity\nEthnicity of Person\nString\nNone\n\n\n\n\n\n\n\nExploratory analysis\nHere you’ll use graphical and descriptive techniques to explore the allegation of discriminatory allocation of benefits.\n\nQuestion 2: Alleged discrimination\nConstruct a table of median expenditures by ethnicity that also shows the sample size for each ethnic group in the data.\n\nSlice the ethnicity and expenditure variables from dds, group by ethnicity, and calculate the median expenditure. Store the resulting dataframe as median_expend_by_eth.\nCompute the sample sizes for each ethnicity using .value_counts(): obtain a pandas series indexed by ethnicity with a single column named n. You’ll need to use .rename(...) to avoid having the column named Ethnicity. Store this pandas series as ethnicity_n.\nUse pd.concat(...) to append the sample sizes in ethnicity_n to the median expenditures in median_expend_by_eth. Store the result as tbl_1.\n\nPrint tbl_1. Does expenditure seem to differ by ethnicity? Does sample size?\nType your answer here, replacing this text.\n\n# compute median expenditures\nmedian_expend_by_eth = dds.iloc[:, 4:6].groupby('Ethnicity').median() # SOLUTION\n\n# compute sample sizes\nethnicity_n = dds.Ethnicity.value_counts().rename('n') # SOLUTION\n\n# concatenate\ntbl_1 = pd.concat([median_expend_by_eth, ethnicity_n], axis = 1) # SOLUTION\n\n# print\ntbl_1\n\n\n\n\n\n\n\n\nExpenditures\nn\n\n\n\n\nAmerican Indian\n41817.5\n4\n\n\nAsian\n9369.0\n129\n\n\nBlack\n8687.0\n59\n\n\nHispanic\n3952.0\n376\n\n\nMulti Race\n2622.0\n26\n\n\nNative Hawaiian\n40727.0\n3\n\n\nOther\n3316.5\n2\n\n\nWhite not Hispanic\n15718.0\n401\n\n\n\n\n\n\n\nSOLUTION\nYes, median expenditure does seem to differ between groups – for example, the median benifit amount allocated to hispanic people is less than half of the that allocated to black people and less than a third of that allocated to white non-hispanic people. The sample sizes differ considerably, ranging from 2 to 401.\n\ngrader.check(\"q2\")\n\n\n\n\nQuestion 3: Plot median expenditures\nConstruct a point-and-line plot of median expenditure (y) against ethnicity (x), with: * ethnicities sorted by descending median expenditure; * the median expenditure axis shown on the log scale; * the y-axis labeled ‘Median expenditure’; and * no x-axis label (since the ethnicity group names are used to label the axis ticks, the label ‘Ethnicity’ is redundant).\nStore the result as fig_1 and display the plot.\nHints: * you’ll need to use tbl_1.reset_index() to obtain the ethnicity group as a variable; * recall that .mark_line(point = True) will add points to a line plot; * sorting can be done using alt.X(..., sort = alt.EncodingSortField(field = ..., order = ...))\n\n# BEGIN SOLUTION\nfig_1 = alt.Chart(\n    tbl_1.reset_index()\n).mark_line(\n    point = True\n).encode(\n    x = alt.X(\n        'index', \n        title = '',\n        sort = alt.EncodingSortField(field = 'Expenditures', \n                                     order = 'descending')\n    ),\n    y = alt.Y(\n        'Expenditures', \n        title = 'Median expenditure', \n        scale = alt.Scale(type = 'log'))\n)\n\n# display\nfig_1\n# END SOLUTION\n\n\n\n\n\n\n\n\n\n\nQuestion 4: Age and expenditure\nHow does expenditure differ by age? Construct a scatterplot of expenditure against age. Store the plot as fig_2. In one or two sentences, comment on the plot – what is the main pattern it reveals?\nType your answer here, replacing this text.\n\n# construct scatterplot\n# BEGIN SOLUTION\nfig_2 = alt.Chart(dds).mark_point().encode(\n    x = 'Age',\n    y = alt.Y('Expenditures', scale = alt.Scale(type = 'log'))\n)\n# END SOLUTION\n\n# display\nfig_2 # SOLUTION\n\n\n\n\n\n\nSOLUTION\nOverall, benefit amounts increase with age, but there is a substantial jump at 18-21. This is most likely because underage recipients live with their families and have less financial need.\n\nPrecisely because recipients have different needs at different ages that translate to jumps in expenditure, age has been discretized into age cohorts defined based on need level. Going forward, we’ll work with these age cohorts – by treating age as discrete, we won’t need to attempt to model the discontinuities in the relationship between age and expenditure.\nThe cohort labels are stored as Age Cohort in the dataset. There are six cohorts; the cell below coerces the labels to an ordered category, puts them in the proper order, and prints the category levels.\n\n# convert data types\ndds_cat = dds.astype({'Age Cohort': 'category', 'Ethnicity': 'category', 'Gender': 'category'}).copy()\n\ndds_cat['Age Cohort'] = dds_cat['Age Cohort'].cat.as_ordered().cat.reorder_categories(\n    dds_cat['Age Cohort'].cat.categories[[0, 5, 1, 2, 3, 4]]\n)\n\n# age cohorts\ndds_cat['Age Cohort'].cat.categories\n\nIndex(['0 to 5', '6 to 12', '13 to 17', '18 to 21', '22 to 50', '51+'], dtype='object')\n\n\nHere is an explanation of how the cohort age boundaries were chosen:\n\nThe 0-5 cohort (preschool age) has the fewest needs and requires the least amount of funding. For the 6-12 cohort (elementary school age) and 13-17 (high school age), a number of needed services are provided by schools. The 18-21 cohort is typically in a transition phase as the consumers begin moving out from their parents’ homes into community centers or living on their own. The majority of those in the 22-50 cohort no longer live with their parents but may still receive some support from their family. Those in the 51+ cohort have the most needs and require the most amount of funding because they are living on their own or in community centers and often have no living parents.\n\nNote that the ordering can be retrieved using .cat.codes, which coerces an ordered categorical variable to its integer encoding (0 for lowest level, 1 for next lowest, and so on). It will be helpful to store the ordering for plotting purposes.\n\n# retrieve ordering\ndds_cat['cohort_order'] = dds_cat['Age Cohort'].cat.codes.head()\ndds_cat.head()\n\n\n\n\n\n\n\n\nId\nAge Cohort\nAge\nGender\nExpenditures\nEthnicity\ncohort_order\n\n\n\n\n0\n10210\n13 to 17\n17\nFemale\n2113\nWhite not Hispanic\n2.0\n\n\n1\n10409\n22 to 50\n37\nMale\n41924\nWhite not Hispanic\n4.0\n\n\n2\n10486\n0 to 5\n3\nMale\n1454\nHispanic\n0.0\n\n\n3\n10538\n18 to 21\n19\nFemale\n6400\nHispanic\n3.0\n\n\n4\n10568\n13 to 17\n13\nMale\n4412\nWhite not Hispanic\n2.0\n\n\n\n\n\n\n\n\n\n\nQuestion 5: age structure of the sample\nHere you’ll explore the age composition of each ethnic group in the sample.\n\n\nGroup the data by ethnic group and tabulate the sample sizes for each group. Use dds_cat so that the order of age cohorts is preserved. Store the result as samp_sizes.\n\n\nVisualize the age structure of each ethnic group in the sample. Construct a point-and-line plot of the sample size (y) against age cohort (x) by ethnicity (color or linetype). Make sure to preserve the ordering of age cohorts on the x axis (hint: create a variable like cohort_order above). Store the plot as fig_3 and display.\n\n\nComment on the figure. Are there differences in age composition by ethnic group among the individuals sampled?\nType your answer here, replacing this text.\n\n# compute sample sizes for each age/ethnic group\nsamp_sizes = dds_cat.groupby(\n    ['Age Cohort', 'Ethnicity'] # SOLUTION\n).Id.count().reset_index().rename(\n    columns = {'Id': 'n'} # SOLUTION\n)\n\n# construct plot\n# BEGIN SOLUTION\n# add column with category codes\nsamp_sizes['cohort_order'] = samp_sizes['Age Cohort'].cat.codes # SOLUTION\n\nfig_3 = alt.Chart(samp_sizes).mark_line(point = True).encode(\n    x = alt.X('Age Cohort', \n              title = '',\n              sort = alt.EncodingSortField(field = 'cohort_order', \n                                           order = 'ascending')),\n    y = alt.Y('n', \n              title = 'Sample size', \n              scale = alt.Scale(type = 'sqrt')),\n    color = 'Ethnicity'\n)\n# END SOLUTION\n\n# display\nfig_3 # SOLUTION\n\n\n\n\n\n\nSOLUTION\nThere are differences in age; for example, the white non-hispanic group comprises more individuals in the 22-50 cohort, whereas the hispanic group comprises more individuals in the 13-17 cohort.\n\nAge structure among ethnic groups might be related to the observed differences in median expenditure, because we know that:\n\n\namong the individuals in the sample, age distributions differed by ethnic group\n\n\nage is related to benefit expenditure\n\n\nTo see this, think through an example.\n\n\n\nQuestion 6: potential confounding\nLook at the age distribution for Multi Race and consider the age-expenditure relationship. Can you explain why the median expenditure for this group might be lower than the others? Answer in 1-2 sentences.\nType your answer here, replacing this text.\nSOLUTION\nFor the multiracial group, most recipients are under 17, and younger cohorts tend to recieve less expenditure.\n\n\n\n\nQuestion 7: correcting for age\nHopefully, the last few prompts convinced you that the apparent discrimination could simply be an artefact of differing age structure. You can investigate this by plotting median expenditure against ethnicity, as in figure 1, but now also correcting for age cohort.\nConstruct an Altair point-and-line chart based on dds_cat with:\n\nethnicity on the x axis\nno x axis label\nmedian expenditure on the y axis\nthe y axis displayed on the log scale\nage cohort mapped to color and sorted in order of age\nlines connecting points that display the median expenditure for each ethnicity and cohort, with one line per age cohort\n\nStore the result as fig_4 and display the graphic.\n\n# construct plot\n# BEGIN SOLUTION\nfig_4 = alt.Chart(dds_cat).mark_line(point = True).encode(\n    x = alt.X('Ethnicity', title = ''),\n    y = alt.Y('median(Expenditures)', \n              scale = alt.Scale(type = 'log')),\n    color = alt.Color('Age Cohort:O', \n                      sort = alt.EncodingSortField(field = 'cohort_order', \n                                                   order = 'ascending'))\n)\n# END SOLUTION\n\n# display\nfig_4 # SOLUTION\n\n\n\n\n\n\n\n\n\n\nRegression analysis\nNow that you’ve thoroughly explored the data, you’ll use a linear model in this part to estimate the differences in median expenditure that you observed graphically in part 1.\nMore specifically, you’ll model the log of expenditures (response variable) as a function of gender, age cohort, and ethnicity:\n\\[\n\\log\\left(\\text{expend}_i\\right)\n    = \\beta_0 + \\underbrace{\\beta_1\\left(\\text{6-12}\\right)_i + \\cdots + \\beta_5\\left(\\text{51+}\\right)_i}_\\text{age cohort} + \\underbrace{\\beta_6\\text{female}_i}_\\text{sex} + \\underbrace{\\beta_7\\text{hispanic}_i + \\cdots + \\beta_{13}\\text{other}_i}_\\text{ethnicity} + \\epsilon_i\n\\]\nIn this model, all of the explanatory variables are categorical and encoded using indicators; in this case, the linear model coefficients capture means for each group.\nBecause this model is a little different than the examples you’ve seen so far in two respects – the response variable is log-transformed and all explanatory variables are categorical – some comments are provided below on these features. You can review or skip the comments, depending on your level of interest in understanding the model better mathematically.\nCommments about parameter interpretation\nIn particular, each coefficient represents a difference in means from the ‘baseline’ group. All indicators are zero for a white male recipient between ages 0 and 5, so this is the baseline group and:\n\\[\\mathbb{E}\\left(\\log(\\text{expend})\\;|\\; \\text{male, white, 0-5}\\right) = \\beta_1\\]\nThen, the expected log expenditure for a hispanic male recipient between ages 0 and 5 is:\n\\[\\mathbb{E}\\left(\\log(\\text{expend})\\;|\\; \\text{male, hispanic, 0-5}\\right) = \\beta_0 + \\beta_7\\]\nSo \\(\\beta_7\\) is the difference in mean log expenditure between hispanic and white recipients after accounting for gender and age. The other parameters have similar interpretations.\nWhile the calculation shown above may seem a little foreign, you should know that the parameters represent marginal differences in means between genders (holding age and ethnicity fixed), between ages (holding gender and ethnicity fixed), and between ethnicities (holding age and gender fixed).\nComments about the log transformation\nThe response in this model is the log of expenditures (this gives a better model for a variety of reasons). The statistical assumption then becomes that:\n\\[\\log(\\text{expend})_i \\sim N\\left(\\mathbf{x}_i'\\beta, \\sigma^2\\right)\\]\nIf the log of a random variable \\(Y\\) is normal, then \\(Y\\) is known as a lognormal random variable; it can be shown mathematically that the exponentiated mean of \\(\\log Y\\) is the median of \\(Y\\). As a consequence, according to our model:\n\\[\\text{median}(\\text{expend}_i) = \\exp\\left\\{\\mathbf{x}_i'\\beta\\right\\}\\]\nYou’ll work on the log scale throughout to avoid complicating matters, but know that this model for the log of expenditures is equivalently a model of the median expenditures.\nThe cell below reorders the category levels to match the model written above. To ensure the parameters appear in the proper order, this reordering is done for you.\n\n# remove ID and quantitative age\nreg_data = dds_cat.copy().drop(columns = ['Id', 'Age'])\n\n# reorder ethnicity\nreg_data['Ethnicity'] = reg_data.Ethnicity.cat.as_ordered().cat.reorder_categories(\n    reg_data.Ethnicity.cat.categories[[7, 3, 2, 1, 5, 0, 4, 6]]\n)\n\n# reorder gender\nreg_data['Gender'] = reg_data.Gender.cat.as_ordered().cat.reorder_categories(['Male', 'Female'])\n\n\nQuestion 8: Data preprocessing\nObtain the explanatory variable matrix and response vector needed to fit the linear model.\n\nUse pd.get_dummies(..., drop_first = True) to create the indicator variable encodings for gender, ethnicity, and age. Note that this function can process multiple categorical variables at once. Store the data frame of indicators for all three variables as indicators.\nAdd an intercept to obtain the explanatory variable matrix. Store this as a data frame called x.\nStore the response variable as a pandas series named y.\n\n\nindicators = pd.get_dummies(reg_data.loc[:, ['Gender', 'Ethnicity', 'Age Cohort']], drop_first = True) # SOLUTION\nx = sm.tools.add_constant(indicators) # SOLUTION\ny = np.log(reg_data.Expenditures) # SOLUTION\n\n\ngrader.check(\"q8\")\n\n\n\nQuestion 9: model fitting\nFit the model:\n\\[\n\\log\\left(\\text{expend}_i\\right)\n    = \\beta_0 + \\beta_1\\left(\\text{6-12}\\right)_i + \\cdots + \\beta_5\\left(\\text{51+}\\right)_i\n        + \\beta_6\\text{female}_i\n        + \\beta_7\\text{hispanic}_i + \\cdots + \\beta_{13}\\text{other}_i\n        + \\epsilon_i\n\\]\nStore the parameter estimates and standard errors as a data frame named coef_tbl. Index the data frame by variable name, and don’t forget to include the error variance estimate. Display the result.\n\n# fit model\nmlr = sm.OLS(endog = y, exog = x) # SOLUTION\nrslt = mlr.fit() # SOLUTION\n\n# retreive estimates and std errors\ncoef_tbl = pd.DataFrame(\n    # BEGIN SOLUTION\n    {'estimate': rslt.params, \n    'standard error': np.sqrt(rslt.cov_params().values.diagonal())},\n    index = x.columns\n    # END SOLUTION\n)\ncoef_tbl.loc['error variance', 'estimate'] = rslt.scale # SOLUTION\n\n# display\ncoef_tbl\n\n\n\n\n\n\n\n\nestimate\nstandard error\n\n\n\n\nconst\n7.092439\n0.041661\n\n\nGender_Female\n0.039784\n0.020749\n\n\nEthnicity_Hispanic\n0.038594\n0.024893\n\n\nEthnicity_Black\n0.041713\n0.045725\n\n\nEthnicity_Asian\n-0.021103\n0.033470\n\n\nEthnicity_Native Hawaiian\n-0.030725\n0.189967\n\n\nEthnicity_American Indian\n-0.054396\n0.164910\n\n\nEthnicity_Multi Race\n0.041024\n0.067680\n\n\nEthnicity_Other\n-0.189877\n0.232910\n\n\nAge Cohort_6 to 12\n0.490276\n0.043855\n\n\nAge Cohort_13 to 17\n1.101010\n0.042783\n\n\nAge Cohort_18 to 21\n2.023844\n0.043456\n\n\nAge Cohort_22 to 50\n3.470836\n0.043521\n\n\nAge Cohort_51+\n3.762393\n0.049561\n\n\nerror variance\n0.107005\nNaN\n\n\n\n\n\n\n\n\ngrader.check(\"q9\")\n\nNow look at both the estimates and standard errors for each level of each categorical variable; if some estimates are large for at least one level and the standard errors aren’t too big, then estimated mean log expenditures differ according to the value of that variable when the other variables are held constant.\nFor example: the estimate for Gender_Female is 0.04; that means that, if age and ethnicity are held fixed, the estimated difference in mean log expenditure between female and male recipients is 0.04. If \\(\\log(a) - \\log(b) = 0.04\\), then \\(\\frac{a}{b} = e^{0.04} \\approx 1.041\\); so the estimated expenditures (not on the log scale) differ by a factor of about 1, i.e., are about the same. Further, the standard error is 0.02, so the estimate is within 2SE of 0; the difference could well be zero. So the model suggests there is no difference in expenditure by gender.\n\n\n\nQuestion 10: interpretation\nDo the parameter estimates suggest differences in expenditure by age or ethnicity?\nFirst consider the estimates and standard errors for each level of age, and state whether any differences in mean log expenditure between levels appear significant; if so, cite one example. Then do the same for the levels of ethnicity. Answer in 2-4 sentences.\n(Hint: it may be helpful scratch work to exponentiate the coefficient estimates and consider whether they differ by much from 1.)\nType your answer here, replacing this text.\n\n# exponentiate parameter estimates\nnp.exp(rslt.params) # SOLUTION\n\nconst                        1202.838256\nGender_Female                   1.040586\nEthnicity_Hispanic              1.039348\nEthnicity_Black                 1.042595\nEthnicity_Asian                 0.979118\nEthnicity_Native Hawaiian       0.969742\nEthnicity_American Indian       0.947057\nEthnicity_Multi Race            1.041877\nEthnicity_Other                 0.827061\nAge Cohort_6 to 12              1.632767\nAge Cohort_13 to 17             3.007203\nAge Cohort_18 to 21             7.567356\nAge Cohort_22 to 50            32.163632\nAge Cohort_51+                 43.051330\ndtype: float64\n\n\nSOLUTION\nThe estimates suggest significant differences in expenditure between all age cohorts; for example, the 22-50 cohort receives an estimated 32 times the median expenditure (see comments on the log transform) of the 0-5 cohort, after accounting for gender and ethnicity. The estimates suggest no significant differences according to ethnicity; all estimated differences are near 0 on the log scale, and near 1 when exponentiated.\n\nNow as a final step in the analysis, you’ll visualize your results. The idea is simple: plot the estimated mean log expenditures for each group. Essentially you’ll make a version of your figure 4 from part 1 in which the points are estimated rather than observed. So the model visualization graphic will look similar to your exploratory figure.\nThe cell below constructs a prediction grid for you. This grid comprises all unique combinations of the age, sex, and ethnicity categories.\n\n# obtain unique values of each categorical variable\ngenders = reg_data.Gender.cat.categories.values\nethnicities = reg_data.Ethnicity.cat.categories.values\nages = reg_data['Age Cohort'].cat.categories.values\n\n# generate mesh\ngx, ex, ax = np.meshgrid(genders, ethnicities, ages)\ngrid = np.array([gx.ravel(), ex.ravel(), ax.ravel()]).T\n\n# coerce to dataframe\ngrid_df = pd.DataFrame(grid, columns = ['Gender', 'Ethnicity', 'Age Cohort'])\ngrid_df.head()\n\n\n\n\n\n\n\n\nGender\nEthnicity\nAge Cohort\n\n\n\n\n0\nMale\nWhite not Hispanic\n0 to 5\n\n\n1\nMale\nWhite not Hispanic\n6 to 12\n\n\n2\nMale\nWhite not Hispanic\n13 to 17\n\n\n3\nMale\nWhite not Hispanic\n18 to 21\n\n\n4\nMale\nWhite not Hispanic\n22 to 50\n\n\n\n\n\n\n\n\n\nQuestion 11: compute predictions\nCalculate predictions with confidence intervals for the predicted mean for each grid point; append these to grid_df and store the result as pred_df. Ensure that the column containing the predictions is named mean.\nNote that you will need to generate indicators in order to compute the predictions; this can be done in the same way that the data were preprocessed. Note also that you will need to arrange the indicator columns in exactly the same order that they appear in the explanatory variable matrix in order to generate valid predictions.\n\n# generate indicators based on the prediction grid\ngrid_indicators = pd.get_dummies(grid_df, prefix = None).drop(columns = {'Gender_Male', 'Ethnicity_White not Hispanic', 'Age Cohort_0 to 5'}) # SOLUTION\n\n# add an intercept and arrange columns to match x\nx_grid = sm.tools.add_constant(grid_indicators).loc[:, x.columns.values.tolist()] # SOLUTION\n\n# compute predictions\npreds = rslt.get_prediction(x_grid) # SOLUTION\n\n# append values of categorical variables at grid points to predictions\npred_df = pd.concat([grid_df, preds.summary_frame()], axis = 1) # SOLUTION\n\n# preview\npred_df.head()\n\n\n\n\n\n\n\n\nGender\nEthnicity\nAge Cohort\nmean\nmean_se\nmean_ci_lower\nmean_ci_upper\nobs_ci_lower\nobs_ci_upper\n\n\n\n\n0\nMale\nWhite not Hispanic\n0 to 5\n7.092439\n0.041661\n7.010685\n7.174194\n6.445331\n7.739548\n\n\n1\nMale\nWhite not Hispanic\n6 to 12\n7.582715\n0.032020\n7.519881\n7.645550\n6.937724\n8.227707\n\n\n2\nMale\nWhite not Hispanic\n13 to 17\n8.193450\n0.029301\n8.135950\n8.250950\n7.548956\n8.837943\n\n\n3\nMale\nWhite not Hispanic\n18 to 21\n9.116283\n0.029439\n9.058513\n9.174053\n8.471765\n9.760801\n\n\n4\nMale\nWhite not Hispanic\n22 to 50\n10.563276\n0.025658\n10.512926\n10.613626\n9.919380\n11.207171\n\n\n\n\n\n\n\n\ngrader.check(\"q11\")\n\n\n\n\nQuestion 12: model visualization\nPlot estimated mean log expenditure (y) against ethnicity (x) by age cohort (color) and gender (facet). Construct a line plot with points at each estimated value, and include confidence bands. Use a sequential color scale for age and ensure that the age cohorts are in appropriate order (you may want to construct another cohort_order variable as before for this purpose).\n\n# add cohort ordering\npred_df['cohort_order'] = pred_df['Age Cohort'].astype('category').cat.reorder_categories(new_categories = ages).cat.as_ordered().cat.codes # SOLUTION\n\n# construct point-and-line plot\nlines = alt.Chart(pred_df).mark_line(point = True).encode(\n    x = alt.X('Ethnicity', title = ''), # SOLUTION\n    y = alt.Y('mean', title = 'Estimated log median expentiure', scale = alt.Scale(zero = False)), # SOLUTION\n    color = alt.Color('Age Cohort:O', sort = alt.EncodingSortField(field = 'cohort_order', order = 'ascending')) # SOLUTION\n)\n\n# construct confidence bands\nbands = alt.Chart(pred_df).mark_area(opacity = 0.4).encode(\n    x = 'Ethnicity', # SOLUTION\n    y = 'mean_ci_lower', # SOLUTION\n    y2 = 'mean_ci_upper', # SOLUTION\n    color = alt.Color('Age Cohort:O', sort = alt.EncodingSortField(field = 'cohort_order', order = 'ascending')) # SOLUTION\n)\n\n# layer then facet\nfig_5 = (lines + bands).facet(column = 'Gender') # SOLUTION\n\n# display\nfig_5\n\n\n\n\n\n\n\n\n\n\nQuestion 13: uncertainty\nWhich estimates have greater uncertainty and why? Identify the ethnic groups for which the uncertainty band is relatively wide in the plot. Why might uncertainty be higher for these groups? Answer in 2 sentences.\n(Hint: it may help to refer to figure 3.)\nType your answer here, replacing this text.\nSOLUTION\nEstimated expenditures for American Indian, Other, and Native Hawaiian all have high uncertainty. This is because the sample sizes are small for those groups; with relatively little information about them in the data, estimates are likely to vary more.\n\n\n\n\nCommunicating results\nReview your exploratory and regression analyses above, and then answer the following questions.\n\n\nQuestion 14: summary\nWrite a one-paragraph summary of your analysis. Focus on answering the question, ‘do the data provide evidence of ethnic or gender discrimination in allocation of DDS funds?’\nYour summary should include the following:\n\na description of the data indicating observations, variables, and sampling mechanism;\na description of any important exploratory findings;\na description of the method you used to analyze the data (don’t worry about capturing every detail);\na description of the findings of the analysis;\nan answer to the question.\n\nType your answer here, replacing this text.\nSOLUTION\nThis case study investigated potential discrimination in allocation of California Department of Developmental Services (DDS) benefit funds for people with disabilities. Data on 1000 randomly selected benefit recipients were analyzed; the age, ethnicity, and gender of the recipient were recorded along with benefit amount. While there is an apparent difference in the median benefit amount by ethnic group, after considering the age of the recipients there are no substantial differences. A regression analysis was conducted to estimate groupwise differences in benefit amounts, and showed that while there were significant differences between age cohorts, the benefit amounts were similar across ethnicities and genders after adjusting for age. The data thus provide no evidence of discrimination.\n\n\n\n\nSubmission\n\nSave the notebook.\nRestart the kernel and run all cells. (CAUTION: if your notebook is not saved, you will lose your work.)\nCarefully look through your notebook and verify that all computations execute correctly and all graphics are displayed clearly. You should see no errors; if there are any errors, make sure to correct them before you submit the notebook.\nDownload the notebook as an .ipynb file. This is your backup copy.\nExport the notebook as PDF and upload to Gradescope.\n\n\nTo double-check your work, the cell below will rerun all of the autograder tests.\n\ngrader.check_all()"
  },
  {
    "objectID": "hw/hw2-seda/hw2-seda.html",
    "href": "hw/hw2-seda/hw2-seda.html",
    "title": "Background",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"hw2-seda.ipynb\")\nimport numpy as np\nimport pandas as pd\nimport altair as alt\n# disable row limit for plotting\nalt.data_transformers.disable_max_rows()\n# uncomment to ensure graphics display with pdf export\n# alt.renderers.enable('mimetype')\nGender achievement gaps in education have been well-documented over the years – studies consistently find boys outperforming girls on math tests and girls outperforming boys on reading and language tests. A particularly controversial article was published in Science in 1980 arguing that this pattern was due to an ‘innate’ difference in ability (focusing on mathematics rather than on reading and language). Such views persisted in part because studying systematic patterns in achievement nationwide was a challenge due to differential testing standards across school districts and the general lack of availability of large-scale data.\nIt is only recently that data-driven research has begun to reveal socioeconomic drivers of achievement gaps. The Standford Educational Data Archive (SEDA), a publicly available database on academic achievement and educational opportunity in U.S. schools, has supported this effort. The database is part of a broader initiave aiming to improve educational opportunity by enabling researchers and policymakers to identify systemic drivers of disparity.\nThe database standardizes average test scores for schools 10,000 U.S. school districts relative to national standards to allow comparability between school districts and across grade levels and years. The test score data come from the U.S. Department of Education. In addition, multiple data sources (American Community Survey and Common Core of Data) are integrated to provide district-level socioeconomic and demographic information.\nA study of the SEDA data published in 2018 identified the following persistent patterns across grade levels 3 - 8 and school ears from 2008 through 2015: * a consistent reading and language achievement gap favoring girls; * no national math achievement gap on average; and * local math achievement gaps that depend on the socioeconomic conditions of school districts. You can read about the main findings of the study in this brief NY Times article.\nBelow, we’ll work with selected portions of the database. The full datasets can be downloaded here."
  },
  {
    "objectID": "hw/hw2-seda/hw2-seda.html#assignment-objectives",
    "href": "hw/hw2-seda/hw2-seda.html#assignment-objectives",
    "title": "Background",
    "section": "Assignment objectives",
    "text": "Assignment objectives\nIn this assignment, you’ll explore achievement gaps in California school districts in 2018, reproducing the findings described in the article above on a more local scale and with the most recent SEDA data. You’ll practice the following:\n\nreview of data documentation\nassessment of sampling design and scope of inference\ndata tidying operations\n\nslicing and filtering\nmerging multiple data frames\npivoting tables\nrenaming and reordering variables\n\nconstructing exploratory graphics and visualizing trends\ndata aggregations\nnarrative summary of exploratory analysis"
  },
  {
    "objectID": "hw/hw2-seda/hw2-seda.html#test-score-data",
    "href": "hw/hw2-seda/hw2-seda.html#test-score-data",
    "title": "Background",
    "section": "Test score data",
    "text": "Test score data\nThe first few rows of the test data are shown below. The columns are:\n\n\n\n\n\n\n\nColumn name\nMeaning\n\n\n\n\nsedalea\nDistrict ID\n\n\ngrade\nGrade level\n\n\nstateabb\nState abbreviation\n\n\nsedaleaname\nDistrict name\n\n\nsubject\nTest subject\n\n\ncs_mn_...\nEstimated mean test score\n\n\ncs_mnse_...\nStandard error for estimated mean test score\n\n\ntotgyb_...\nNumber of individual tests used to estimate the mean score\n\n\n\n\n# import seda data\nca_main = pd.read_csv('data/ca-main.csv')\nca_cov = pd.read_csv('data/ca-cov.csv')\n\n# preview test score data\nca_main.head(3)\n\nThe test score means for each district are named cs_mn_... with an abbreviation indicating subgroup (such as mean score for all cs_mean_all, for boys cs_mean_mal, for white students cs_mn_wht, and so on). Notice that these are generally small-ish: decimal numbers between -0.5 and 0.5.\nThese means are estimated from a number of individual student tests and standardized relative to national averages. They represent the number of standard deviations by which a district mean differs from the national average. So, for instance, the value cs_mn_all = 0.1 indicates that the district average is estimated to be 0.1 standard deviations greater than the national average on the corresponding test and at the corresponding grade level.\n\n\nQuestion 1: Interpreting test score values\nInterpret the average math test score for all 4th grade students in Acton-Agua Dulce Unified School District (the first row of the dataset shown above).\nType your answer here, replacing this text."
  },
  {
    "objectID": "hw/hw2-seda/hw2-seda.html#covariate-data",
    "href": "hw/hw2-seda/hw2-seda.html#covariate-data",
    "title": "Background",
    "section": "Covariate data",
    "text": "Covariate data\nThe first few rows of the covariate data are shown below. The column information is as follows:\n\n\n\nColumn name\nMeaning\n\n\n\n\nsedalea\nDistrict ID\n\n\ngrade\nGrade level\n\n\nsedaleanm\nDistrict name\n\n\nurban\nIndicator: is the district in an urban locale?\n\n\nsuburb\nIndicator: is the district in a suburban locale?\n\n\ntown\nIndicator: is the district in a town locale?\n\n\nrural\nIndicator: is the district in a rural locale?\n\n\nlocale\nDescription of district locale\n\n\nRemaining variables\nDemographic and socioeconomic measures\n\n\n\n\nca_cov.head(3)\n\nYou will only be working with a handful of the demographic and socioeconomic measures, so you can put off getting acquainted with those until selecting a subset of variables.\n\n\nQuestion 2: Data semantics\nIn the non-public data, observational units are students – test scores are measured for each student. However, in the SEDA data you’ve imported, scores are aggregated to the district level by grade. Let’s regard estimated test score means for each grade as distinct variables, so that an observation consists in a set of estimated means for different grade levels and groups. In this view, what are the observational units in the test score dataset? Are they the same or different for the covariate dataset?\nType your answer here, replacing this text.\n\n\n\nQuestion 3: Sample sizes\nHow many observational units are in each dataset? Count the number of units in the test dataset and the number of units in the covariate dataset separately. Store the values as ca_cov_units and ca_main_units, respectively.\n(Hint: use .nunique().)\n\nca_cov_units = ...\nca_main_units = ...\n\nprint('units in covariate data: ', ca_cov_units)\nprint('units in test score data: ', ca_main_units)\n\n\ngrader.check(\"q3\")\n\n\n\n\nQuestion 4: Sample characteristics and scope of inference\nAnswer the questions below about the sampling design in a short paragraph. You do not need to dig through any data documentation in order to resolve these questions.\n\n\nWhat is the relevant population for the datasets you’ve imported?\n\n\nAbout what proportion (to within 0.1) of the population is captured in the sample? (Hint: have a look at this website.)\n\n\nConsidering that the sampling frame is not identified clearly, what kind of dataset do you suspect this is (e.g., administrative, data from a ‘typical sample’, census, etc.)?\n\n\n\nIn light of your description of the sample characteristics, what is the scope of inference for this dataset?\n\n\nType your answer here, replacing this text."
  },
  {
    "objectID": "hw/hw2-seda/hw2-seda.html#gender-gaps-and-socioeconomic-factors",
    "href": "hw/hw2-seda/hw2-seda.html#gender-gaps-and-socioeconomic-factors",
    "title": "Background",
    "section": "Gender gaps and socioeconomic factors",
    "text": "Gender gaps and socioeconomic factors\nThe cell below generates a panel of scatterplots showing the relationship between estimated gender gap and socioeconomic factors for all grade levels by test subject. The plot suggests that the reading gap favors girls consistently across the socioeconomic spectrum – in a typical district girls seem to outperform boys by 0.25 standard deviations of the national average. By contrast, the math gap appears to depend on socioeconomic factors – boys only seem to outperform girls under better socioeconomic conditions.\n\n# plot gap against socioeconomic variables by subject for all grades\nfig1 = alt.Chart(plot_df).mark_circle(opacity = 0.1).encode(\n    y = 'Gap',\n    x = alt.X('Measure', scale = alt.Scale(zero = False), title = ''),\n    color = 'Gap type'\n).properties(\n    width = 100,\n    height = 100\n).facet(\n    column = alt.Column('Socioeconomic variable')\n).resolve_scale(x = 'independent')\n\nfig1\n\n\n\nQuestion 13: Relationships by grade level\nDoes the pattern shown in the plot above persist within each grade level? Modify the plot above to show these relationships by grade level: generate a panel of scatterplots of gap against socioeconomic measures by subject, where each column of the panel corresponds to one socioeconomic variable and each row corresponds to one grade level; the result should by a 5x5 panel. Resize the width and height of each facet so that the panel is of reasonable size. Keep a fixed axis scale for the variable of interest, but allow the axis scales for socioeconomic variables to vary independently. Store the plot as fig2; display the figure and provide an answer to the question of interest in the text cell.\n(Hint: you may find it useful to have a look at the altair documentation on compound charts, and lab 3, for examples to follow.)\nType your answer here, replacing this text.\n\n# plotting codes here\n...\n\n# display\n...\n\n\n\n\n\nQuestion 14: Association with grade level\nDo gaps shift across grade levels? It’s not so easy to tell from the last figure. Construct a 2x5 panel of scatterplots showing estimated achievement gap against each of the 5 socioeconomic variables, with one row per test subject. Display grade level using a color gradient. Store the plot as fig3; display the figure and answer the question of interest in a short sentence or two in the text cell provided.\nType your answer here, replacing this text.\n\n# plotting codes here\n...\n\n# display\n...\n\n\nWhile the magnitude of the achievement gaps seems to depend very slightly on grade level (figure 3), the form of relationship between achievement gap and socioeconomic factors does not differ from grade to grade (figure 2).\nGiven that the relationships between achievement gaps and socioeconomic factors don’t change drastically across grade levels, it is reasonable to look at the average relationship between estimated achievement gap and median income after aggregating across grade.\n\n\nQuestion 15: Aggregation across grade levels\nCompute the mean estimated achievement gap in each subject across grade levels by district using District ID and retain the district-level socioeconomic variables. Store the resulting data frame as seda_data_agg.\nNote: best practice here would be to aggregate just the test scores by district and then re-merge the result with the district-level socioeconomic variables. However, since the district-level socioeconomic variables do not differ by grade within a district, averaging them across grade levels by district together with the test scores will simply return their unique values; so the aggregation can be applied across all columns for a fast-and-loose way to obtain the desired result.\n\n# aggregate across grades\n...\n\n# print first few rows\n...\n\n\ngrader.check(\"q15\")\n\n\n\nQuestion 16: Melt aggregated data for plotting\nSimilar to working with the disaggregated data, it will be helpful for plotting to melt the two gap variables into a single column. Follow the example above at the beginning of this section to melt only the test score gap columns (not the district-level variables – we will not create scatterplot panels as before). Name the new columns Subject and Average estimated gap; store the resulting data frame as agg_plot_df and print the first four rows.\n\n# format for plotting\n...\n\n# print four rows\n...\n\n\ngrader.check(\"q16\")\n\n\n\n\nQuestion 17: District average gaps\nConstruct a scatterplot of the average estimated gap against log(Median income) by subject for each district and add trend lines (see lab 4). Store the plot as fig4. Describe and interpret the plot in a few sentences.\nType your answer here, replacing this text.\n\n# scatterplot\n...\n\n# trend line\ntrend = ...\n\n# combine layers\nfig4 = ...\n\n# display\n...\n\n\nNow let’s try to capture this pattern in tabular form. The cell below adds an Income bracket variable by cutting the median income into 8 contiguous intervals using pd.cut(), and tabulates the average socioeconomic measures and estimated gaps across districts by income bracket. Notice that with respect to the gaps, this displays the pattern that is shown visually in the figures above.\n\nseda_data_agg['Income bracket'] = pd.cut(np.e**seda_data_agg['log(Median income)'], 8)\nseda_data_agg.groupby('Income bracket').mean().drop(columns = ['District ID', 'log(Median income)'])\n\n\n\nQuestion 18: Proportion of districts with a math gap\nWhat proportion of districts in each income bracket have an average estimated math achievement gap favoring boys? Answer this question by performing the following steps:\n\nAppend an indicator variable Math gap favoring boys to seda_data_agg that records whether the average estimated math gap favors boys by more than 0.1 standard deviations relative to the national average.\nCompute the proportion of districts in each income bracket for which the indicator is true: group by bracket and take the mean. Store this as income_bracket_boys_favored\n\n\n# define indicator\nseda_data_agg['Math gap favoring boys'] = ...\n\n# proportion of districts with gap favoring boys, by income bracket\n...\n\n# print result\n...\n\n\ngrader.check(\"q18\")\n\n\n\nQuestion 19: Statewide averages\nTo wrap up the exploration, calculate a few statewide averages to get a sense of how some of the patterns above compare with the state as a whole.\n\n\nCompute the statewide average estimated achievement gaps. Store the result as state_avg.\n\n\nCompute the proportion of districts in the state with a math gap favoring boys. Store this result as math_boys_proportion\n\n\nCompute the proportion of districts in the state with a math gap favoring girls. You will need to define a new indicator within seda_data_agg to perform this calculation.\n\n\n\n# statewide average\nstate_avg = ...\n\n# proportion of districts in the state with a math gap favoring boys\nmath_boys_proportion = ...\n\n# proportion of districts in the state with a math gap favoring girls\nseda_data_agg['Math gap favoring girls'] = ...\nmath_girls_proportion = ...\n\n\ngrader.check(\"q19\")"
  },
  {
    "objectID": "hw/hw3-diatom/hw3-diatom-soln.html",
    "href": "hw/hw3-diatom/hw3-diatom-soln.html",
    "title": "Background: diatoms and paleoclimatology",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"hw3-diatom.ipynb\")\n\n\nimport numpy as np\nimport pandas as pd\nimport altair as alt\nfrom sklearn.decomposition import PCA\nimport statsmodels.api as sm\n# disable row limit for plotting\nalt.data_transformers.disable_max_rows()\n# uncomment to ensure graphics display with pdf export\n# alt.renderers.enable('mimetype')\n\nDataTransformerRegistry.enable('default')\n\n\nDiatoms are a type of phytoplankton – they are photosynthetic algae that function as primary producers in aquatic ecosystems. Diatoms are at the bottom of the food web: they are consumed by filter feeders, like clams, mussels, and many fish, which are in turn consumed by larger organisms like scavengers and predators and, well, us. As a result, changes in the composition of diatom species in marine ecosystems have ripple effects that can dramatically alter overall community structure in any environment of which marine life forms a part.\nDiatoms have glass bodies. As a group of organisms, they display a great diversity of body shapes, and many are quite elaborate. The image below, taken from a Scientific American article, shows a small sample of their shapes and structures.\n\n\n\nBecause they are made of glass, diatoms preserve extraordinarily well over time. When they die, their bodies sink and form part of the sediment. Due to their abundance, there is a sort of steady rain of diatoms forming part of the sedimentation process, which produces sediment layers that are dense with diatoms.\nSedimentation is a long-term process spanning great stretches of time, and the deeper one looks in sediment, the older the material. Since diatoms are present in high density throughout sedimentation layers, and they preserve so well, it is possible to study their presence over longer time spans – potentially hundreds of thousands of years.\nA branch of paleoclimatology is dedicated to studying changes in biological productivity on geologic time scales, and much research in this area has involved studying the relative abundances of diatoms. In this assignment, you’ll do just that on a small scale and work with data from sediment cores taken in the gulf of California at the location indicated on the map:\n\n\n\nThe data is publicly available: &gt; Barron, J.A., et al. 2005. High Resolution Guaymas Basin Geochemical, Diatom, and Silicoflagellate Data. IGBP PAGES/World Data Center for Paleoclimatology Data Contribution Series # 2005-022. NOAA/NGDC Paleoclimatology Program, Boulder CO, USA.\nIn this assignment, you’ll use the exploratory techniques we’ve been discussing in class to analyze the relative abundances of diatom taxa over a time span of 15,000 years. This will involve practicing the following:\n\ndata import and preprocessing\ngraphical techniques for visualizing distributions\nmultivariate analysis with PCA\n\n\nDiatom data\nThe data are diatom counts sampled from evenly-spaced depths in a sediment core from the gulf of California. In sediment cores, depth correlates with time before the present – deeper layers are older – and depths are typically chosen to obtain a desired temporal resolution. The counts were recorded by sampling material from sediment cores at each depth, and examining the sampled material for phytoplankton cells. For each sample, phytoplankton were identified at the taxon level and counts of diatom taxa were recorded along with the total number of phytoplankton cells identified. Thus:\n\nThe observational units are sediment samples.\n\nThe variables are depth (age), diatom abundance counts, and the total number of identified phytoplankton. Age is inferred from radiocarbon.\nOne observation is made at each depth from 0cm (surface) to 13.71 cm.\n\nThe table below provides variable descriptions and units for each column in the dataframe.\n\n\n\n\n\n\n\n\nVariable\nDescription\nUnits\n\n\n\n\nDepth\nDepth interval location of sampled material in sediment core\nCentimeters (cm)\n\n\nAge\nRadiocarbon age\nThousands of years before present (KyrBP)\n\n\nA_curv\nAbundance of Actinocyclus curvatulus\nCount (n)\n\n\nA_octon\nAbundance of Actinocyclus octonarius\nCount (n)\n\n\nActinSpp\nAbundance of Actinoptychus species\nCount (n)\n\n\nA_nodul\nAbundance of Azpeitia nodulifer\nCount (n)\n\n\nCocsinSpp\nAbundance of Coscinodiscus species\nCount (n)\n\n\nCyclotSpp\nAbundance of Cyclotella species\nCount (n)\n\n\nRop_tess\nAbundance of Roperia tesselata\nCount (n)\n\n\nStephanSpp\nAbundance of Stephanopyxis species\nCount (n)\n\n\nNum.counted\nNumber of diatoms counted in sample\nCount (n)\n\n\n\nThe cell below imports the data.\n\n# import diatom data\ndiatoms_raw = pd.read_csv('data/barron-diatoms.csv')\ndiatoms_raw.head(5)\n\n\n\n\n\n\n\n\nDepth\nAge\nA_curv\nA_octon\nActinSpp\nA_nodul\nCoscinSpp\nCyclotSpp\nRop_tess\nStephanSpp\nNum.counted\n\n\n\n\n0\n0.00\n1.33\n5.0\n2.0\n32\n14.0\n21\n22.0\n1.0\n1.0\n201\n\n\n1\n0.05\n1.37\n8.0\n2.0\n31\n16.0\n20\n16.0\n7.0\n2.0\n200\n\n\n2\n0.10\n1.42\n8.0\n6.0\n33\n18.0\n29\n7.0\n1.0\n1.0\n200\n\n\n3\n0.15\n1.46\n11.0\n1.0\n21\n1.0\n12\n28.0\n25.0\n3.0\n200\n\n\n4\n0.20\n1.51\n11.0\n1.0\n38\n3.0\n18\n24.0\n3.0\nNaN\n300\n\n\n\n\n\n\n\nThe data are already in tidy format, because each row is an observation (a set of measurements on one sample of sediment) and each column is a variable (one of age, depth, or counts). However, examine rows 3 and 4, and note:\n\nNaNs are present\nThe number of individuals counted in each sample varies by a lot from sample to sample.\n\nLet’s address those before conducting initial explorations.\nThe NaNs are an artefact of the data recording – if no diatoms in a particular taxa are observed, a - is entered in the table (you can verify this by checking the .csv file). In these cases the value isn’t missing, but rather zero. These entries are parsed by pandas as NaNs, but they correspond to a value of 0 (no diatoms observed).\n\nQuestion 1: Filling NaNs\nUse .fill_na() to replace all NaNs by zeros, and store the result as diatoms_mod1. Store rows 4 and 5 (index, not integer location) of the resulting dataframe as diatoms_mod1_examplerows and display these rows.\n\ndiatoms_mod1 = diatoms_raw.fillna(0) # SOLUTION\n\n# print rows 4 and 5\ndiatoms_mod1_examplerows = diatoms_mod1.loc[4:5, :] # SOLUTION\nprint(diatoms_mod1_examplerows)\n\n   Depth   Age  A_curv  A_octon  ActinSpp  A_nodul  CoscinSpp  CyclotSpp  \\\n4   0.20  1.51    11.0      1.0        38      3.0         18       24.0   \n5   0.25  1.55     4.0      9.0        30     10.0         16       14.0   \n\n   Rop_tess  StephanSpp  Num.counted  \n4       3.0         0.0          300  \n5      16.0         0.0          203  \n\n\n\ngrader.check(\"q1\")\n\nSince the total number of phytoplankton counted in each sample varies, the raw counts are not directly comparable – e.g., a count of 18 is actually a different abundance in a sample with 200 individuals counted than in a sample with 300 individuals counted.\nFor exploratory analysis, you’ll want the values to be comparable across rows. This can be achieved by a simple transformation so that the values are relative abundances: proportions of phytoplankton observed from each taxon.\n\n\nQuestion 2: Counts to proportions\nConvert the counts to proportions by dividing by the relevant entry in the Num.counted column. There are a few ways to do this, but here’s one approach:\n\nSet Depth and Age to row indices using .set_index(...) and store the result as diatoms_mod2.\nStore the Num.counted column from diatoms_mod2 as sampsize.\nUse .div(...) to divide entrywise every column in diatoms_mod2 by sampsize and store the result as diatoms_mod3.\nDrop the Num.counted column from diatoms_mod3 and reset the index; store the result as diatoms.\n\nCarry out these steps and print the first four rows of diatoms.\n(Hint: careful with the axis = ... argument in .div(...); you may want to look at the documentation and check one or two values manually to verify the calculation works as intended.)\n\n# set depth, age to indices\ndiatoms_mod2 = diatoms_mod1.set_index(['Depth', 'Age']) # SOLUTION\n\n# store sample sizes\nsampsize = diatoms_mod2['Num.counted'] # SOLUTION\n\n# divide\ndiatoms_mod3 = diatoms_mod2.div(sampsize, axis = 0) # SOLUTION\n\n# drop num.counted and reset index\ndiatoms = diatoms_mod3.drop(columns = 'Num.counted').reset_index() # SOLUTION\n\n# print\ndiatoms.head() # SOLUTION\n\n\n\n\n\n\n\n\nDepth\nAge\nA_curv\nA_octon\nActinSpp\nA_nodul\nCoscinSpp\nCyclotSpp\nRop_tess\nStephanSpp\n\n\n\n\n0\n0.00\n1.33\n0.024876\n0.009950\n0.159204\n0.069652\n0.104478\n0.109453\n0.004975\n0.004975\n\n\n1\n0.05\n1.37\n0.040000\n0.010000\n0.155000\n0.080000\n0.100000\n0.080000\n0.035000\n0.010000\n\n\n2\n0.10\n1.42\n0.040000\n0.030000\n0.165000\n0.090000\n0.145000\n0.035000\n0.005000\n0.005000\n\n\n3\n0.15\n1.46\n0.055000\n0.005000\n0.105000\n0.005000\n0.060000\n0.140000\n0.125000\n0.015000\n\n\n4\n0.20\n1.51\n0.036667\n0.003333\n0.126667\n0.010000\n0.060000\n0.080000\n0.010000\n0.000000\n\n\n\n\n\n\n\n\ngrader.check(\"q2\")\n\nTake a moment to think about what the data represent. They are relative abundances over time; essentially, snapshots of the community composition of diatoms over time, and thus information about how ecological community composition changes.\nBefore diving in, it will be helpful to resolve two matters:\n\nHow far back in time do the data go?\nWhat is the time resolution of the data?\n\n\n\nQuestion 3: Time span\nWhat is the geological time span covered by the data? Compute the minimum and maximum age using .aggregate(...) and store the result as min_max_age. (You will need to use .aggregate() to pass the automated tests.)\nNote: This may be a new function for you, but it’s simple: it takes as an argument a list of functions that will be applied to the dataframe (columnwise by default). So for example, to get the mean and variance of each column in df, one would use df.aggregate(['mean', 'var']). See the documentation for further examples.\nRemember: age is reported as thousands of years before present, so Age = 2 means 2000 years ago.\n\nmin_max_age = diatoms.Age.aggregate(['min', 'max']) # SOLUTION\nmin_max_age\n\nmin     1.33\nmax    15.19\nName: Age, dtype: float64\n\n\n\ngrader.check(\"q3\")\n\n\n\n\nQuestion 4: Time resolution\n(i) How are the observations spaced in time?\nFollow these steps:\n\nExtract the Age column from diatoms, sort the values in ascending order, compute the differences between consecutive rows, and store the result as diffs.\n\nHint: use .sort_values() and .diff().\nNotice: that the first difference is NaN, because there is no previous value to compare the first row with. Drop this entry when you store diffs.\n\nMake a simple count histogram (no need to manually bin or convert to density scale) with bins of width 0.02 (20 years). Store this as fig1 and display the figure.\n\nLabel the x axis ‘Time step between consecutive samples’\nPut the y axis on a square root scale so that bins with one observation are discernible\n\n\n(ii) What is the typical time step in years?\nWrite your answer based on the count histogram.\nType your answer here, replacing this text.\n\n# store differences\ndiffs = pd.DataFrame({'diff': diatoms.Age.sort_values().diff().loc[1:, ]}) # SOLUTION\n\n# construct histogram\nfig1 = alt.Chart(diffs).mark_bar().encode(\n    x = alt.X('diff', bin = alt.Bin(step = 0.02), title = 'Time step between consecutive samples'), # SOLUTION\n    y = alt.Y('count()', scale = alt.Scale(type = 'sqrt')) # SOLUTION\n)\n\n# display\nfig1\n\n\n\n\n\n\nSOLUTION\nMost time steps are 40-60 years.\n\ngrader.check(\"q4\")\n\n\n\n\n\nUnivariate explorations\nTo begin, you’ll examine the variation in relative abundance over time for the eight individual taxa, one variable at a time.\nHere are some initial questions in this spirit that will help you to hone in and develop more focuesed exploratory questions: * Which taxa are most and least abundant on average over time? * Which taxa vary the most over time?\nThese can be answered by computing simple summary statistics for each column in the diatom data.\n\nQuestion 5: Summary statistics\nUse .aggregate(...) to find the mean and standard deviation of relative abundances for each taxon. Follow these steps:\n\nDrop the depth and age variables before performing the aggregation.\nUse .transpose() to ensure that the table is rendered in long form (8 rows by 2 columns rather than 2 columns by 8 rows).\nStore the resulting dataframe as diatom_summary display.\n\n\ndiatom_summary = diatoms.iloc[:, 2:10].aggregate(['mean', 'std']).transpose() # SOLUTION\n\n# print the dataframe\ndiatom_summary\n\n\n\n\n\n\n\n\nmean\nstd\n\n\n\n\nA_curv\n0.028989\n0.018602\n\n\nA_octon\n0.018257\n0.016465\n\n\nActinSpp\n0.135900\n0.053797\n\n\nA_nodul\n0.072940\n0.092677\n\n\nCoscinSpp\n0.085925\n0.031795\n\n\nCyclotSpp\n0.070366\n0.042423\n\n\nRop_tess\n0.060448\n0.076098\n\n\nStephanSpp\n0.002447\n0.007721\n\n\n\n\n\n\n\n\ngrader.check(\"q5\")\n\nIt will be easier to determine which taxa are most/least abundant and most variable by displaying this information visually.\n\n\n\nQuestion 6: Visualizing summary statistics\n(i) Create a plot of the average relative abundances and their variation over time.\n\nReset the index of diatom_summary so that the taxon names are stored as a column and not an index. Store the result as plot_df.\nCreate an Altair chart based on plot_df with no marks – just alt.Chart(...).encode(...) – and pass the columnn of taxon names to the Y encoding channel with the title ‘Taxon’ and sorted in descending order of mean relative abundance. Store the result as base.\n\nHint: alt.Y(..., sort = {'field': 'column', 'order': 'descending'}) will sort the Y channel by ‘column’ in descending order.\n\nModify base to create a point plot of the average relative abundances for each taxon; store the result as means.\n\nAverage relative abundance (the mean you calculated in Q1 (a)) should appear on the x axis, and taxon on the y axis.\nSince the Y encoding was already specified in base, you do not need to add a Y encoding at this stage.\nGive the x axis the title ‘Average relative abundance’.\n\nModify base to create a plot with bars spanning two standard deviations in either direction from the mean. Store the result as bars.\n\nFirst use base.transform_calculate(...) to compute lwr and upr for the positions of the bar endpoints:\n\n\\(\\texttt{lwr} = \\texttt{mean} - 2\\times\\texttt{std}\\)\n\\(\\texttt{upr} = \\texttt{mean} + 2\\times\\texttt{std}\\).\n\nThen append .mark_errorbar().encode(...) to the chain:\n\npass lwr:Q to the X encoding channel with the title ‘Average relative abundance’ (to match the point plot)\npass upr:Q to the X2 encoding channel (no specific title needed).\n\n\nLayer the plots: means + bars. Store the result as fig2 and display the figure.\n\nIt may help to have a look at this example.\n(ii) Based on the figure, answer the following questions.\n\nWhich taxon is most abundant on average over time?\nWhich taxon is most rare on average over time?\nWhich taxon varies most in relative abundance over time?\n\nType your answer here, replacing this text.\n\n# reset index\nplot_df = diatom_summary.reset_index() # SOLUTION\n\n# create base chart\nbase = alt.Chart(plot_df).encode(\n    y = alt.Y('index', title = 'Taxon', sort = {'field': 'mean', 'order': 'descending'}) # SOLUTION\n)\n\n# create point plot\nmeans = base.mark_point().encode(\n    x = alt.X('mean', title = 'Average relative abundance') # SOLUTION\n)\n\n# create bar plot\nbars = base.transform_calculate(\n    lwr = 'datum.mean - 2*datum.std', # SOLUTION\n    upr = 'datum.mean + 2*datum.std' # SOLUTION\n).mark_errorbar().encode(\n    x = alt.X('lwr:Q', title = 'Average relative abundance'), # SOLUTION\n    x2 = 'upr:Q' # SOLUTION\n)\n\n# layer\nfig2 = means + bars # SOLUTION\n\n# display\nfig2\n\n\n\n\n\n\nSOLUTION Actinoptychus is the most abundant on average (the point for that taxon is highest). Stephanopyxis is the rarest on average. (the point for that taxon is lowest). Azpeitia nodulifer shows the most variation (the bar for that taxon is widest).\n\nNow that you have a sense of the typical abundances for each taxon (measured by means) and the variations in abundance (measured by standard deviations), you’ll dig in a bit further and examine the variation in abundance of the most variable taxon.\n\n\n\nQuestion 7: Distribution of Azpeitia nodulifer abundance over time\n(i) Construct a density scale histogram of the relative abundances of Azpeitia nodulifer and overlay a kernel density estimate.\nUse the diatoms dataframe and a bin width of 0.03. Be sure to choose an appropriate kernel and smoothing bandwidth. Store the result as fig3.\n(ii) Answer the following questions in a few sentences.\n\nWhich values are common?\nWhich values are rare?\nHow spread out are the values?\nAre values spread evenly or irregularly?\n\nType your answer here, replacing this text.\n\n# density scale histogram\nhist = alt.Chart(diatoms).transform_bin(\n    as_ = 'bin', # SOLUTION\n    field = 'A_nodul', # SOLUTION\n    bin = alt.Bin(step = 0.03) # SOLUTION\n).transform_aggregate(\n    Count = 'count()', # SOLUTION\n    groupby = ['bin'] # SOLUTION\n).transform_calculate(\n    Density = 'datum.Count/(0.03*230)', # SOLUTION\n    binshift = 'datum.bin + 0.015' # SOLUTION\n).mark_bar(size = 25, opacity = 0.8).encode(\n    x = alt.X('binshift:Q', title = 'Relative abundance', scale = alt.Scale(domain = (0.03, 0.38))), # SOLUTION \n    y = 'Density:Q' # SOLUTION\n)\n\n# compute density estimate\n# BEGIN SOLUTION\nkde = sm.nonparametric.KDEUnivariate(diatoms.A_nodul)\nkde.fit(kernel = 'triw', bw = 0.05, fft = False, cut = 0) \nkde_df = pd.DataFrame({'A_nodul': kde.support, 'Density': kde.density}) \n# END SOLUTION\n\n# plot kde\nsmooth = alt.Chart(kde_df).mark_line(color = 'black').encode(x = 'A_nodul', y = 'Density') # SOLUTION\n\n# layer\nfig3 = hist + smooth + smooth.mark_area(opacity = 0.3, color = 'black') # SOLUTION\n\n# display\nfig3\n\n\n\n\n\n\nSOLUTION Low abundances are most common. Relative abundances over 0.33 are rare. Almost all abundances are between 0 and 0.33. Values are concentrated near zero and then more or less evenly distributed between 0.06 and 0.33. Overall, the frequency of abundances decays for larger values.\n\n\nComment: There are a disproportionately large number of zeroes, because in many samples no Azpeitia nodulifer diatoms were observed. This is a common phenomenon in ecological data, and even has a name: it results in a ‘zero inflated’ distribution of values. The statistician to identify and name the phenomenon was Diane Lambert in 1992. Zero inflation can present a variety of challenges. You may have noticed, for example, that there was no bandwidth parameter for the KDE that both captured the shape of the histogram near zero and away from zero, regardless of the choice of kernel. The key difficulty with zero inflated data is that no common probability distribution fits the data well. This requires the use of mixtures as probability models, which are challenging to incorporate into common statistical models.\n\nThere was a transition between geological epochs during the time span covered by the diatom data. The oldest data points in the diatom data correspond to the end of the Pleistocene epoch (ice age), at which time there was a pronounced warming (Late Glacial Interstadial, 14.7 - 12.9 KyrBP) followed by a return to glacial conditions (Younger Dryas, 12.9 - 11.7 KyrBP).\nThis fluctuation can be seen from temperature reconstructions. Below is a plot of sea surface temperature reconstructions off the coast of Northern California. Data come from the following source:\n\nBarron et al., 2003. Northern Coastal California High Resolution Holocene/Late Pleistocene Oceanographic Data. IGBP PAGES/World Data Center for Paleoclimatology. Data Contribution Series # 2003-014. NOAA/NGDC Paleoclimatology Program, Boulder CO, USA.\n\nThe shaded region indicates the time window with unusually large flucutations in sea surface temperature; this window roughly corresponds to the dates of the climate event.\n\n# import sea surface temp reconstruction\nseatemps = pd.read_csv('data/barron-sst.csv')\n\n# line plot of time series\nline = alt.Chart(seatemps).mark_line().encode(\n    x = alt.X('Age', title = 'Thousands of years before present'),\n    y = 'SST'\n)\n\n# highlight region with large variations\nhighlight = alt.Chart(\n    pd.DataFrame(\n        {'SST': np.linspace(0, 14, 100), \n         'upr': np.repeat(11, 100), \n         'lwr': np.repeat(15, 100)}\n    )\n).mark_area(opacity = 0.2, color = 'orange').encode(\n    y = 'SST',\n    x = alt.X('upr', title = 'Thousands of years before present'),\n    x2 = 'lwr'\n)\n\n# add smooth trend\nsmooth = line.transform_loess(\n    on = 'Age',\n    loess = 'SST',\n    bandwidth = 0.2\n).mark_line(color = 'black')\n\n# layer\nline + highlight + smooth\n\n\n\n\n\n\n\n\n\nQuestion 8: Conditional distributions of relative abundance\nDoes the distribution of relative abundance of Azpeitia nodulifer differ when variation in sea temperatures was higher (before 11KyrBP)?\n(i) Plot kernel density estimates to show the distribution of relative abundances before and after 11KyrBP.\nUse the Altair implementation of Gaussian KDE: 1. Use .transform_caluculate(...) to calculate an indicator variable, pleistocene, that indicates whether Age exceeds 11. 2. Use .transform_density(...) to compute KDEs separately for observations of relative abundance before and after 11KyrBP. + Hint: group by pleistocene 3. Plot the KDEs distinguished by color; give the color legend the title ‘Before 11KyrBP’ and store the plot as kdes. 4. Add a shaded area beneath the KDE curves. Adjust the opacity of the area to your liking.\nStore the result as fig4 and display the figure.\n(ii) Does the distribution seem to change between epochs? If so, how?\nAnswer based on the figure in a few sentences.\nType your answer here, replacing this text.\n\n# BEGIN SOLUTION\nkdes = alt.Chart(diatoms).transform_calculate(\n    pleistocene = 'datum.Age &gt; 11'\n).transform_density(\n    density = 'A_nodul',\n    groupby = ['pleistocene'],\n    as_ = ['Relative abundance', 'Density'],\n    bandwidth = 0.025,\n    extent = [0, 0.4],\n    steps = 500\n).mark_line(color = 'black').encode(\n    x = 'Relative abundance:Q',\n    y = 'Density:Q',\n    color = alt.Color('pleistocene:N', title = 'Pleistocene')\n)\n\nkdes + kdes.mark_area(opacity = 0.2)\n# END SOLUTION\n\n\n\n\n\n\nSOLUTION Relative abundances of A. nodulifer are low during the end of the Pleistocene epoch and do not vary much; the taxon is relatively rare. However, following the climate shift, relative abundances are distributed between around 5% and 35%, indicating that the taxon is much more common. Typically abundance is around either 15% or 25% during the latter period.\n\n\n\n\nVisualizing community composition with PCA\nSo far you’ve seen that the abundances of one taxon – Azpeitia nodulifer – change markedly before and after a shift in climate conditions. In this part you’ll use PCA to compare variation in community composition among all eight taxa during the late Pleistocene and Holocene epochs.\n\nQuestion 9: Pairwise correlations in relative abundances\n(i) Compute the pairwise correlations between relative abundances and make a heatmap of the correlation matrix.\nBe sure to remove or set to indices the Depth and Age variables before computing the correlation matrix. Save the matrix as corr_mx.\n\nMelt corr_mx to obtain a dataframe with three columns:\n\nrow, which contains the values of the index of corr_mx (taxon names);\ncolumn, which contains the names of the columns of corr_mx (also taxon names); and\nCorrelation, which contains the values of corr_mx.\nStore the result as corr_mx_long.\n\nCreate an Altair chart based on corr_mx_long and construct the heatmap by following the examples indicated above.\n\nAdjust the color scheme to blueorange over the extent (-1, 1) to obtain a diverging color gradient where a correlation of zero is blank (white).\nAdjust the color legend to indicate the color values corresponding to correlations of 1, 0.5, 0, -0.5, and -1.\nSort the rows and columns in ascending order of correlation.\n\n\n(ii) How does A. nodulifer seem to vary with the other taxa, if at all?\nAnswer in a few sentences based on the heatmap.\nType your answer here, replacing this text.\n\ncorr_mx = diatoms.set_index(['Depth', 'Age']).corr() # SOLUTION\n\n# melt corr_mx\n# BEGIN SOLUTION\ncorr_mx_long = corr_mx.reset_index().rename(\n    columns = {'index': 'row'}\n).melt(\n    id_vars = 'row',\n    var_name = 'col',\n    value_name = 'Correlation'\n)\n# END SOLUTION\n\n# construct heatmap\n# BEGIN SOLUTION\nfig5 = alt.Chart(corr_mx_long).mark_rect().encode(\n    x = alt.X('col', title = '', sort = {'field': 'Correlation', 'order': 'ascending'}),\n    y = alt.Y('row', title = '', sort = {'field': 'Correlation', 'order': 'ascending'}),\n    color = alt.Color('Correlation', \n                      scale = alt.Scale(scheme = 'blueorange',\n                                        domain = (-1, 1), \n                                        type = 'sqrt'),\n                     legend = alt.Legend(tickCount = 5))\n).properties(width = 300, height = 300)\n# END SOLUTION\n\n# display\nfig5\n\n\n\n\n\n\nSOLUTION\nAzpeitia nodulifer abundances are negatively correlated with abundances of all other taxa – all entries in the A_nodul row are blue. This indicates that when A. nodulifer diatoms are abundant, diatoms in other taxa tend to be less abundant; conversely, when A. nodulifer are not abundant, other taxa tend to be more abundant.\n\ngrader.check(\"q9\")\n\n\n\nQuestion 10: Computing and selecting principal components\nHere you’ll perform all of the calculations involved in PCA and check the variance ratios to select an appropriate number of principal components. The parts of this question correspond to the individual steps in this process.\n(i) Center and scale the data columns.\nFor PCA it is usually recommended to center and scale the data; set Depth and Age as indices and center and scale the relative abundances. Store the normalized result as pcdata.\n(ii) Compute the principal components.\nCompute all 8 principal components. For this part you do not need to show any specific output.\n(iii) Examine the variance ratios.\nCreate a dataframe called pcvars with the variance information by following these steps:\n\nStore the proportion of variance explained (called .explained_variance_ratio_ in the PCA output) as a dataframe named pcvars with just one column named Proportion of variance explained.\nAdd a column named Component to pcvars with the integers 1 through 8 as values (indicating the component number).\nAdd a column named Cumulative variance explained to pcvars that is the cumulative sum of Proportion of variance explained.\n\nHint: slice the Proportion of variance explained column and use .cumsum(axis = ...).\n\n\nFor this part you do not need to show any specific output.\n(iv) Plot the variance explained by each PC.\nUse pcvars to construct a dual-axis plot showing the proportion of variance explained (left y axis) and cumulative variance explained (right y axis) as a function of component number (x axis), with points indicating the variance ratios and lines connecting the points. Follow these steps:\n\nConstruct a base chart that encodes only Component on the X channel. Store this as base.\nMake a base layer for the proportion of variance explained that modifies base by encoding Proportion of variance explained on the Y channel. Store the result as prop_var_base.\n\nGive the Y axis title a distinct color of your choosing via alt.Y(..., axis = alt.Axis(titleColor = ...)).\n\nMake a base layer for the cumulative variance explained that modifies base by endocing Cumulative variance explained on the Y channel. Store the result as cum_var_base.\n\nGive the Y axis title another distinct color of your choosing via alt.Y(..., axis = alt.Axis(titleColor = ...)).\n\nCreate a plot layer for the proportion of variance explained by combining points (prop_var_base.mark_point()) with lines (prop_var_base.mark_line()). Store the result as cum_var.\n\nApply the color you chose for the axis title to the points and lines.\n\nRepeat the previous step for the cumulative variance explained.\n\nApply the color you chose for the axis title to the points and lines.\n\nLayer the plots together using alt.layer(l1, l2).resolve_scale(y = 'independent').\n\nStore the result as fig6 and display the figure.\n(v) How many PCs should be used?\nPropose an answer based on the variance explained plots and indicate how much total variation your proposed number of components capture jointly.\nType your answer here, replacing this text.\n\n## (i) center and scale data\n\n# helper variable pcdata_raw; set Depth and Age as indices\npcdata_raw = diatoms.set_index(['Depth', 'Age']) # SOLUTION\n\n# center and scale the relative abundances\npcdata = (pcdata_raw - pcdata_raw.mean())/pcdata_raw.std() # SOLUTION\n\n\n## (ii) compute pcs\n\npca = PCA(8) # SOLUTION\npca.fit(pcdata) # SOLUTION\n\nPCA(n_components=8)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCAPCA(n_components=8)\n\n\n\n## (iii) retrieve variance info\n\n# store proportion of variance explained as a dataframe\npcvars = pd.DataFrame({'Proportion of variance explained': pca.explained_variance_ratio_}) # SOLUTION\n\n# add component number as a new column\npcvars['Component'] = np.arange(1, 9) # SOLUTION\n\n# add cumulative variance explained as a new column\npcvars['Cumulative variance explained'] = pcvars.iloc[:, 0].cumsum(axis = 0) # SOLUTION\n\n\n## (iv) plot variance explained\n\n# encode component axis only as base layer\nbase = alt.Chart(pcvars).encode(x = 'Component') # SOLUTION\n\n# make a base layer for the proportion of variance explained\n# BEGIN SOLUTION\nprop_var_base = base.encode(\n    y = alt.Y('Proportion of variance explained',\n              axis = alt.Axis(titleColor = '#57A44C'))\n)\n# END SOLUTION\n\n# make a base layer for the cumulative variance explained\n# BEGIN SOLUTION\ncum_var_base = base.encode(\n    y = alt.Y('Cumulative variance explained', axis = alt.Axis(titleColor = '#5276A7'))\n)\n# END SOLUTION\n\n# add points and lines to each base layer\n# BEGIN SOLUTION\nprop_var = prop_var_base.mark_line(stroke = '#57A44C') + prop_var_base.mark_point(color = '#57A44C')\ncum_var = cum_var_base.mark_line() + cum_var_base.mark_point()\n# END SOLUTION\n\n# layer the layers\nfig6 = alt.layer(prop_var, cum_var).resolve_scale(y = 'independent') # SOLUTION\n\n# display\nfig6\n\n\n\n\n\n\nSOLUTION Two is a reasonable choice. The first two PC’s capture over 20% of covariation each; after that, the remaining PC’s capture relatively much less (8-12% each). The first two PCs together explain roughly half of the total variation in relative abundances. (Three is also a reasonable choice; three components would capture over 70% of total variation in relative abundances.)\n\ngrader.check(\"q10\")\n\nNow that you’ve performed the calculations for PCA, you can move on to the fun/difficult part: figuring out what they say about the data.\nThe first step in this process is to examine the loadings. Each principal component is a linear combination of the relative abundances by taxon, and the loadings tell you how that combination is formed; the loadings are the linear combination coefficients, and thus correspond to the weight of each taxon in the corresponding principal component. Some useful points to keep in mind:\n\na high loading value (negative or positive) indicates that a variable strongly influences the principal component;\na negative loading value indicates that\n\nincreases in the value of a variable decrease the value of the principal component\nand decreases in the value of a variable increase the value of the principal component;\n\na positive loading value indicates that\n\nincreases in the value of a variable increase the value of the principal component\nand decreases in the value of a variable decrease the value of the principal component;\n\nsimilar loadings between two or more variables indicate that the principal component reflects their average;\ndivergent loadings between two sets of variables indicates that the principal component reflects their difference.\n\n\n\nQuestion 11: Interpreting component loadings\n(i) Extract the loadings from pca.\nStore the loadings for the first two principal components (called .components_ in the PCA output) in a dataframe named loading_df. Name the columns PC1 and PC2, and append a column Taxon with the corresponding variable names, and print the resulting dataframe.\n(ii) Construct loading plots\nConstruct a line-and-point plot connecting the loadings of the first two principal components. Display the value of the loading on the y axis and the taxa names on the x axis, and show points indicating the loading values. Distinguish the PC’s by color, and add lines connecting the loading values for each principal component. Store the result as fig7 and display the figure – you may need to resize for better readability.\nHint: you will need to first melt loading_df to long form with three columns – the taxon name, the principal component (1 or 2), and the value of the loading.\n(iii) Interpret the first principal component.\nIn a few sentences, answer the following questions. 1. Which taxa are up-weighted and which are down-weighted in this component? 2. How would you describe the principal component in context (e.g., average abundance among a group, differences in abundances, etc.)? 3. How would you interpret a larger value of the PC versus a smaller value of the PC in terms of diatom communnity composition?\n(iv) Interpret the second principal component.\nAnswer the same questions for component 2.\nType your answer here, replacing this text.\n\n## (i) retrieve loadings\n# store the loadings as a data frame with appropriate names\nloading_df = pd.DataFrame(pca.components_).transpose().rename(columns = {0: 'PC1', 1: 'PC2'}).loc[:, ['PC1', 'PC2']] # SOLUTION\n\n# add a column with the taxon names\nloading_df['Taxon'] = pcdata.columns.values # SOLUTION\n\n\n## (ii) construct loading plots\n# melt from wide to long\nloading_plot_df = loading_df.melt(\n    id_vars = 'Taxon', # SOLUTION\n    var_name = 'PC', # SOLUTION\n    value_name = 'Loading' # SOLUTION\n)\n\n# create base layer with encoding\nbase = alt.Chart(loading_plot_df).encode(\n    x = 'Taxon', # SOLUTION\n    y = 'Loading', # SOLUTION\n    color = 'PC' # SOLUTION\n)\n\n# store horizontal line at zero\nrule = alt.Chart(pd.DataFrame({'Loading': 0}, index = [0])).mark_rule().encode(y = 'Loading', size = alt.value(2)) # SOLUTION\n\n# layer points + lines + rule to construct loading plot\nfig7 = base.mark_point() + base.mark_line() + rule # SOLUTION\n\n# show\nfig7.properties(width = 400, height = 150) # SOLUTION\n\n\n\n\n\n\nSOLUTION\nFor PC1, the loading is large and positive for A. nodulifer, and all other loadings are negative in varying magnitude. So this component is heavily up-weighted by high abundance of A. nodulifer and downweighted by high abunances of other taxa. In effect, it reflects the difference in relative abundance between A. nodulifer and a weighted average of all other taxa; when the PC is positive, A. nodulifer are more abundant than other taxa, and when the PC is negative, A. nodulifer is less abundant than other taxa.\nFor PC2, the loading is large and positive for two taxa, Cyclotella and R. tesselata, and large and negative for two taxa, Coscinodiscus and Actinoptychus; all other loadings are negligibly small. So this component is heavily up-weighted by a high average abundance of the first two and down-weighted by a high average abundance of the second two. In effect, it reflects the difference in average relative abundance between two groups of taxa; when the PC is positive, the first group are more abundant than other taxa, and when the PC is negative, the second group is more abundant than other taxa.\n\ngrader.check(\"q11\")\n\nRecall that there was a shift in climate around 11,000 years ago, and A. nodulifer abundances seemed to differ before and after the shift.\nYou can now use PCA to investigate whether not just individual abundances but community composition may have shifted around that time. To that end, let’s think of the principal components as ‘community composition indices’:\n\nconsider PC1 a nodulifer/non-nodulifer community composition index; and\nconsider PC2 a complex community composition index.\n\nA pattern of variation or covariation in the principal components can be thought of as reflecting a particular ecological community composition dynamic – a way that community composition varies throughout time. Here you’ll look for distinct patterns of variation/covariation before and after 11,000 years ago via an exploratory plot of the principal components.\n\n\nQuestion 12: Visualizing community composition shift\n(i) Project the centered and scaled data onto the first two component directions.\nThis sounds a little more complicated than it is – all that means is compute the values of the principal components for each data point. Create a dataframe called projected_data containing just the first two principal components as two columns named PC1 and PC2, and two additional columns with the Age and Depth variables.\n(ii) Construct a scatterplot of PC1 and PC2 by epoch.\nConstruct a scatterplot of the principal components with observations colored according to whether they occurred in the Pleistocene or Holocene epoch. Store the result as fig8 and display the figure.\n(iii) Comment on the plot: does there appear to be any change in community structure?\nAnswer in a few sentences.\nType your answer here, replacing this text.\n\n## (i) project pcdata onto first two components; store as data frame\n\n# retrieve principal component scores for pc1 and pc2\nprojected_data = pd.DataFrame(pca.fit_transform(pcdata)).iloc[:, 0:2].rename(columns = {0: 'PC1', 1: 'PC2'}) # SOLUTION\n\n# adjust index\nprojected_data.index = pcdata.index # SOLUTION\nprojected_data = projected_data.reset_index() # SOLUTION\n\n\n## (ii) construct scatterplot\n# BEGIN SOLUTION\nfig8 = alt.Chart(projected_data).transform_calculate(\n    holocene = 'datum.Age &lt; 11'\n).mark_point().encode(\n    x = alt.X('PC1:Q', title = 'Nodulifer/non-nodulifer composition'),\n    y = alt.Y('PC2:Q', title = 'Complex community composition'),\n    color = alt.Color('holocene:N', title = 'Holocene')\n)\n# END SOLUTION\n\n# display\nfig8 # SOLUTION\n\n\n\n\n\n\nSOLUTION\nWhile there is some overlap, observations seem to largely fall into distinct clusters according to epoch. There does seem to be a shift, but the shift is multivariate: the clusters are not separable on one axis alone.\n\ngrader.check(\"q12\")\n\n\n\n\n(Optional) Question 13: Multi-panel visualization\nSometimes it’s helpful to see marginal distributions together with a scatterplot. Follow the steps below to create a multi-panel figure with marginal density estimates appended to the projected scatter from the previous question.\n\nCreate an Altair chart based on projected_data and use .transform_calculate(...) to define a variable holocene that indicates whether Age is older than 11,000 years. Store the result as base.\nModify base to add points with the following encodings.\n\nPass PC1 to the X encoding channel and title the axis ‘A. Nodulifer/non-A. nodulifer composition’.\nPass PC2 to the Y encoding channel and title the axis ‘Complex community composition’.\nPass the variable you created in step 1. to the color encoding channel and title it ‘Holocene’. Store the result as scatter.\n\nConstruct plots of kernel density estimates for each principal component conditional on age being older than 11,000 years:\n\nmodify base to create a top_panel plot with the KDE curves for PC1, with color corresponding to the age indicator from the .transform_calculate(...) step in making the base layer;\nmodify base again to create a side_panel plot with the KDE curves for PC2, rotated 90 degrees relative to the usual orientation (flip the typical axes), and with color corresponding to the age indicator from the .transform_calculate(...) step in making the base layer.\n\nThen, resize these panels appropriately (top should be thin, side should be narrow), and use Altair’s faceting operators & (vertical concatenation) and | (horizontal concatenation) to combine them with your scatterplot.\n\nStore the result as fig9 and display the figure.\n\n# make base layer\n# BEGIN SOLUTION\nbase = alt.Chart(projected_data).transform_calculate(\n    holocene = 'datum.Age &lt; 11'\n)\n# END SOLUTION\n\n# data scatter\n# BEGIN SOLUTION\nscatter = base.mark_point().encode(\n    x = alt.X('PC1:Q', title = 'Nodulifer/non-nodulifer composition'),\n    y = alt.Y('PC2:Q', title = 'Complex community composition'),\n    color = alt.Color('holocene:N', title = 'Holocene')\n)\n\n# construct upper panel (kdes for pc1)\n# BEGIN SOLUTION\npc1_kde = base.transform_density(\n    density = 'PC1',\n    groupby = ['holocene'],\n    as_ = ['PC1', 'Density'],\n    extent = [-3, 5],\n    steps = 500\n).encode(\n    x = alt.X('PC1:Q', axis = None),\n    y = alt.Y('Density:Q', axis = None),\n    color = alt.Color('holocene:N', title = 'Holocene')\n)\n\ntop_panel = (pc1_kde.mark_line() + pc1_kde.mark_area(opacity = 0.2)).properties(height = 50)\n# END SOLUTION\n\n# construct side panel (kdes for pc2)\n# BEGIN SOLUTION\npc2_kde = base.transform_density(\n    density = 'PC2',\n    groupby = ['holocene'],\n    as_ = ['PC2', 'Density'],\n    extent = [-3, 5],\n    steps = 500\n).encode(\n    y = alt.Y('PC2:Q', axis = None),\n    x = alt.X('Density:Q', axis = None),\n    color = alt.Color('holocene:N', title = 'Holocene')\n)\n\nside_panel = (pc2_kde.mark_line(order = False) + pc2_kde.mark_area(order = False, opacity = 0.2)).properties(width = 50)\n# END SOLUTION\n\n# facet\nfig9 = top_panel & (scatter | side_panel) # SOLUTION\n\n# display\nfig9\n\n\n\n\n\n\n\n\n\n\nCommunicating results\nTake a moment to review and reflect on the results of your analysis in the previous parts. Think about how you would describe succinctly what you’ve learned from the diatom data.\n\n\nQuestion 14: Summary\nWrite a brief paragraph (3-5 sentences) that addresses the following questions by referring to your results above.\n\nHow would you characterize the typical ecological community composition of diatom taxa before and after 11,000 years ago?\n\nHint: focus on the side and top panels and the typical values of each index in the two time periods.\n\nDoes the variation in ecological community composition over time seem to differ before and after 11,000 years ago?\n\nHint: focus on the shape of data scatter.\n\n\nType your answer here, replacing this text.\nSOLUTION\nBefore 11,000 years ago, A. nodulifer were more abundant relative to other taxa (PC1 is typically positive), and the same period is characterized by low levels of abundance of other taxa, but possibly some proportion of Coscinodiscus and Actinoptychus in the community (PC2 is typically slightly negative); most variation in the community composition is driven by variation in the relative abundance of A. nodulifer. By contrast, between 11,000 years ago and now, A.nodulifer are generally less abundant and most of the variation in community composition is driven by alternations in the relative abundances of Coscinodiscus and Actinoptychus with those of Cyclotella and R. tesselata. The markedly different relationships between the community composition indices given by the PCs suggests that the dynamics of community composition may have shifted around the time of the last major climate change event, and in particular that the present community is both more dynamic (varies more) and diverse (based on a greater number of taxa).\n\n\n\n\nQuestion 15: Further work\nWhat more might you like to know, given what you’ve learned? Pose a question that your exploratory analysis raises for you.\n\nAnswer\nType your answer here.\nSOLUTION Can be anything, but here are two of my questions:\n\nWhat distinguishes A. nodulifer biologically and ecologically from other taxa?\nWas this change in the ecological community of diatoms associated with corresponding ecological changes higher in the food web?\n\n\n\n\n\n\nSubmission\n\nSave the notebook.\nRestart the kernel and run all cells. (CAUTION: if your notebook is not saved, you will lose your work.)\nCarefully look through your notebook and verify that all computations execute correctly and all graphics are displayed clearly. You should see no errors; if there are any errors, make sure to correct them before you submit the notebook.\nDownload the notebook as an .ipynb file. This is your backup copy.\nExport the notebook as PDF and upload to Gradescope.\n\n\nTo double-check your work, the cell below will rerun all of the autograder tests.\n\ngrader.check_all()"
  },
  {
    "objectID": "hw/hw1-brfss/hw1-brfss.html",
    "href": "hw/hw1-brfss/hw1-brfss.html",
    "title": "Background",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"hw1-brfss.ipynb\")\n\n\nimport numpy as np\nimport pandas as pd\nimport altair as alt\n# disable row limit for plotting\nalt.data_transformers.disable_max_rows()\n# uncomment to ensure graphics display with pdf export\n# alt.renderers.enable('mimetype')\n\nThe Behavioral Risk Factor Surveillance System (BRFSS) is a long-term effort administered by the CDC to collect data on behaviors affecting physical and mental health, past and present health conditions, and access to healthcare among U.S. residents. The BRFSS comprises telephone surveys of U.S. residents conducted annually since 1984; in the last decade, over half a million interviews have been conducted each year. This is the largest such data collection effort in the world, and many countries have developed similar programs. The objective of the program is to support monitoring and analysis of factors influencing public health in the United States.\nEach year, a standard survey questionnaire is developed that includes a core component comprising questions about: demographic and household information; health-related perceptions, conditions, and behaviors; substance use; and diet. Trained interviewers in each state call randomly selected telephone (landline and cell) numbers and administer the questionnaire; the phone numbers are chosen so as to obtain a representative sample of all households with telephone numbers. Take a moment to read about the 2019 survey here.\nIn this assignment you’ll import and subsample the BRFSS 2019 data and perform a simple descriptive analysis exploring associations between adverse childhood experiences, health perceptions, tobacco use, and depressive disorders. This is an opportunity to practice:\n\nreview of data documentation\ndata assessment and critical thinking about data collection\ndataframe transformations in pandas\ncommunicating and interpreting grouped summaries\n\n\nData import and assessment\nThe cell below imports select columns from the 2019 dataset as a pandas DataFrame. The file is big, so this may take a few moments. Run the cell and then have a quick look at the first few rows and columns.\n\n# store variable names of interest\nselected_vars = ['_SEX', '_AGEG5YR', \n                 'GENHLTH', 'ACEPRISN', \n                 'ACEDRUGS', 'ACEDRINK', \n                 'ACEDEPRS', 'ADDEPEV3', \n                 '_SMOKER3', '_LLCPWT']\n\n# import full 2019 BRFSS dataset\nbrfss = pd.read_csv('data/brfss2019.zip', compression = 'zip', usecols = selected_vars)\n\n# invert sampling weights\nbrfss['_LLCPWT'] = 1/brfss._LLCPWT\n\n# print first few rows\nbrfss.head()\n\n\nQuestion 1: Data dimensions\nCheck the dimensions of the dataset. Store the dimensions as nrows and ncolumns.\n\nnrows, ncolumns = ...\n\nprint(nrows, ncolumns)\n\n\ngrader.check(\"q1\")\n\n\n\n\nQuestion 2: Row and column information\nNow that you’ve imported the data, you should verify that the dimensions conform to the format you expect based on data documentation and ensure you understand what each row and each column represents.\nCheck the number of records (interviews conducted) reported and variables measured for 2019 by reviewing the surveillance summaries by year, and then answer the following questions in a few sentences:\n\nDoes the number of rows match the number of reported records?\nHow many columns were imported, and how many columns are reported in the full dataset?\nWhat does each row in the brfss dataframe represent?\nWhat does each column in the brfss dataframe represent\n\nType your answer here, replacing this text.\n\n\n\n\nQuestion 3: Sampling design and data collection\nSkim the overview documentation for the 2019 BRFSS data. Focus specifically the ‘Background’ and ‘Data Collection’ sections, read selectively for relevant details, and answer the following questions in a few sentences:\n\nWho conducts the interviews and how long does a typical interview last?\nWho does an interviewer speak to in each household?\nWhat criteria must a person meet to be interviewed?\nWho can’t appear in the survey? Give two examples.\nWhat is the study population (i.e., all individuals who could possibly be sampled)?\nDoes the data contain any identifying information?\n\nType your answer here, replacing this text.\n\n\n\n\nQuestion 4: Variable descriptions\nYou’ll work with the small subset variables imported above: sex, age, general health self-assessment, smoking status, depressive disorder, and adverse childhood experiences (ACEs). The names of these variables as they appear in the raw dataset are defined in the cell in which you imported the data as selected_vars. It is often useful, and therefore good practice, to include a brief description of each variable at the outset of any reported analyses, both for your own clarity and for that of any potential readers. Open the 2019 BRFSS codebook in your browser and use text searching to locate each of the variable names of interest. Read the codebook entries and fill in the second column in the table below with a one-sentence description of each variable identified in selected_vars. Rephrase the descriptions in your own words – do not copy the codebook descriptions verbatim.\n\n\n\nVariable name\nDescription\n\n\n\n\nGENHLTH\n\n\n\n_SEX\n\n\n\n_AGEG5YR\n\n\n\nACEPRISN\n\n\n\nACEDRUGS\n\n\n\nACEDRINK\n\n\n\nACEDEPRS\n\n\n\nADDEPEV3\n\n\n\n_SMOKER3\n\n\n\n\n\n\n\n\nSubsampling\nTo simplify life a little, we’ll draw a large random sample of the rows and work with that in place of the full dataset. This is known as subsampling.\nThe cell below draws a random subsample of 10k records. Because the subsample is randomly drawn, we should not expect it to vary in any systematic way from the overall dataset, and distinct subsamples should have similar properties – therefore, results downstream should be similar to an analysis of the full dataset, and should also be possible to replicate using distinct subsamples.\n\n# for reproducibility\nnp.random.seed(32221) \n\n# randomly sample 10k records\nsamp = brfss.sample(n = 10000, \n                    replace = False, \n                    weights = '_LLCPWT')\n\nAsides:\n\nNotice that the random number generator seed is set before carrying out this task – this ensures that every time the cell is run, the same subsample is drawn. As a result, the computations in this notebook are reproducible: when I run the notebook on my computer, I get the same results as you get when you run the notebook on your computer.\nNotice also that sampling weights provided with the dataset are used to draw a weighted sample. Some respondents are more likely to be selected than others from the general population of U.S. adults with phone numbers, so the BRFSS calculates derived weights that are inversely proportional to estimates of the probability that the respondent is included in the survey. This is a somewhat sophisticated calculation, however if you’re interested, you can read about how these weights are calculated and why in the overview documentation you used to answer the questions above. We use the sampling weights in drawing the subsample so that we get a representative sample of U.S. adults with phone numbers.\nNotice the missing values. How many entries are missing in each column? The cell below computes the proportion of missing values for each of the selected variables. We’ll return to this issue later on.\n\n\n# proportions of missingness \nsamp.isna().mean()\n\n\n\nTidying\nIn the following series of questions you’ll tidy up the subsample by performing these steps:\n\nselecting columns of interest;\nreplacing coded values of question responses with responses;\ndefining new variables based on existing ones;\nrenaming columns.\n\nThe goal of this is to produce a clean version of the dataset that is well-organized, intuitive to navigate, and ready for analysis.\nThe variable entries are coded numerically to represent certain responses. These should be replaced by more informative entries. We can use the codebook to determine which number means what, and replace the values accordingly.\nThe cell below replaces the numeric values for _AGEG5YR by their meanings, illustrating how to use .replace() with a dictionary to convert the numeric coding to interpretable values. The basic strategy is:\n\nStore the variable coding for VAR as a dictionary var_codes.\nUse .replace({'VAR': var_codes}) to modify values.\n\nIf you need additional examples, check the pandas documentation for .replace().\n\n# dictionary representing variable coding\nage_codes = {\n    1: '18-24', 2: '25-29', 3: '30-34',\n    4: '35-39', 5: '40-44', 6: '45-49',\n    7: '50-54', 8: '55-59', 9: '60-64',\n    10: '65-69', 11: '70-74', 12: '75-79',\n    13: '80+', 14: 'Unsure/refused/missing'\n}\n\n# recode age categories\nsamp_mod1 = samp.replace({'_AGEG5YR': age_codes})\n\n# check result\nsamp_mod1.head()\n\n\nQuestion 5: Recoding variables\nFollowing the example immediately above and referring to the 2019 BRFSS codebook, replace the numeric codings with response categories for each of the following variables:\n\n_SEX\nGENHLTH\n_SMOKER3\n\nNotice that above, the first modification (slicing) was stored as samp_mod1, and was a function of samp. You’ll follow this pattern, creating samp_mod2, samp_mod3, and so on so that each step (modification) of your data manipulations is stored separately, for easy troubleshooting.\n\nRecode _SEX: define a new dataframe samp_mod2 that is the same as samp_mod1 but with the _SEX variable recoded as M and F.\nRecode GENHLTH: define a new dataframe samp_mod3 that is the same as samp_mod2 but with the GENHLTH variable recoded as Excellent, Very good, Good, Fair, Poor, Unsure, and Refused.\nRecode _SMOKER3: define a new dataframe samp_mod4 that is the same as samp_mod3 but with _SMOKER3 recoded as Daily, Some days, Former, Never, and Unsure/refused/missing.\nPrint the first few rows of samp_mod4.\n\n\n# define dictionary for sex\nsex_codes = ...\n\n# recode sex\nsamp_mod2 = ...\n\n# define dictionary for health\nhealth_codes = ...\n\n# recode health\nsamp_mod3 = ...\n\n# define dictionary for smoking\nsmoke_codes = ...\n\n# recode smoking\nsamp_mod4 = ...\n\n# print a few rows\n...\n\n\ngrader.check(\"q5\")\n\n\n\nQuestion 6: Value replacement\nNow all the variables except the adverse childhood experience and depressive disorder question responses are represented interpretably. In the codebook that the answer key is identical for these remaining variables.\nThe numeric codings can be replaced all at once by applying .replace() to the dataframe with an argument of the form\n\ndf.replace({'var1': varcodes1, 'var2': varcodes1, ..., 'varp': varcodesp})\n\nDefine a new dataframe samp_mod5 that is the same as samp_mod4 but with the remaining variables recoded according to the answer key Yes, No, Unsure, Refused. Print the first few rows of the result using .head().\n\n# define dictionary\nanswer_codes = ...\n\n# recode\nsamp_mod5 = ...\n\n# check using head()\n...\n\n\ngrader.check(\"q6\")\n\nFinally, all the variables in the dataset are categorical. Notice that the current data types do not reflect this.\n\nsamp_mod5.dtypes\n\nLet’s coerce the variables to category data types using .astype().\n\n# coerce to categorical\nsamp_mod6 = samp_mod5.astype('category')\n\n# check new data types\nsamp_mod6.dtypes\n\n\n\nQuestion 7: Define ACE indicator variable\nDownstream analysis of ACEs will be facilitated by having an indicator variable that is a 1 if the respondent answered ‘Yes’ to any ACE question, and a 0 otherwise – that way, you can easily count the number of respondents reporting ACEs by summing up the indicator or compute the proportion by taking an average.\nTo this end, define a new logical variable:\n\nadverse_conditions: did the respondent answer yes to any of the adverse childhood condition questions?\n\nYou can accomplish this task in several steps:\n\nObtain a logical array indicating the positions of the ACE variables (hint: use .columns to obtain the column index and operate on the result with .str.startswith(...).). Store this as ace_positions.\nUse the logical array ace_positions to select the ACE columns via .loc[]. Store this as ace_data.\nObtain a dataframe that indicates whether each entry is a ‘Yes’ (hint: use the boolean operator ==, which is a vectorized operation). Store this as ace_yes.\nCompute the row sums using .sum(). Store this as ace_numyes.\nDefine the new variable as ace_numyes &gt; 0.\n\nStore the result as samp_mod7, and print the first few rows using .head().\n\n\n# copy samp_mod6\nsamp_mod7 = samp_mod6.copy()\n\n# ace column positions\nace_positions = ...\n\n# ace data\nace_data = ...\n\n# ace yes indicators\nace_yes = ...\n\n# number of yesses\nace_numyes = ...\n\n# assign new variable\nsamp_mod7['adverse_conditions'] = ...\n\n# check result using .head()\n...\n\n\ngrader.check(\"q7\")\n\n\n\nQuestion 8: Define missingness indicator variable\nAs you saw earlier, there are some missing values for the ACE questions. These arise whenever a respondent is not asked these questions. In fact, answers are missing for nearly 80% of the respondents in our subsample. We should keep track of this information. Define a missing indicator:\n\nadverse_missing: is a response missing for at least one of the ACE questions?\n\n\n# copy modification 7\nsamp_mod8 = samp_mod7.copy()\n\n# define missing indicator using loc\n...\n\n# check using head()\n\n\ngrader.check(\"q8\")\n\n\n\nQuestion 9: Filter respondents who did not answer ACE questions\nSince values are missing for the ACE question if a respondent was not asked, we can remove these observations and do any analysis conditional on respondents having been asked the ACE questions. Use your indicator variable adverse_missing to filter out respondents who were not asked the ACE questions.\nNote that this dramatically limits the scope of inference for subsequent analyses to only those locations where the ACE module was included in the survey.\n\nsamp_mod9 = ...\n\n\ngrader.check(\"q9\")\n\n\n\nQuestion 10: Define depression indicator variable\nIt will prove similarly helpful to define an indicator for reported depression:\n\ndepression: did the respondent report having been diagnosed with a depressive disorder?\n\nFollow the same strategy as above for the ACE variables, and store the result as samp_mod10. See if you can perform the calculation of the new variable in a single line of code. Print the first few rows using .head().\n\n# copy samp_mod9\nsamp_mod10 = samp_mod9.copy()\n\n# define new variable using loc\n...\n\n# check using .head()\n...\n\n\ngrader.check(\"q10\")\n\n\n\nQuestion 11: Final dataset\nFor the final dataset, drop the respondent answers to individual questions, the missingness indicator, and select just the derived indicator variables along with general health, sex, age, and smoking status. Check the pandas documentation for .rename() and follow the examples to rename the latter variables:\n\ngeneral_health\nsex\nage\nsmoking\n\nSee if you can perform both operations (slicing and renaming) in a single chain. Store the result as data.\n\nsamp_mod10.columns\n\n\n# slice and rename\ndata = ...\n\n# check using .head()\n\n\ngrader.check(\"q11\")\n\n\n\n\nDescriptive analysis\nNow that you have a clean dataset, you’ll use grouping and aggregation to compute several summary statistics that will help you explore whether there is an apparent association between experiencing adverse childhood conditions and self-reported health, smoking status, and depressive disorders in areas where the ACE module was administered.\nThe basic strategy will be to calculate the proportions of respondents who answered yes to one of the adverse experience questions when respondents are grouped by the other variables.\n\nQuestion 12: Proportion of respondents reporting ACEs\nCalculate the overall proportion of respondents in the subsample that reported experiencing at least one adverse condition (given that they answered the ACE questions). Use .mean(); store the result as mean_ace and print.\n\n# proportion of respondents reporting at least one adverse condition\nmean_ace = ...\n\n# print\nmean_ace\n\n\ngrader.check(\"q12\")\n\nDoes the proportion of respondents who reported experiencing adverse childhood conditions vary by general health?\nThe cell below computes the porportion separately by general health self-rating. Notice that the depression variable is dropped so that the result doesn’t also report the proportion of respondents reporting having been diagnosed with a depressive disorder. Notice also that the proportion of missing values for respondents indicating each general health rating is shown.\n\n# proportions grouped by general health\ndata.drop(\n    columns = 'depression'\n).groupby(\n    'general_health'\n).mean(numeric_only = True)\n\nNotice that the row index lists the general health rating out of order. This can be fixed using a .loc[] call and the dictionary that was defined for the variable coding.\n\n# same as above, rearranging index\nace_health = data.drop(\n    columns = 'depression'\n).groupby(\n    'general_health'\n).mean(\n    numeric_only = True\n).loc[list(health_codes.values()), :]\n\n# print\nace_health\n\n\n\nQuestion 13: Association between smoking status and ACEs\nDoes the proportion of respondents who reported experiencing adverse childhood conditions vary by smoking status?\nFollowing the example above for computing the proportion of respondents reporting ACEs by general health rating, calculate the proportion of respondents reporting ACEs by smoking status (be sure to arrange the rows in appropriate order of smoking status) and store as ace_smoking.\n\n# proportions grouped by smoking status\nace_smoking = data.drop(\n    columns = 'depression'\n).groupby(\n    'smoking'\n).mean(\n    numeric_only = True\n...\n\n# print\nace_smoking\n\n\ngrader.check(\"q13\")\n\n\n\nQuestion 14: Association between depression and ACEs\nDoes the proportion of respondents who reported experiencing adverse childhood conditions vary by smoking status?\nCalculate the proportion of respondents reporting ACEs by whether respondents had been diagnosed with a depressive disorder and store as ace_depr.\n\n# proportions grouped by having experienced depression\nace_depr = data.groupby(\n    'depression'\n).mean(\n    numeric_only = True\n...\n\n# print\nace_depr\n\n\ngrader.check(\"q14\")\n\n\n\nQuestion 15: Exploring subgroupings\nDoes the apparent association between general health and ACEs persist after accounting for sex?\nRepeat the calculation of the proportion of respondents reporting ACEs by general health rating, but also group by sex. Store the result as ace_health_sex.\n\n# group by general health and sex\nace_health_sex = data.drop(\n    columns = 'depression'\n).groupby(\n    ['general_health', 'sex']\n...\n\n\ngrader.check(\"q15\")\n\nThe cell below rearranges the table a little for better readability.\n\n# pivot table for better display\nace_health_sex.reset_index().pivot(columns = 'sex', index = 'general_health', values = 'adverse_conditions').loc[list(health_codes.values()), :]\n\nEven after rearrangement, the table in the last question is a little tricky to read (few people like visually scanning tables). This information would be better displayed in a plot. The example below generates a bar chart showing the summaries you calculated in Q2(d), with the proportion on the y axis, the health rating on the x axis, and separate bars for the two sexes.\n\n# coerce indices to columns for plotting\nplot_df = ace_health_sex.reset_index()\n\n# specify order of general health categories\ngenhealth_order = list(health_codes.values())\nplot_df.general_health.cat.set_categories(genhealth_order, inplace=True)\nplot_df.sort_values([\"general_health\"], inplace=True)\n\n# plot\nalt.Chart(plot_df).mark_bar().encode(\n    x = alt.X('general_health', \n              sort = ['general_health'],\n              title = 'Self-rated general health'),\n    y = alt.Y('adverse_conditions',\n              title = 'Prop. of respondents reporting ACEs'),\n    color = 'sex',\n    column = 'sex'\n).properties(\n    width = 200, \n    height = 200\n)\n\n\n\n\nQuestion 16: Visualization\nUse the example above to plot the proportion of respondents reporting ACEs against smoking status for men and women.\nHint: you only need to modify the example by substituting smoking status for general health.\n\n# dataframe of proportions grouped by smoking status\nace_smoking_sex = ...\n\n# coerce indices to columns for plotting\n...\n\n# specify order of general health categories\n...\n\n# plot\n...\n\n\n\n\n\nCommunicating results\nHere you’ll be asked to reflect briefly on your findings.\n\n\nQuestion 17: Summary\nIs there an observed association between reporting ACEs and general health, smoking status, and depression among survey respondents who answered the ACE questions?\nWrite a two to three sentence answer to the above question summarizing your findings. State an answer to the question in your first sentence, and then in your second/third sentences describe exactly what you observed in the foregoing descriptive analysis of the BRFSS data. Be precise, but also concise. There is no need to describe any of the data manipulations, survey design, or the like.\nType your answer here, replacing this text.\n\n\n\n\nQuestion 18: Scope of inference\nRecall from the overview documentation all the care that the BRFSS dedicates to collecting a representative sample of the U.S. adult population with phone numbers. Do you think that your findings provide evidence of an association among the general public (not just the individuals survey)? Why or why not? Answer in two sentences.\nType your answer here, replacing this text.\n\n\n\n\nQuestion 19: Bias\nWhat is a potential source of bias in the survey results, and how might this affect the proportions you’ve calculated?\nAnswer in one or two sentences.\nType your answer here, replacing this text.\n\n\n\nComment\nNotice that the language ‘association’ is non-causual: we don’t say that ACEs cause (or don’t cause) poorer health outcomes. This is intentional, because the BRFSS data are what are known as ‘observational’ data, i.e. not originating from a controlled experiment. There could be unobserved factors that explain the association.\nTo take a simple example, dog owners live longer, but the reason is simply that dog owners walk more – so it’s the exercise, not the dogs, that cause an increase in longevity. An observational study that doesn’t measure exercise would show a positive association between dog ownership and lifespan, but it’s a non-causal relationship.\n(As an interesting/amusing aside, there is a well known study that established an association between birdkeeping and lung cancer; obviously this is non-causal, yet the study authors recommended that individuals at high risk for cancer avoid ‘avian exposure’, as they were unsure of the mechanism.)\nSo there could easily be unobserved factors that account for the observed association in the BRFSS data. We guard against over-interpreting the results by using causally-neutral language.\n\n\n\nSubmission\n\nSave the notebook.\nRestart the kernel and run all cells. (CAUTION: if your notebook is not saved, you will lose your work.)\nCarefully look through your notebook and verify that all computations execute correctly and all graphics are displayed clearly. You should see no errors; if there are any errors, make sure to correct them before you submit the notebook.\nDownload the notebook as an .ipynb file. This is your backup copy.\nExport the notebook as PDF and upload to Gradescope.\n\n\nTo double-check your work, the cell below will rerun all of the autograder tests.\n\ngrader.check_all()"
  },
  {
    "objectID": "miscellany.html",
    "href": "miscellany.html",
    "title": "Miscellany",
    "section": "",
    "text": "Automated tests in assignment notebooks are a guide, not a confirmation or refutation of your answer. Don’t rely too heavily on them, but do read the output message if they fail and think about what the message is telling you. On some occasions they will fail despite a correct answer; on others they will pass despite an incorrect answer. Furthermore, they will guide you to one particular strategy for obtaining the solution; most problems admit a few possible strategies.\nAll assignments are due on Mondays. You get two free late assignments. Late submissions are due Wednesdays.\nStart your homeworks and mini-projects early.\nTake your own notes during class; don’t simply rely on lecture slides."
  },
  {
    "objectID": "miscellany.html#reminders-and-suggestions",
    "href": "miscellany.html#reminders-and-suggestions",
    "title": "Miscellany",
    "section": "",
    "text": "Automated tests in assignment notebooks are a guide, not a confirmation or refutation of your answer. Don’t rely too heavily on them, but do read the output message if they fail and think about what the message is telling you. On some occasions they will fail despite a correct answer; on others they will pass despite an incorrect answer. Furthermore, they will guide you to one particular strategy for obtaining the solution; most problems admit a few possible strategies.\nAll assignments are due on Mondays. You get two free late assignments. Late submissions are due Wednesdays.\nStart your homeworks and mini-projects early.\nTake your own notes during class; don’t simply rely on lecture slides."
  },
  {
    "objectID": "miscellany.html#troubleshooting",
    "href": "miscellany.html#troubleshooting",
    "title": "Miscellany",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nIf you need to recover the distribution copy of any assignment notebook, perhaps due to accidentally deleting cells or similar issues, simply rename the notebook containing your work on the LSIT server and then redeploy the notebook from the course website link.\nIf you try to open a notebook and the server fails at the ‘synchronizing git repository’ stage, open the LSIT server separately, rename the pstat100-content directory, and then try opening the notebook from the website link again. If successfull, you will need to migrate all of your previous work into the new pstat100-content directory."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html",
    "href": "labs/lab2-sampling/lab2-sampling.html",
    "title": "Lab 2: Sampling designs and statistical bias",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab2-sampling.ipynb\")\nimport numpy as np\nimport pandas as pd\nimport altair as alt\n# disable row limit for plotting\nalt.data_transformers.disable_max_rows()\n# uncomment to ensure graphics display with pdf export\n# alt.renderers.enable('mimetype')\nIn this lab you’ll explore through simulation how nonrandom sampling can produce datasets with statistical properties that are distored relative to the population that the sample was drawn from. This kind of distortion is known as bias.\nIn common usage, the word ‘bias’ means disproportion or unfairness. In statistics, the concept has the same connotation – biased sampling favors certain observational units over others, and biased estimates are estimates that favor larger or smaller values than the truth. The goal of this lab is to refine your understanding about what statistical bias is and is not and develop your intuition about potential mechanisms by which bias is introduced and the effect that this can have on sample statistics.\nObjectives:"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html#sampling-designs",
    "href": "labs/lab2-sampling/lab2-sampling.html#sampling-designs",
    "title": "Lab 2: Sampling designs and statistical bias",
    "section": "Sampling designs",
    "text": "Sampling designs\nThe sampling design of a study refers to the way observational units are selected from the collection of all observational units. Any design can be expressed by the probability that each unit is included in the sample. In a random sample, all units are equally likely to be included.\nFor example, you might want to learn about U.S. residents (population), but only be able for ethical or practical reasons to study adults (sampling frame), and decide to do a mail survey of 2000 randomly selected addresses in each state (sampling design). Each collection of 2000 addresses may constitute a random sample of households, but even with a 100% response rate the survey results will not be a random sample of adult U.S. residents because individuals share addresses and the population sizes are different from state to state."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html#bias",
    "href": "labs/lab2-sampling/lab2-sampling.html#bias",
    "title": "Lab 2: Sampling designs and statistical bias",
    "section": "Bias",
    "text": "Bias\nFormally, bias describes the ‘typical’ deviation of a sample statistic the correspongind population value.\nFor example, if a particular sampling design tends to produce an average measurement around 1.5 units, but the true average in the population is 2 units, then the estimate has a bias of -0.5 units. The language ‘typical’ and ‘tends to’ is important here. Estimates are never perfect, so just because an estimate is off by -0.5 units for one sample doesn’t make it biased – it is only biased if it is consistently off.\nAlthough bias is technically a property of a sample statistic (like the sample average), it’s common to talk about a biased sample – this term refers to a dataset collected using a sampling design that produces biased statistics.\nThis is exactly what you’ll explore in this lab – the relationship between sampling design and bias."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html#simulated-data",
    "href": "labs/lab2-sampling/lab2-sampling.html#simulated-data",
    "title": "Lab 2: Sampling designs and statistical bias",
    "section": "Simulated data",
    "text": "Simulated data\nYou will be simulating data in this lab. Simulation is a great means of exploration because you can control the population properties, which are generally unknown in practice.\nWhen working with real data, you just have one dataset, and you don’t know any of the properties of the population or what might have happened if a different sample were collected. That makes it difficult to understand sampling variation and impossible to directly compare the sample properties to the population properties.\nWith simulated data, by contrast, you control how data are generated with exact precision – so by extension, you know everything there is to know about the population. In addition, repeated simulation of data makes it possible to explore the typical behavior of a particular sampling design, so you can learn ‘what usually happens’ for a particular sampling design by direct observation."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html#hypothetical-population",
    "href": "labs/lab2-sampling/lab2-sampling.html#hypothetical-population",
    "title": "Lab 2: Sampling designs and statistical bias",
    "section": "Hypothetical population",
    "text": "Hypothetical population\nTo provide a little context to this scenario, imagine that you’re measuring eucalyptus seeds to determine their typical diameter. The cell below simulates diameter measurements for a hypothetical population of 5000 seeds; imagine that this is the total number of seeds in a small grove at some point in time.\n\n# simulate seed diameters\nnp.random.seed(40221) # for reproducibility\npopulation = pd.DataFrame(\n    data = {'diameter': np.random.gamma(shape = 2, scale = 1/2, size = 5000), \n            'seed': np.arange(5000)}\n).set_index('seed')\n\n# check first few rows\npopulation.head(3)\n\n\nQuestion 1\nCalculate the mean diameter for the hypothetical population and store the value as mean_diameter.\n\nmean_pop_diameter = ...\n\nmean_pop_diameter\n\n\ngrader.check(\"q1\")\n\n\n\nQuestion 2\nCalculate the standard deviation of diameters for the hypothetical population and store the value as std_dev_pop_diameter.\n\nstd_dev_pop_diameter = ...\nstd_dev_pop_diameter\n\n\ngrader.check(\"q2\")\n\nThe cell below produces a histogram of the population values – the distribution of diameter measurements among the hypothetical population – with a vertical line indicating the population mean.\n\n# base layer\nbase_pop = alt.Chart(population).properties(width = 400, height = 300)\n\n# histogram of diameter measurements\nhist_pop = base_pop.mark_bar(opacity = 0.8).encode(\n    x = alt.X('diameter', \n              bin = alt.Bin(maxbins = 20), \n              title = 'Diameter (mm)', \n              scale = alt.Scale(domain = (0, 6))),\n    y = alt.Y('count()', title = 'Number of seeds in population')\n)\n\n# vertical line for population mean\nmean_pop = base_pop.mark_rule(color='blue').encode(\n    x = 'mean(diameter)'\n)\n\n# display\nhist_pop + mean_pop"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html#random-sampling",
    "href": "labs/lab2-sampling/lab2-sampling.html#random-sampling",
    "title": "Lab 2: Sampling designs and statistical bias",
    "section": "Random sampling",
    "text": "Random sampling\nImagine that your sampling design involves collecting bunches of plant material from several locations in the grove and sifting out the seeds with a fine sieve until you obtaining 250 seeds. We’ll suppose that using your collection method, any of the 5000 seeds is equally likely to be obtained, so that your 250 seeds comprise a random sample of the population.\nWe can simulate samples obtained using your hypothetical design by drawing values without replacement from the population.\n\n# draw a random sample of seeds\nnp.random.seed(40221) # for reproducibility\nsample = population.sample(n = 250, replace = False)\n\n\nQuestion 3\nCalculate the mean diameter of seeds in the simulated sample and store the value as mean_sample_diameter.\n\nmean_sample_diameter = ...\nmean_sample_diameter\n\n\ngrader.check(\"q3\")\n\nYou should see above that the sample mean is close to the population mean. In fact, all sample statistics are close to the population; this can be seen by comparing the distribution of sample values with the distribution of population values.\n\n# base layer\nbase_samp = alt.Chart(sample).properties(width = 400, height = 300)\n\n# histogram of diameter measurements\nhist_samp = base_samp.mark_bar(opacity = 0.8).encode(\n    x = alt.X('diameter', \n              bin = alt.Bin(maxbins = 20),\n              scale = alt.Scale(domain = (0, 6)),\n              title = 'Diameter (mm)'),\n    y = alt.Y('count()', title = 'Number of seeds in sample')\n)\n\n# vertical line for population mean\nmean_samp = base_samp.mark_rule(color='blue').encode(\n    x = 'mean(diameter)'\n)\n\n# display\nhist_samp + mean_samp | hist_pop + mean_pop\n\nWhile there are some small differences, the overall shape is similar and the sample mean is almost exactly the same as the population mean. So with this sampling design, you obtained a dataset with few distortions of the population properties, and the sample mean is a good estimate of the population mean.\n\n\nAssessing bias through simulation\nYou may wonder: does that happen all the time, or was this just a lucky draw? This question can be answered by simulating a large number of samples and checking the average behavior to see whether the undistorted representation of the population is typical for this sampling design.\nThe cell below estimates the bias of the sample mean by:\n\ndrawing 1000 samples of size 300;\nstoring the sample mean from each sample;\ncomputing the average difference between the sample means and the population mean.\n\n\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\nnsim = 1000\n\n# storage for the sample means\nsamp_means = np.zeros(nsim)\n\n# repeatedly sample and store the sample mean\nfor i in range(0, nsim):\n    samp_means[i] = population.sample(n = 250, replace = False).diameter.mean()\n\nThe bias of the sample mean is its average distance from the population mean. We can estimate this using our simulation results as follows:\n\n# bias\nsamp_means.mean() - population.diameter.mean()\n\nSo the average error observed in 1000 simulations was about 0.001 mm! This suggests that the sample mean is unbiased: on average, there is no error. Therefore, at least with respect to estimating the population mean, random samples appear to be unbiased samples.\nHowever, unbiasedness does not mean that you won’t observe estimation error. There is a natural amount of variability from sample to sample, because in each sample a different collection of seeds is measured. We can estimate this as well using the simulation results by checking the standard deviation of the sample means across all 1000 samples:\n\nsamp_means.std()\n\nSo on average, the sample mean varies by about 0.04 mm from sample to sample.\nWe could also check how much the sample mean deviates from the population mean on average by computing root mean squared error:\n\nnp.sqrt(np.sum((samp_means - population.diameter.mean())**2)/1000)\n\nNote that this is very close to the variance of the sample mean across simulations, but not exactly the same; this latter calculation measures the spread around the population mean, and is a conventional measure of estimation accuracy.\nThe cell below plots a histogram representing the distribution of values of the sample mean across the 1000 samples you simulated (this is known as the sampling distribution of the sample mean). It shows a peak right at the population mean (blue vertical line) but some symmetric variation to either side – most values are between about 0.93 and 1.12.\n\n# plot the simulated sampling distribution\nsampling_dist = alt.Chart(pd.DataFrame({'sample mean': samp_means})).mark_bar().encode(\n    x = alt.X('sample mean', bin = alt.Bin(maxbins = 30), title = 'Value of sample mean'),\n    y = alt.Y('count()', title = 'Number of simulations')\n)\n\nsampling_dist + mean_pop"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html#biased-sampling",
    "href": "labs/lab2-sampling/lab2-sampling.html#biased-sampling",
    "title": "Lab 2: Sampling designs and statistical bias",
    "section": "Biased sampling",
    "text": "Biased sampling\nIn this scenario, you’ll use the same hypothetical population of eucalyptus seed diameter measurements and explore the impact of a biased sampling design.\nIn the first design, you were asked to imagine that you collected and sifted plant material to obtain seeds. Suppose you didn’t know that the typical seed is about 1mm in diameter and decided to use a sieve that is a little too coarse, tending only to sift out larger seeds and letting smaller seeds pass through. As a result, small seeds have a lower probability of being included in the sample and large seeds have a higher probability of being included in the sample.\nThis kind of sampling design can be described by assigning differential sampling weights \\(w_1, \\dots, w_N\\) to each observation. The cell below defines some hypothetical weights such that larger diameters are more likely to be sampled.\n\npopulation_mod1 = population.copy()\n\n\n# inclusion weight as a function of seed diameter\ndef weight_fn(x, r = 10, c = 1.5):\n    out = 1/(1 + np.e**(-r*(x - c)))\n    return out\n\n# create a grid of values to use in plotting the function\ngrid = np.linspace(0, 6, 100)\nweight_df = pd.DataFrame(\n    {'seed diameter': grid,\n     'weight': weight_fn(grid)}\n)\n\n# plot of inclusion probability against diameter\nweight_plot = alt.Chart(weight_df).mark_area(opacity = 0.3, line = True).encode(\n    x = 'seed diameter',\n    y = 'weight'\n).properties(height = 100)\n\n# show plot\nweight_plot\n\nThe actual probability that a seed is included in the sample – its inclusion probability – is proportional to the sampling weight. These inclusion probabilities \\(\\pi_i\\) can be calculated by normalizing the weights \\(w_i\\) over all seeds in the population \\(i = 1, \\dots, 5000\\):\n\\[\\pi_i = \\frac{w_i}{\\sum_i w_i}\\]\nIt may help you to picture how the weights will be used in sampling to line up this plot with the population distribution. In effect, we will sample more from the right tail of the population distribution, where the weight is nearest to 1.\n\nhist_pop & weight_plot\n\nThe following cell draws a sample with replacement from the hypothetical seed population with seeds weighted according to the inclusion probability given by the function above.\n\n# assign weight to each seed\npopulation_mod1['weight'] = weight_fn(population_mod1.diameter)\n\n# draw weighted sample\nnp.random.seed(40721)\nsample2 = population_mod1.sample(n = 250, replace = False, weights = 'weight').loc[:, ['diameter']]\n\n\nQuestion 4\nCalculate the mean diameter of seeds in the simulated sample and store the value as mean_sample2_diameter.\n\nmean_sample2_diameter = ...\n\nmean_sample2_diameter\n\n\ngrader.check(\"q4\")\n\n\n\n\nQuestion 5\nShow side-by-side plots of the distribution of sample values and the distribution of population values, with vertical lines indicating the corresponding mean on each plot.\nHint: copy the cell that produced this plot in scenario 1 and replace sample with sample2. Utilizing different methods is also welcome.\n\n# base layer\nbase_samp = ...\n\n# histogram of diameter measurements\nhist_samp = ...\n\n# vertical line for population mean\nmean_samp = ...\n\n# combine layers\n\n# display\nhist_samp + mean_samp | hist_pop + mean_pop\n\n\n\n\nAssessing bias through simulation\nHere you’ll mimic the simulation done in scenario 1 to assess the bias of the sample mean under this new sampling design.\n\npopulation_mod1.head()\n\n\n\nQuestion 6\nInvestigate the bias of the sample mean by:\n\ndrawing 1000 samples with observations weighted by inclusion probability;\nstoring the collection of sample means from each sample as samp_means;\ncomputing the average difference between the sample means and the population mean (in that order!) and storing the result as avg_diff.\n\n(Hint: copy the cell that performs this simulation in scenario 1, and be sure to replace population with population_mod1 and adjust the sampling step to include weights = ... with the appropriate argument.)\n\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\n\n# storage for the sample means\n\n# repeatedly sample and store the sample mean in the samp_means array\n\n# bias\n\navg_diff\n\n\ngrader.check(\"q6\")\n\n\n\n\nQuestion 7\nDoes this sampling design seem to introduce bias? If so, does the sample mean tend to over-estimate or under-estimate the population mean?\nType your answer here, replacing this text."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html#hypothetical-population-1",
    "href": "labs/lab2-sampling/lab2-sampling.html#hypothetical-population-1",
    "title": "Lab 2: Sampling designs and statistical bias",
    "section": "Hypothetical population",
    "text": "Hypothetical population\nSuppose you’re interested in determining the average beak-to-tail length of red-tailed hawks to help differentiate them from other hawks by sight at a distance. Females and males differ slightly in length – females are generally larger than males. The cell below generates length measurements for a hypothetical population of 3000 females and 2000 males.\n\n# for reproducibility\nnp.random.seed(40721)\n\n# simulate hypothetical population\nfemale_hawks = pd.DataFrame(\n    data = {'length': np.random.normal(loc = 57.5, scale = 3, size = 3000),\n            'sex': np.repeat('female', 3000)}\n)\n\nmale_hawks = pd.DataFrame(\n    data = {'length': np.random.normal(loc = 50.5, scale = 3, size = 2000),\n            'sex': np.repeat('male', 2000)}\n)\n\npopulation_hawks = pd.concat([female_hawks, male_hawks], axis = 0)\n\n# preview\npopulation_hawks.groupby('sex').head(2)\n\nThe cell below produces a histogram of the lengths in the population overall (bottom panel) and when distinguished by sex (top panel).\n\nbase = alt.Chart(population_hawks).properties(height = 200)\n\nhist = base.mark_bar(opacity = 0.5, color = 'red').encode(\n    x = alt.X('length', \n              bin = alt.Bin(maxbins = 40), \n              scale = alt.Scale(domain = (40, 70)),\n              title = 'length (cm)'),\n    y = alt.Y('count()', \n              stack = None,\n              title = 'number of birds')\n)\n\nhist_bysex = hist.encode(color = 'sex').properties(height = 100)\n\nhist_bysex & hist\n\nThe population mean – average length of both female and male red-tailed hawks – is shown below.\n\n# population mean\npopulation_hawks.mean(numeric_only = True)\n\nFirst try drawing a random sample from the population:\n\n# for reproducibility\nnp.random.seed(40821)\n\n# randomly sample\nsample_hawks = population_hawks.sample(n = 300, replace = False)\n\n\nQuestion 8\nDo you expect that the sample will contain equal numbers of male and female hawks? Think about this for a moment (you don’t have to provide a written answer), and then compute the proportions of individuals in the sample of each sex and store the result as a dataframe named proportion_hawks_sample. The dataframe should have one column named proportion and two rows indexed by sex.\nHint: group by sex, use .count(), and divide by the sample size. Be sure to rename the output column appropriately, as the default behavior produces a column called length.\n\nproportion_hawks_sample = ...\n\nproportion_hawks_sample\n\n\ngrader.check(\"q8\")\n\nThe sample mean is shown below, and is fairly close to the population mean. This should be expected, since you already saw in scenario 1 that random sampling is an unbiased sampling design with respect to the mean.\n\nsample_hawks.mean(numeric_only = True)"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html#biased-sampling-1",
    "href": "labs/lab2-sampling/lab2-sampling.html#biased-sampling-1",
    "title": "Lab 2: Sampling designs and statistical bias",
    "section": "Biased sampling",
    "text": "Biased sampling\nLet’s now consider a biased sampling design. Usually, length measurements are collected from dead specimens collected opportunistically. Imagine that male mortality is higher, so there are better chances of finding dead males than dead females. Suppose in particular that specimens are five times as likely to be male; to represent this situation, we’ll assign sampling weights of 5/6 to all male hawks and weights of 1/6 to all female hawks.\n\ndef weight_fn(sex, p = 5/6):\n    if sex == 'male':\n        out = p\n    else:\n        out = 1 - p\n    return out\n\nweight_df = pd.DataFrame(\n    {'length': [50.5, 57.5],\n     'weight': [5/6, 1/6],\n     'sex': ['male', 'female']})\n\nwt = alt.Chart(weight_df).mark_bar(opacity = 0.5).encode(\n    x = alt.X('length', scale = alt.Scale(domain = (40, 70))),\n    y = alt.Y('weight', scale = alt.Scale(domain = (0, 1))),\n    color = 'sex'\n).properties(height = 70)\n\nhist_bysex & wt\n\n\nQuestion 9\nDraw a weighted sample sample_hawks_biased from the population population_hawks using the weights defined by weight_fn, and compute and store the value of the sample mean as sample_hawks_biased_mean.\n\n# for reproducibility\nnp.random.seed(40821)\n\n# assign weights\n\n# randomly sample\n\n# compute mean\nsample_hawks_biased_mean = ...\n\nsample_hawks_biased_mean\n\n\ngrader.check(\"q9\")\n\n\n\nQuestion 10\nInvestigate the bias of the sample mean by:\n\ndrawing 1000 samples with observations weighted by weight_fn;\nstoring the sample mean from each sample as samp_means_hawks;\ncomputing the average difference between the sample means and the population mean and storing the resulting value as avg_diff_hawks.\n\n\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\n\n# storage for the sample means\nsamp_means_hawks = ...\n\n# repeatedly sample and store the sample mean in the samp_means array\n\n# bias\navg_diff_hawks = ...\n\navg_diff_hawks\n\n\ngrader.check(\"q10\")\n\n\n\n\nQuestion 11\nReflect a moment on your simulation result in question 3c. If instead female mortality is higher and specimens for measurement are collected opportunistically, as described in the previous sampling design, do you expect that the average length in the sample will be an underestimate or an overestimate of the population mean? Explain why in 1-2 sentences, and carry out a simulation to check your intuition.\nType your answer here, replacing this text.\n\nnp.random.seed(40221) # for reproducibility\n\n# invert weights\n\n# storage for the sample means\n\n# repeatedly sample and store the sample mean\n\n# compute bias\nestimated_bias = ...\n\nestimated_bias"
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted.html",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted.html",
    "title": "Lab 0: Getting started",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab0-gettingstarted.ipynb\")\nThis lab is meant to help you familiarize yourself with using the LSIT server and Jupyter notebooks. Some light review of numpy arrays is also included."
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted.html#jupyter-notebooks",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted.html#jupyter-notebooks",
    "title": "Lab 0: Getting started",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nJupyter notebooks are organized into ‘cells’ that can contain either text or codes. For example, this is a text cell.\nTechnically, Jupyter is an application/interface that runs atop a kernel – a programming-language-specific independent environment in which code cells are executed. This basic organization allows for interactive computing with text integration.\nSelecting a cell and pressing Enter will enter edit mode and allow you to edit the cell. From edit mode, pressing Esc will revert to command mode and allow you to navigate the notebook’s cells.\nIn edit mode, most of the keyboard is dedicated to typing into the cell’s editor. Thus, in edit mode there are relatively few shortcuts. In command mode, the entire keyboard is available for shortcuts, so there are many more. Here are a few useful ones:\n\nCtrl + Return : Evaluate the current cell\nShift + Return: Evaluate the current cell and move to the next\nSaving the notebook: s\nBasic navigation: up one cell k, down one cell j\na : create a cell above\nb : create a cell below\ndd : delete a cell\nz : undo the last cell operation\nm : convert a cell to markdown\ny : convert a cell to code\n\nTake a moment to find out what the following commands do:\n\nCell editing: x, c, v, z\nKernel operations: i, 0 (press twice)\n\n\n# Practice the above commands on this cell\n\n\nRunning Cells and Displaying Output\nRun the following cell.\n\nprint(\"Hello, World!\")\n\nIn Jupyter notebooks, all print statements are displayed below the cell. Furthermore, the output of only the last line is displayed following the cell upon execution.\n\n\"Will this line be displayed?\"\n\nprint(\"Hello\" + \",\", \"world!\")\n\n5 + 3\n\n\n\nViewing Documentation\nTo output the documentation for a function, use the help() function.\n\nhelp(print)\n\nYou can also use Jupyter to view function documentation inside your notebook. The function must already be defined in the kernel for this to work.\nBelow, click your mouse anywhere on print() and use Shift + Tab to view the function’s documentation.\n\nprint('Welcome to this course!')\n\n\n\nImporting Libraries\nIn this course, we will be using common Python libraries to help us retrieve, manipulate, and perform operations on data. By convention, we import all libraries at the very top of the notebook. There are also a set of standard aliases that are used to shorten the library names. Below are some of the libraries that you may encounter throughout the course, along with their respective aliases.\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted.html#practice-questions-and-numpy-review",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted.html#practice-questions-and-numpy-review",
    "title": "Lab 0: Getting started",
    "section": "Practice questions and numpy review",
    "text": "Practice questions and numpy review\nMost assignments for this class will be given as notebooks organized into explanation and prompts followed by response cells; you will complete assignments by filling in all of the response cells.\nMany response cells are followed by a test cell that performs a few checks on your work. Please be aware that test cells don’t always confirm that your response is correct or incorrect. They are meant to give you some useful feedback, but it’s your responsibility to interpret the feedback – please be sure to read and think about test output if tests fail, and make your own assessment of whether you need to revise your response.\nBelow are a few practice questions for you to familiarize yourself with the process. These assume familiarity with basic python syntax and the numpy package.\n\nQuestion 1\nWrite a function summation that evaluates the following summation for \\(n \\geq 1\\):\n\\[\\sum_{i=1}^{n} \\left(i^3 + 5 i^3\\right)\\]\nHint: np.arange(5).sum() will generate an array comprising \\(1, 2, \\dots, 5\\) and then add up the elements of the array.\n\ndef summation(n):\n    \"\"\"Compute the summation i^3 + 5 * i^3 for 1 &lt;= i &lt;= n.\"\"\"\n    ...\n\n\ngrader.check(\"q1\")\n\nUse your function to compute the sum for…\n\n# n = 2\n...\n\n\n# n = 20\n...\n\n\n\nQuestion 2\nThe core of numpy is the array. Let’s use np.array to create an array. It takes a sequence, such as a list or range (remember that list elements are included between the square brackets [ and ], such as [1, 5, 3]).\nBelow, create an array containing the values 1, 2, 3, 4, and 5 (in that order) and assign it the name my_array.\n\nmy_array = ...\n\n\ngrader.check(\"q2\")\n\nNumpy arrays are integer-indexed by position, with the first element indexed as position 0. Elements can be retrieved by enclosing the desired positions in brackets [].\n\nmy_array[3]\n\nTo retrieve consecutive positions, specify the starting index and the ending index separated by :, for instance, arr[from:to]. This syntax is non-inclusive of the left endpoint, meaning that the starting index is not included in the output.\n\nmy_array[2:4]\n\nIn addition to values in the array, we can access attributes such as array’s shape and data type that can be retrieved by name using syntax of the form array.attr. Some useful attributes are:\n\n.shape, a tuple with the length of each array dimension\n.size, the length of the first array dimension\n.dtype, the data type of the entries (float, integer, etc.)\n\nA full list of attributes is here.\n\nmy_array.shape\n\n\nmy_array.size\n\n\nmy_array.dtype\n\nArrays, unlike Python lists, cannot store items of different data types.\n\n# A regular Python list can store items of different data types\n[1, '3']\n\n\n# Arrays will convert everything to the same data type\nnp.array([1, '3'])\n\n\n# Another example of array type conversion\nnp.array([5, 8.3])\n\nArrays are also useful in performing vectorized operations. Given two or more arrays of equal length, arithmetic will perform element-wise computations across the arrays.\nFor example, observe the following:\n\n# Python list addition will concatenate the two lists\n[1, 2, 3] + [4, 5, 6]\n\n\n# NumPy array addition will add them element-wise\nnp.array([1, 2, 3]) + np.array([4, 5, 6])\n\nArrays can be subsetted by index position, as shown above, or by a logical vector of the same length. For example:\n\nexample_arr = np.arange(4, 10)\nexample_arr\n\nSuppose we want the last three elements. One option is to use index position:\n\nexample_arr[3:6]\n\nOr a logical vector:\n\nexample_arr[np.array([False, False, False, True, True, True])]\n\nThe latter approach allows one to subset based on a condition defined by the values of the vector. For example, we can use the condition \\(x \\geq 7\\) to obtain the logical vector used above.\n\nexample_arr &gt;= 7\n\nAnd then we can subset just as before:\n\nexample_arr[example_arr &gt;= 7]\n\nYou’ll see this done frequently, and it’s sometimes referred to as filtering, because we’re selectively removing values.\n\n\nQuestion 3\nGiven the array random_arr, create an array containing all values \\(x\\) such that \\(2x^4 &gt; 1\\). Name the array valid_values.\n\n# for reproducibility - setting the seed will result in the same random draw each time\nnp.random.seed(42)\n\n# draw 60 uniformly random integers between 0 and 1\nrandom_arr = np.random.rand(60)\n\n# solution here\nvalid_values = ...\n\n\ngrader.check(\"q3\")\n\n\n\nA note on np.arange and np.linspace\nUsually we use np.arange to return an array that steps from a to b with a fixed step size s. While this is fine in some cases, we sometimes prefer to use np.linspace(a, b, N), which divides the interval [a, b] into N equally spaced points.\nnp.arange(start, stop, step) produces an array with all the numbers starting at start, incremendted up by step, stopping before stop is reached. For example, the value of np.arange(1, 6, 2) is an array with elements 1, 3, and 5 – it starts at 1 and counts up by 2, then stops before 6. np.arange(4, 9, 1) is an array with elements 4, 5, 6, 7, and 8. (It doesn’t contain 9 because np.arange stops before the stop value is reached.)\nnp.linspace always includes both end points while np.arange will not include the second end point b. For this reason, especially when we are plotting ranges of values we tend to prefer np.linspace.\nNotice how the following two statements have different parameters but return the same result.\n\nnp.arange(-5, 6, 1.0)\n\n\nnp.linspace(-5, 5, 11)\n\nCheck your understanding. Will np.arange(1, 10) produce an array that contains 10? Add a cell below and check to confirm your answer.\n\nTo double-check your work, the cell below will rerun all of the autograder tests.\n\ngrader.check_all()"
  },
  {
    "objectID": "labs/lab6-regression/lab6-regression.html",
    "href": "labs/lab6-regression/lab6-regression.html",
    "title": "Lab 6: Regression",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab6-regression.ipynb\")\nimport numpy as np\nimport pandas as pd\nimport altair as alt\nimport statsmodels.api as sm\n\n# disable row limit for plotting\nalt.data_transformers.disable_max_rows()\n# uncomment to ensure graphics display with pdf export\n# alt.renderers.enable('mimetype')\nThis lab covers the nuts and bolts of fitting linear models. The linear model expresses a response variable, \\(y\\), as a linear function of \\(p - 1\\) explanatory variables \\(x_1, \\dots, x_{p - 1}\\) and a random error \\(\\epsilon\\). Its general form is:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_{p - 1} x_{p - 1} + \\epsilon \\qquad \\epsilon \\sim N(0, \\sigma^2)\\]\nUsually, the response and explanatory variables and error term are indexed by observation \\(i = 1, \\dots, n\\) so that the model describes a dataset comprising \\(n\\) values of each variable:\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_{p - 1} x_{i, p - 1} + \\epsilon_i \\qquad\\begin{cases} \\epsilon_i \\sim N(0, \\sigma^2) \\\\ i = 1, \\dots, n\\end{cases}\\]\nBecause the indices get confusing to keep track of, it is much easier to express the model in matrix form as\n\\[\\mathbf{y} = \\mathbf{X}\\beta + \\epsilon\\]\nwhere:\n\\[\\mathbf{y} = \\left[\\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array}\\right]_{\\;n \\times 1} \\qquad\n    \\mathbf{X} = \\left[\\begin{array}{cccc}\n        1 &x_{11} &\\cdots &x_{1, p - 1} \\\\\n        1 &x_{21} &\\cdots &x_{2, p - 1} \\\\\n        \\vdots &\\vdots &\\ddots &\\vdots \\\\\n        1 &x_{n1} &\\cdots &x_{n, p - 1}\n        \\end{array}\\right]_{\\;n \\times p} \\qquad\n    \\beta = \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{p - 1} \\end{array} \\right]_{\\;p \\times 1} \\qquad\n    \\epsilon = \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right]_{\\;n \\times 1}\\]\nFitting a model of this form means estimating the parameters \\(\\beta_0, \\beta_1, \\dots, \\beta_{p - 1}\\) and \\(\\sigma^2\\) from a set of data.\nWhen fitting a linear model, it is also of interest to quantify uncertainty by estimating the variability of \\(\\hat{\\beta}\\) and measure overall quality of fit. This lab illustrates that process and the computations involved.\nObjectives\nIn this lab, you’ll learn how to:\nThroughout you’ll use simple visualizations to help make the connection between fitted models and the aspects of a dataset that model features describe."
  },
  {
    "objectID": "labs/lab6-regression/lab6-regression.html#estimation",
    "href": "labs/lab6-regression/lab6-regression.html#estimation",
    "title": "Lab 6: Regression",
    "section": "Estimation",
    "text": "Estimation\n‘Fitting’ a model refers to computing estimates; statsmodels.OLS() will fit a linear regression model based on the response vector and explanatory variable matrix. Note that the model structure is implicit – OLS will fit \\(y = X\\beta + \\epsilon\\) no matter what, so you need to be sure you have arranged \\(X\\) and \\(y\\) correctly to fit the model that you intend.\n\n# fit model\nslr = sm.OLS(endog = y, exog = x)\n\nThis returns an object of a distinct model class specific to OLS:\n\ntype(slr)\n\nAssociated with the class are various attributes and methods. From the model instance, .fit() retrieves the model results:\n\ntype(slr.fit())\n\nNote, however, that slr.fit() will not produce any interesting output:\n\nslr.fit()\n\nWhat the .fit() method does is create a results object that contains parameter estimates and other quantities we might want to retrieve.\n\nrslt = slr.fit()\n\nThe coeffient estimates \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\) are:\n\nrslt.params\n\nThe error variance estimate \\(\\hat{\\sigma}^2\\) is:\n\nrslt.scale\n\nIt was noted in lecture that the variances and covariances of \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\) are given by the matrix:\n\\[\\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1}\n  = \\left[\\begin{array}{cc}\n        \\text{var}\\hat{\\beta}_0 & \\text{cov}\\left(\\hat{\\beta}_0, \\hat{\\beta}_1\\right) \\\\\n        \\text{cov}\\left(\\hat{\\beta}_1, \\hat{\\beta}_0\\right) & \\text{var}\\hat{\\beta}_1\n        \\end{array}\\right]\\]\nSo we can estimate these quantities, which quantify the variation and covariation of the estimated coefficients, by plugging in the estimated error variance and computing \\(\\hat{\\sigma}^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\). This estimate is:\n\nrslt.cov_params()\n\nStandard errors for the coefficient estimates are obtained from the diagonal entries. We might create a nice summary of all the estimates as follows:\n\ncoef_tbl = pd.DataFrame({'estimate': rslt.params.values,\n              'standard error': np.sqrt(rslt.cov_params().values.diagonal())},\n              index = x.columns)\ncoef_tbl.loc['error variance', 'estimate'] = rslt.scale\n\ncoef_tbl\n\nLastly, a standard metric often reported with linear models is the \\(R^2\\) score, which is interpreted as the proportion of variation in the response captured by the model.\n\n# compute R-squared\nrslt.rsquared\n\nSo, the expected years of education for women in a country explains 72% of variability in fertility rates, and furthermore, according to the fitted model:\n\nfor a country in which women are entirely uneducated, the estimated mean fertility rate is 7.5 children on average by the end of a woman’s reproductive period\neach additional year of education for women is associated with a decrease in a country’s fertility rate of an estimated 0.43\nafter accounting for women’s education levels, fertility rates vary by a standard deviation of \\(0.66 = \\sqrt{0.438}\\) across countries\n\n\nQuestion 3: center the explanatory variable\nNote that no countries report an expected zero years of education for women, so the meaning of the intercept is artificial. As we saw in lecture, centering the explanatory variable can improve interpretability of the intercept. Center the expected years of education for women and refit the model by following the steps outlined below. Display the coefficient estimates and standard errors.\n\n# center the education column by subtracting its mean from each value\neduc_ctr = ...\n\n# reconstruct the explanatory variable matrix\nx_ctr = ...\n\n# fit new model\nslr_ctr = ...\nrslt_ctr = ...\n\n# arrange estimates and se's in a dataframe and display\n...\n\n\ngrader.check(\"q3\")"
  },
  {
    "objectID": "labs/lab6-regression/lab6-regression.html#fitted-values-and-residuals",
    "href": "labs/lab6-regression/lab6-regression.html#fitted-values-and-residuals",
    "title": "Lab 6: Regression",
    "section": "Fitted values and residuals",
    "text": "Fitted values and residuals\nThe fitted value for \\(y_i\\) is the value along the line specified by the model that corresponds to the matching explanatory variable \\(x_i\\). In other words:\n\\[\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\]\nThese can be obtained directly from rslt:\n\n# fitted values\nrslt.fittedvalues\n\nThe result is an array with length matching the number of rows in x; note the index for the pandas series – the fitted values are returned in the same order as the observations used to fit the model.\n\n(rslt.fittedvalues.index == x.index).all()\n\nRecall that model residuals are the difference between observed and fitted values:\n\\[e_i = y_i - \\hat{y}_i\\]\nThese are similarly retrievable as an attribute of the regression results:\n\n# residuals\nrslt.resid\n\nNote again that these are returned in the same order as the original observations.\n\nQuestion 4: calculations ‘by hand’\nCalculate the fitted values and residuals manually. Store the results as arrays fitted_manual and resid_manual, respectively.\nHint: use matrix-vector multiplication.\n\nfitted_manual = ...\nresid_manual = ...\n\n\ngrader.check(\"q4\")\n\nIt is often convenient to add the fitted values and residuals as new columns in reg_data:\n\n# append fitted values and residuals\nreg_data['fitted_slr'] = rslt.fittedvalues\nreg_data['resid_slr'] = rslt.resid\n\nreg_data.head(3) \n\nWe can use this augmented dataframe to visualize the deterministic part of the model:\n\n# construct line plot\nslr_line = alt.Chart(reg_data).mark_line().encode(\n    x = 'educ_expected_yrs_f',\n    y = 'fitted_slr'\n)\n\n# layer\nscatter_educ + slr_line\n\nTo obtain uncertainty bands about the estimated mean, we’ll compute predictions at each observed value using .get_prediction() – this method by default returns standard errors associated with each prediction.\n\npreds = rslt.get_prediction(x)\n\nThe predictions are stored as .predicted_mean. Since we computed predictions at the observed values, the predictions should match the fitted values:\n\n(preds.predicted_mean == rslt.fittedvalues).all()\n\nStandard errors are stored as .se_mean. Uncertainty bands are typically drawn \\(2SE\\) in either direction from the fitted values; so we’ll append those values to the original data.\n\nreg_data['lwr_mean'] = preds.predicted_mean - 2*preds.se_mean\nreg_data['upr_mean'] = preds.predicted_mean + 2*preds.se_mean\n\nreg_data.head()\n\nWe can use these to shade in the area between the lower and upper limits.\n\nband = alt.Chart(reg_data).mark_area(opacity = 0.2).encode(\n    x = 'educ_expected_yrs_f',\n    y = 'lwr_mean',\n    y2 = 'upr_mean'\n)\n\n# layer\nscatter_educ + slr_line + band\n\nAs discussed in lecture, we can also compute and display uncertainty bounds for predicted observations (rather than the mean). These will be wider, because there is more uncertainty associated with predicting observations compared with estimating the mean.\n\n\nQuestion 5: prediction intervals\nThe standard error for predictions is stored with the output of .get_prediction() as the attribute .se_obs – standard error for observations. Use this and follow the example above to compute 95% uncertainty bounds for the observations. Add the lower and upper bounds as new columns of reg_data named lwr_obs and upr_obs, respectively. Construct a plot showing data scatter, the model predictions, and prediction uncertainty bands.\n\n# compute prediction uncertainty bounds\nreg_data['lwr_obs'] = ...\nreg_data['upr_obs'] = ...\n\n# construct plot showing prediction uncertainty\n...\n\n\ngrader.check(\"q5\")\n\nRecall that the interpretation of the prediction band is that 95% of the time, the band will cover the observed values.\n\n\nQuestion 6: coverage\nWhat proportion of observed values are within the prediction bands? Compute and store this value as coverage_prop.\n\ncoverage_prop = ...\n\n\ngrader.check(\"q6\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas.html",
    "href": "labs/lab1-pandas/lab1-pandas.html",
    "title": "Lab 1: Pandas Overview",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab1-pandas.ipynb\")\nPandas is one of the most widely used Python libraries in data science. In this lab, you will learn commonly used data tidying operations/tools in Pandas."
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas.html#creating-dataframes-basic-manipulations",
    "href": "labs/lab1-pandas/lab1-pandas.html#creating-dataframes-basic-manipulations",
    "title": "Lab 1: Pandas Overview",
    "section": "Creating DataFrames & Basic Manipulations",
    "text": "Creating DataFrames & Basic Manipulations\nA dataframe is a table in which each column has a type; there is an index over the columns (typically string labels) and an index over the rows (typically ordinal numbers). An index is represented by a series object, which is a one-dimensional labeled array. Here you’ll cover:\n\ncreating dataframes from scratch;\nretrieving attributes;\ndataframe indexing;\nadding, removing, and renaming columns.\n\n\nCreating dataframes from scratch\nThe documentation for the pandas DataFrame class provide two primary syntaxes to create a data frame from scratch:\n\nfrom a dictionary\nrow-wise tuples\n\nSyntax 1 (dictionary): You can create a data frame by specifying the columns and values using a dictionary (a concatenation of named lists) as shown below.\nThe keys of the dictionary are the column names, and the values of the dictionary are lists containing the row entries.\n\n# define a dataframe using dictionary syntax\nfruit_info = pd.DataFrame( \n    data = { 'fruit': ['apple', 'orange', 'banana', 'raspberry'],\n             'color': ['red', 'orange', 'yellow', 'pink']\n           })\n\n# print\nfruit_info\n\nSyntax 2 (row tuples): You can also define a dataframe by specifying the rows as tuples.\nEach row corresponds to a distinct tuple, and the column indices are specified separately.\n\n# define the same dataframe using tuple syntax \nfruit_info2 = pd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"]\n)\n\n# print\nfruit_info2\n\n\n\nDataframe Attributes\nDataFrames have several basic attributes:\n\n.shape contains dimensions;\n.dtypes contains data types (float, integer, object, etc.)\n.size first (row) dimension;\n.values contains an array comprising each entry in the dataframe.\n.columns contains the column index;\n.index contains the row index.\n\nYou can obtain these attributes by appending the attribute name to the dataframe name. For instance, the dimensions of a dataframe df can be retrieved by df.shape.\n\n# dimensions\nfruit_info.shape\n\nTo retrieve a two-dimensional numpy array with the values of the dataframe, use df.values. It is sometimes useful to extract this data structure in order to perform vectorized operations, linear algebra, and the like.\n\n# as array\nfruit_info.values\n\n\n\nDataframe Indexing\nThe entries in a dataframe are indexed. Indices for rows and columns are stored as the .index. and .columns attributes, respectively.\n\nfruit_info.columns\n\n\nfruit_info.index\n\nBy default, the row indexing is simply numbering by consecutive integers.\n\nfruit_info.index.values\n\nHowever, rows can alternatively be indexed by labels:\n\n# define with a row index\npd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"],\n    index = [\"fruit 1\", \"fruit 2\", \"fruit 3\", 'fruit 4']\n)\n\nUnlike data frames in R, the row index label figures prominently in certain operations. The elements of the dataframe can be retrived using .loc[ROW-INDEX, COL-INDEX] which specifies the location of data values by name (not by position).\n\n# retrieve row 0, column 'fruit'\nfruit_info.loc[0, 'fruit']\n\nMost of the time rows are indexed numerically, and somewhat confusingly, the syntax for .loc does not require putting the row index 0 in quotes, even though it refers to the row label and not the row number. This is important to remember, because often operations will scramble the order of rows. To see the difference, consider the following:\n\n# non-consecutive row index\npd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"],\n    index = np.array([8, 6, 4, 2])\n)\n\nNow adding .loc[2, 'color'] will retrieve pink, the last row of the dataframe.\n\n# subset\npd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"],\n    index = np.array([8, 6, 4, 2])\n).loc[2, 'color']\n\nTo retrieve values by position, use .iloc. For many, this is more intuitive, as it is most similar to matrix or array indexing in mathematical notation.\n\n# retrieve 0, 0 entry\nfruit_info.iloc[0, 0]\n\n\n\nAdding, removing, and renaming columns\nThere are two ways to add new columns:\n\ndirect specification;\nusing .loc[].\n\nDirect specification: For a dataFrame df, you can add a column with df['new column name'] = ... and assign a list or array of values to the column.\nUsing .loc[]: For a dataframe df, you can add a column with df.loc[:, 'new column name'] = ... and assign a list or array of values to the column.\nBoth accomplish the same task – adding a new column index and populating values for each row – but .loc[] is a little faster.\n\n\nQuestion 1\nUsing direct specification, add to the fruit_info table a new column called rank1 containing integers 1, 2, 3, and 4, which express your personal preference about the taste ordering for each fruit (1 is tastiest; 4 is least tasty). Make sure that the numbers utilized are unique - no ties are allowed.\n\n\n...\n\n# print\nfruit_info\n\n\ngrader.check(\"q1\")\n\nNow, create a new dataframe fruit_info_mod1 with the same information as fruit_info_original, but has the additional column rank2. Let’s start off with making fruit_info_mod1 as a copy of fruit_info:\n\nfruit_info_mod1 = fruit_info.copy()\n\n\n\nQuestion 2\nUsing .loc[], add a column called rank2 to the fruit_info_mod1 table that contains the same values in the same order as the rank1 column.\nHint: .loc will parse : as shorthand for ‘all indices’.\n\n\n...\n\n# print\nfruit_info_mod1\n\n\ngrader.check(\"q2\")\n\nWhen using the .loc[] approach, the : specifies that values are assigned to all rows of the data frame, so the array assigned to the new variable must be the same length as the data frame. What if we only assign values to certain rows? Try running the cell below.\n\n# define new variable just for rows 1 and 2\nfruit_info_mod1.loc[1:2, 'rank3'] = [1, 2]\n\n# check result\nfruit_info_mod1\n\nThe remaining rows are assigned missing values. Notice what this does to the data type:\n\n# check data types\nfruit_info_mod1.dtypes\n\nWe can detect these missing values using .isna():\n\n# returns a logical data frame indicating whether each entry is missing or not\nfruit_info_mod1.isna()\n\nIt would be more helpful to simply see by column whether there are missing values. Appending a .any() to the above command will do the trick:\n\n# detects whether any column has missing entries\nfruit_info_mod1.isna().any()\n\nNow that we’ve had a bit of fun let’s remove those rank variables. Columns can be removed using .drop() with a list of column names to drop as its argument. For example:\n\n# first syntax for .drop()\nfruit_info_mod1.drop(columns = 'color')\n\nThere is an alternate syntax to that shown above, which involves specifying the axis (row vs. column) and index name to drop:\n\n# second syntax for .drop()\nfruit_info_mod1.drop('color', axis = 1)\n\n\n\nQuestion 3\nUse the .drop() method to drop all rank columns you created in fruit_info_mod1. Note that drop does not change the table, but instead returns a new table with fewer columns or rows. To store the result, assign a new name (or write over the old dataframe). Here, assign the result to fruit_info_original.\nHint: Look through the documentation to see how you can drop multiple columns of a Pandas dataframe at once using a list of column names.\n\n\nfruit_info_original = ...\n\n# print\nfruit_info_original\n\n\ngrader.check(\"q3\")\n\nNifty trick: Use df.columns[df.columns.str.startswith('STRING')] to retrieve all indices starting with STRING and ix.values.tolist() to convert an index to an array of index names to obtain a list of column names to drop. Combining these gives df.columns[df.columns.str.startswith('STRING')].values.tolist(), and will return a list of all column names starting with STRING. This can be used in conjunction with the hint to remove all columns starting with rank.\n\n# try the nifty trick here\n\nNow create a new dataframe fruit_info_mod2with the same information as fruit_info_original, but has the column names capitalized. Begin by creating a copy fruit_info_mod2 of fruit_info_original:\n\nfruit_info_mod2 = fruit_info_original.copy()\n\n\n\nQuestion 4\nReview the documentation for .rename(). Based on the examples, rename the columns of fruit_info_mod2 so they begin with capital letters.\nFor many operations, you can change the dataframe ‘in place’ without reassigning the result of the operation to a new name by setting the inplace parameter to True. Use that strategy here.\n\n\n...\n\n# print\nfruit_info_mod2\n\n\ngrader.check(\"q4\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas.html#operations-on-data-frames",
    "href": "labs/lab1-pandas/lab1-pandas.html#operations-on-data-frames",
    "title": "Lab 1: Pandas Overview",
    "section": "Operations on Data Frames",
    "text": "Operations on Data Frames\nWith some basics in place, here you’ll see how to perform subsetting operations on data frames that are useful for tidying up datasets.\n\nSlicing: selecting columns or rows in chunks or by position.\n\nOften imported data contain columns that are either superfluous or not of interest for a particular project.\nYou may also want to examine particular portions of a data frame.\n\nFiltering: selecting rows that meet certain criteria\n\nOften you’ll want to remove duplicate rows, filter missing observations, or select a structured subset of a data frame.\nAlso helpful for inspection.\n\n\nTo illustrate these operations, you’ll use a dataset comprising counts of the given names of babies born in California each year from 1990 - 2018. The cell below imports the baby names data as a data frame from a .csv file. .head() prints the first few rows of the dataset.\n\n# import baby names data\nbaby_names = pd.read_csv('data/baby_names.csv')\n\n# preview first few rows\nbaby_names.head()\n\nYour focus here isn’t on analyzing this data, so we won’t ask you to spend too much effort getting acquainted with it. However, a brief inspection is always a good idea. Let’s check:\n\ndimensions (number of rows and columns);\nhow many distinct states, sexes, and years.\n\nNote that the above dataframe displayed is a preview of the full dataframe.\n\nQuestion 5\nYou’ve already seen how to examine dimensions using dataframe attributes. Check the dimensions of baby_names and store them in dimensions_baby_names.\n\n\ndimensions_baby_names = ...\n\n\ngrader.check(\"q5\")\n\nYou haven’t yet seen how to retrieve the distinct values of an array or series, without duplicates. There are a few different ways to go about this, but one is to count the number of occurrences of each distinct entry in a column. This can be done by retrieving the column as a series using syntax of the form df.colname, and then pass the result to .value_counts():\n\n# count distinct values\nbaby_names.Sex.value_counts()\n\n\n\nQuestion 6\nCount the number of occurences of each distinct year. Create a series occur_per_year that displays the number of occurrences, ordered by year (so that the years are displayed in order). If you add sort = False as an argument to value_counts, the distinct values will be displayed in the order they appear in the dataset.\nHow many years are represented in the dataset? Store your answer as num_years.\n\n\noccur_per_year = ...\n\nnum_years = ...\n\nprint(occur_per_year)\nprint(num_years)\n\n\ngrader.check(\"q6\")\n\n\n\nSlicing: selecting rows and columns\nThere are two fast and simple ways to slice dataframes:\n\nusing .loc to specify rows and columns by index;\nusing .iloc to specify rows and columns by position.\n\nYou have seen simple examples of both of these above. Here we’ll show how to use these two commands to retrieve multiple rows and columns.\n\nSlicing with .loc: specifying index names\nThis method retrieves entries by specifying row and column indexes using syntax of the form df.loc[rows, cols]. The rows and columns can be single indices, a list of indices, or a set of adjacent indices using a colon :. Examples of these usages are shown below.\n\n# single indices -- small slice\nbaby_names.loc[2, 'Name']\n\n\n# a list of indices -- larger slice\nbaby_names.loc[[2, 3], ['Name', 'Count']]\n\n\n# consecutive indices -- a chunk\nbaby_names.loc[2:10, 'Year':'Count']\n\n\n\n\nSlicing with .iloc: specifying entry positions\nAn alternative to specifying the indices in order to slice a dataframe is to specify the entry positions using .iloc (‘integer location’). You have seen an example of this too. As with .loc, .iloc can be used to select multiple rows/columns using either lists of positions or a consecutive set with from:to syntax.\n\n# single position\nbaby_names.iloc[2, 3]\n\n\n# a list of positions\nbaby_names.iloc[[2, 3], [3, 4]]\n\n\n# consecutive positions\nbaby_names.iloc[2:11, 2:5]\n\nWhile these commands may look very similar to their .loc analogs, there are some subtle but important differences. The row selection looks nearly identical, but recall that .loc uses the index and .iloc uses the position; they look so similar because typically index and position coincide.\nHowever, sorting the baby_names dataframe helps to reveal how the position of a row is not necessarily equal to the index of a row. For example, the first row is not necessarily the row associated with index 1. This distinction is important in understanding the difference between .loc[] and .iloc[].\n\n# sort and display\nsorted_baby_names = baby_names.sort_values(by=['Name'])\nsorted_baby_names.head()\n\nHere is an example of how we would get the 2nd, 3rd, and 4th rows with only the Name column of the baby_names dataframe using both iloc[] and loc[]. Observe the difference, especially after sorting baby_names by name.\n\n# example iloc usage\nsorted_baby_names.iloc[1:4, 3]\n\nNotice that using loc[] with 1:4 gives different results, since it selects using the index. The index gets moved around when you perform an operation like sort on the dataframe.\n\n# same syntax, different result\nsorted_baby_names.loc[1:4, \"Name\"]\n\nAbove, the .loc method retrieves all indexes between index 1 and index 4 in the order they appear in the sorted dataset. If instead we want to retrieve the same rows returned by the .iloc command, we need to specify the row indices explicitly as a list:\n\n# retrieve the same rows as iloc using loc\nsorted_baby_names.loc[[178791, 163914, 171112], 'Name']\n\nSometimes it’s useful for slicing (and other operations) to set one of the columns to be a row index, effectively treating one column as a collection of row labels. This can be accomplished using set_index.\n\n# change the (row) index from 0,1,2,... to the name column\nbaby_names_nameindexed = baby_names.set_index(\"Name\") \nbaby_names_nameindexed.head()\n\nWe can now slice by name directly:\n\n# slice rows for ashley and jennifer\nbaby_names_nameindexed.loc[['Ashley', 'Jennifer'], :]\n\n\n\nQuestion 7\nLook up the name of a friend! Store the name as friend_name. Use the name-indexed data frame to slice rows for the name of your choice and the Count, Sex, and Year columns in that order, and store the data frame as friend_slice.\n\n\n# if your friend's name is not in the database, use another name\n\nfriend_name = ...\nfriend_slice = ...\n\n#print\nfriend_slice\n\n\ngrader.check(\"q7\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas.html#filtering",
    "href": "labs/lab1-pandas/lab1-pandas.html#filtering",
    "title": "Lab 1: Pandas Overview",
    "section": "Filtering",
    "text": "Filtering\nFiltering is sifting out rows according to a criterion, and can be accomplished using an array or series of Trues and Falses defined by a comparison. To take a simple example, say you wanted to filter out all names with fewer than 1000 occurrences. First you could define a logical series:\n\n# true if filtering criterion is met, false otherwise\narr = baby_names.Count &gt; 1000\n\nThen you can filter using that array:\n\n# filter\nbaby_names_filtered = baby_names[arr]\nbaby_names_filtered.head()\n\nNotice that the filtered array is much smaller than the overall array – only about 2000 rows correspond to a name occurring more than 1000 times in a year for a gender.\n\n# compare dimensions\nprint(baby_names_filtered.shape)\nprint(baby_names.shape)\n\nYou have already encountered this concept in lab 0 when subsetting an array. For your reference, some commonly used comparison operators are given below.\n\n\n\nSymbol\nUsage\nMeaning\n\n\n\n\n==\na == b\nDoes a equal b?\n\n\n&lt;=\na &lt;= b\nIs a less than or equal to b?\n\n\n&gt;=\na &gt;= b\nIs a greater than or equal to b?\n\n\n&lt;\na &lt; b\nIs a less than b?\n\n\n&gt;\na &gt; b\nIs a greater than b?\n\n\n~\n~p\nReturns negation of p\n\n\n|\np | q\np OR q\n\n\n&\np & q\np AND q\n\n\n^\np ^ q\np XOR q (exclusive or)\n\n\n\nWhat if instead you wanted to filter using multiple conditions? Here’s an example of retrieving rows with counts exceeding 1000 for only the year 2001:\n\n# filter using two conditions\nbaby_names[(baby_names.Year == 2000) & (baby_names.Count &gt; 1000)]\n\n\nQuestion 8\nSelect the girl names in 2010 that were given more than 3000 times, and store them as common_girl_names_2010.\nNote: Any time you use p & q to filter the dataframe, make sure to use df[df[(p) & (q)]] or df.loc[df[(p) & (q)]]). That is, make sure to wrap conditions with parentheses to ensure the intended order of operations.\n\n\ncommon_girl_names_2010 = ...\n\ncommon_girl_names_2010\n\n\ngrader.check(\"q8\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas.html#grouping-and-aggregation",
    "href": "labs/lab1-pandas/lab1-pandas.html#grouping-and-aggregation",
    "title": "Lab 1: Pandas Overview",
    "section": "Grouping and aggregation",
    "text": "Grouping and aggregation\nGrouping and aggregation are useful in generating data summaries, which are often important starting points in exploring a dataset.\n\nAggregation\nAggregation literally means ‘putting together’ (etymologically the word means ‘joining the herd’) – in statistics and data science, this refers to data summaries like an average, a minimum, or a measure of spread such as the sample variance or mean absolute deviation (data herding!). From a technical point of view, operations that take multiple values as inputs and return a single output are considered summaries – in other words, statistics. Some of the most common aggregations are:\n\nsum\nproduct\ncount\nnumber of distinct values\nmean\nmedian\nvariance\nstandard deviation\nminimum/maximum\nquantiles\n\nPandas has built-in dataframe operations that compute most of these summaries across either axis (column-wise or row-wise):\n\n.sum()\n.prod()\n.mean()\n.median()\n.var()\n.std()\n.nunique()\n.min() and .max()\n.quantile()\n\nTo illustrate these operations, let’s filter out all names in 1995.\n\n# filter 1995 names\nnames_95 = baby_names[baby_names.Year == 1995]\n\nHow many individuals were counted in total in 1995? We can address that by computing a sum of the counts:\n\n# n for 1995\nnames_95.Count.sum()\n\nWhat is the typical frequency of all names in 1995? We can address that by computing the average count:\n\n# average count for a name in 1995\nnames_95.Count.mean()\n\n\n\nQuestion 9\nFind how often the most common name 1995 was given and store this as names_95_max_count. Use this value to filter names_95 and find which name was most common that year. Store the filtered dataframe as names_95_most_common_name.\n\n\nnames_95_max_count = ...\nnames_95_most_common_name = ...\n\n\nprint(\"Number of people with the most frequent name in 1995 is :\", names_95_max_count, \"people\")\nprint(\"Most frequent name in 1995 is:\", names_95_most_common_name.values[0])\n\n\ngrader.check(\"q9\")\n\nCaution! If applied to the entire dataframe, the operation df.max() (or any other aggregation) will return the maximum of each column. Notice that the cell below does not return the row you found just now, but could easily be misinterpreted as such. The cell does tell you that the maximum value of sex (alphabetically last) is M and the maximum name (alphabetically last) is Zyanya and the maximum count is 5003; it does not tell you that 5003 boys were named Zyanya in 1995.\n\n# maximum of each variable\nnames_95.max()\n\n\n\nGrouping\nWhat if you want to know the most frequent male and female names? If so, you’ll need to repeat the above operations group-wise by sex.\nIn general, any variable in a dataframe can be used to define a grouping structure on the rows (or, less commonly, columns). After grouping, any dataframe operations will be executed within each group, but not across groups. This can be used to generate grouped summaries, such as the maximum count for boys and girls; as a point of terminology, we’d describe this summary as ‘maximum count by sex’.\nThe .groupby() function defines such a structure; here is the documentation. The cell below groups the names_95 dataframe by sex. Notice that when the grouped dataframe is previewed with .head(), the first few rows are returned for each group.\n\n# grouped dataframe\nnames_95_bysex = names_95.groupby('Sex')\n\n# print\nnames_95_bysex.head(2)\n\nAny aggregation operations applied to the grouped dataframe will be applied separately to the rows where Sex == M and the rows where Sex == F. For example, computing .sum() on the grouped dataframe will show the total number of individuals in the data for 1995 by sex:\n\n# number of individuals by sex\nnames_95_bysex.Count.sum()\n\nThe most frequent boy and girl names can be found using .idxmax() groupwise to obtain the index of the first occurence of the maximum count for each sex, and then slicing with .loc:\n\n# first most common names by sex\nnames_95.loc[names_95_bysex.Count.idxmax(), :]\n\nSince .idxmax() gives the index of the first occurrence, these are the alphabetically first most common names; there could be ties. You know from your work so far that there are no ties for the male names; another filtering step can be used to check for ties among the female names.\n\n# ties?\nnames_95[names_95_bysex.Count.max().values[0] == names_95['Count']]\n\nSo, no ties.\n\n\nQuestion 10\nAre there more girl names or boy names in 1995? Use the grouped dataframe names_95_bysex with the .count() aggregation to find the total number of names for each sex. Store the female and male counts as girl_name_count and boy_name_count, respectfully.\n\n\ngirl_name_count = ...\nboy_name_count = ...\n\n#print\nprint(girl_name_count)\nprint(boy_name_count)\n\n\ngrader.check(\"q10\")\n\n\n\nChaining operations\nYou have already seen examples of this, but pandas and numpy operations can be chained together in sequence. For example, names_95.Count.max() is a chain with two steps: first select the Count column (.count); then compute the maximum (.max()).\nGrouped summaries are often convenient to compute in a chained fashion, rather than by assigning the grouped dataframe a new name and performing operations on the resulting object. For example, finding the total number of boys and girls recorded in the 1995 data can be done with the following chain:\n\n# repeating previous calculation, more streamlined\nnames_95.groupby('Sex').Count.sum()\n\nWe can take this even one step further and also perform the filtering in sequence as part of the chain:\n\n# longer chain\nbaby_names[baby_names.Year == 1995].groupby('Sex').Count.sum()\n\nChains can get somewhat long, but they have the advantage of making codes more efficient, and often more readable. We did above in one step what took several lines before. Further, this chain can almost be read aloud:\n“Take baby names, filter on year, then group by sex, then select name counts, then compute the sum.”\nLet’s now consider computing the average counts of boy and girl names for each year 1990-1995. This can be accomplished by the following chain (notice it is possible to group by multiple variables).\n\n# average counts by sex and year\nbaby_names[baby_names.Year &lt;= 1995].groupby(['Year', 'Sex']).mean(numeric_only = True)\n\nThis display is not ideal. We can ‘pivot’ the table into a wide format by adding a few extra steps in the chain: change the indices to columns; then define a new shape by specifying which column should be the new row index, which should be the new column index, and which values should populate the table.\n\n# average counts by sex and year\nbaby_names[baby_names.Year &lt;= 1995].groupby(\n    ['Year', 'Sex']\n    ).mean(\n    numeric_only = True\n    ).reset_index().pivot(\n    index = 'Sex', columns = 'Year', values = 'Count'\n    )\n\nStyle comment: break long chains over multiple lines with indentation. The above chain is too long to be readable in one line. To balance the readability of codes with the efficiency of chaining, it is good practice to break long chains over several lines, with appropriate indentations.\nHere are some rules of thumb on style.\n\nSeparate comparisons by spaces (a&lt;b as a &lt; b)\nSplit chains longer than 30-40 characters over multiple lines\nSplit lines between delimiters (, )\nIncrease indentation for lines between delimiters\nFor chained operations, try to get each step in the chain shown on a separate line\nFor functions with multiple arguments, split lines so that each argument is on its own line\n\n\n\nQuestion 11\nWrite a chain with appropriate style to display the (first) most common boy and girl names in each of the years 2005-2015. Do this in two steps:\n\nFirst filter baby_names by year, then group by year and sex, and then find the indices of first occurence of the largest counts. Store these indices as ind.\nThen use .loc[] with your stored indices to slice baby_names so as to retrieve the rows corresponding to each most frequent name each year and for each sex; then pivot this table so that the columns are years, the rows are sexes, and the entries are names. Store this as pivot_names.\n\n\n\n...\nprint(ind)\npivot_names\n\n\ngrader.check(\"q11\")\n\n\nTo double-check your work, the cell below will rerun all of the autograder tests.\n\ngrader.check_all()"
  },
  {
    "objectID": "labs/lab5-pca/lab5-pca-soln.html",
    "href": "labs/lab5-pca/lab5-pca-soln.html",
    "title": "Lab 5: Principal components",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab5-pca.ipynb\")\nimport numpy as np\nimport pandas as pd\nimport altair as alt\nfrom scipy import linalg\nfrom statsmodels.multivariate.pca import PCA\n# disable row limit for plotting\nalt.data_transformers.disable_max_rows()\n# uncomment to ensure graphics display with pdf export\n# alt.renderers.enable('mimetype')\n\nDataTransformerRegistry.enable('default')\nPrincipal components analysis (PCA) is a widely-used multivariate analysis technique. Depending on the application, PCA is variously described as:\nThe core technique of PCA is finding linear data transformations that preserve variance.\nWhat does it mean to say that ‘principal components are linear data transformations’? Suppose you have a dataset with \\(n\\) observations and \\(p\\) variables. We can represent the values as a data matrix \\(\\mathbf{X}\\) with \\(n\\) rows and \\(p\\) columns:\n\\[\n\\mathbf{X}\n= \\underbrace{\\left[\\begin{array}{cccc}\n    \\mathbf{x}_1 &\\mathbf{x}_2 &\\cdots &\\mathbf{x}_p\n    \\end{array}\\right]}_{\\text{column vectors}}\n= \\left[\\begin{array}{cccc}\n    x_{11} &x_{12} &\\cdots &x_{1p} \\\\\n    x_{21} &x_{22} &\\cdots &x_{2p} \\\\\n    \\vdots &\\vdots &\\ddots &\\vdots \\\\\n    x_{n1} &x_{n2} &\\cdots &x_{np}\n\\end{array}\\right]\n\\]\nTo say that the principal components are linear data transformations means that each principal component is of the form:\n\\[\n\\text{PC} = \\mathbf{Xv} = v_1 \\mathbf{x}_1 + v_2 \\mathbf{x}_2 + \\cdots + v_p \\mathbf{x}_p\n\\]\nfor some vector \\(\\mathbf{v}\\). In PCA, the following terminology is used:\nAs discussed in lecture, the values of the loadings are found by decomposing the correlation structure.\nObjectives\nIn this lab, you’ll focus on computing and interpreting principal components:\nYou’ll work with a selection of county summaries from the 2010 U.S. census. The first few rows of the dataset are shown below:\n# import tidy county-level 2010 census data\ncensus = pd.read_csv('data/census2010.csv', encoding = 'latin1')\ncensus.head()\n\n\n\n\n\n\n\n\nState\nCounty\nWomen\nWhite\nCitizen\nIncomePerCap\nPoverty\nChildPoverty\nProfessional\nService\n...\nTransit\nOtherTransp\nWorkAtHome\nMeanCommute\nEmployed\nPrivateWork\nSelfEmployed\nFamilyWork\nUnemployment\nMinority\n\n\n\n\n0\nAlabama\nAutauga\n51.567339\n75.788227\n73.749117\n24974.49970\n12.912305\n18.707580\n32.790974\n17.170444\n...\n0.095259\n1.305969\n1.835653\n26.500165\n43.436374\n73.736490\n5.433254\n0.000000\n7.733726\n22.536870\n\n\n1\nAlabama\nBaldwin\n51.151337\n83.102616\n75.694057\n27316.83516\n13.424230\n19.484305\n32.729943\n17.950921\n...\n0.126621\n1.443800\n3.850477\n26.322179\n44.051127\n81.282655\n5.909353\n0.363327\n7.589820\n15.214260\n\n\n2\nAlabama\nBarbour\n46.171840\n46.231594\n76.912223\n16824.21643\n26.505629\n43.559621\n26.124042\n16.463434\n...\n0.495403\n1.621725\n1.501946\n24.518283\n31.921135\n71.594256\n7.149837\n0.089774\n17.525557\n51.943818\n\n\n3\nAlabama\nBibb\n46.589099\n74.499889\n77.397806\n18430.99031\n16.603747\n27.197085\n21.590099\n17.955450\n...\n0.503137\n1.562095\n0.731468\n28.714391\n36.692621\n76.743846\n6.637936\n0.394151\n8.163104\n24.165971\n\n\n4\nAlabama\nBlount\n50.594351\n87.853854\n73.375498\n20532.27467\n16.721518\n26.857377\n28.529302\n13.942519\n...\n0.362632\n0.419941\n2.265413\n34.844893\n38.449142\n81.826708\n4.228716\n0.356493\n7.699640\n10.594744\n\n\n\n\n5 rows × 24 columns\nThe observational units are U.S. counties, and each row is an observation on one county. The values are, for the most part, percentages of the county population. You can find variable descriptions in the metadata file census2010metadata.csv in the data directory."
  },
  {
    "objectID": "labs/lab5-pca/lab5-pca-soln.html#loadings-and-scores",
    "href": "labs/lab5-pca/lab5-pca-soln.html#loadings-and-scores",
    "title": "Lab 5: Principal components",
    "section": "Loadings and scores",
    "text": "Loadings and scores\nIn statsmodels, the module multivariate.pca contains an easy-to-use implementation.\n\n# compute principal components\npca = PCA(data = x_mx, standardize = True) \n\nMost quantities you might want to use in PCA can be retrieved as attributes of pca. In particular:\n\n.loadings contains the loadings\n.scores contains the scores\n.eigenvals contains the variances along each principal axis (see lecture notes)\n\nExamine the loadings below. Each column gives the loadings for one principal component; components are ordered from largest to smallest variance.\n\n# inspect loadings\npca.loadings\n\n\n\n\n\n\n\n\ncomp_00\ncomp_01\ncomp_02\ncomp_03\ncomp_04\ncomp_05\ncomp_06\ncomp_07\ncomp_08\ncomp_09\n...\ncomp_12\ncomp_13\ncomp_14\ncomp_15\ncomp_16\ncomp_17\ncomp_18\ncomp_19\ncomp_20\ncomp_21\n\n\n\n\nWomen\n-0.020055\n0.139958\n0.187600\n-0.176614\n-0.310716\n-0.274826\n-0.537551\n0.235286\n-0.408399\n0.229221\n...\n-0.157758\n0.042221\n-0.113943\n0.190960\n-0.148377\n0.005946\n-0.050017\n-0.047263\n-0.028714\n0.000258\n\n\nWhite\n0.289614\n0.196549\n-0.288902\n-0.078059\n0.242441\n-0.061416\n-0.117174\n0.053329\n-0.054659\n0.022884\n...\n0.027054\n0.393618\n-0.035014\n-0.049062\n0.148105\n0.053617\n-0.007430\n-0.020166\n0.029998\n-0.709644\n\n\nCitizen\n0.050698\n0.064994\n-0.281904\n-0.467986\n0.404244\n-0.025597\n-0.155921\n-0.014819\n-0.027981\n0.098203\n...\n-0.021193\n-0.619243\n0.116601\n0.067243\n-0.179710\n-0.100343\n-0.082991\n0.096689\n0.009372\n0.004363\n\n\nIncomePerCap\n0.334863\n0.020432\n0.284074\n-0.022197\n0.040680\n-0.030926\n0.068452\n-0.016533\n-0.016239\n0.028275\n...\n0.050940\n-0.266482\n-0.070710\n-0.294452\n0.047195\n0.713775\n-0.165984\n-0.185190\n0.195542\n-0.008501\n\n\nPoverty\n-0.365212\n-0.120172\n-0.040170\n-0.128231\n-0.076355\n-0.073253\n-0.136918\n-0.090574\n-0.060953\n0.094904\n...\n0.131664\n0.095263\n0.319450\n-0.047637\n0.227491\n-0.018016\n0.011492\n-0.276425\n0.693615\n0.011429\n\n\nChildPoverty\n-0.364836\n-0.081086\n-0.077433\n-0.098585\n-0.074420\n-0.101475\n-0.128282\n-0.087317\n-0.067509\n0.098937\n...\n0.105867\n0.098662\n0.393329\n-0.163165\n0.202835\n0.405313\n-0.099722\n0.249526\n-0.532390\n-0.007282\n\n\nProfessional\n0.240139\n-0.175611\n0.287636\n-0.258789\n-0.094541\n-0.004750\n0.082678\n-0.071358\n0.087102\n0.223003\n...\n0.204392\n0.056316\n0.102994\n-0.045454\n-0.053741\n-0.107540\n0.608185\n-0.051955\n-0.148216\n-0.007049\n\n\nService\n-0.203254\n-0.139714\n0.005957\n-0.122145\n0.377802\n0.257543\n0.157723\n0.090144\n-0.585981\n-0.153836\n...\n0.026180\n0.029447\n0.005202\n-0.064617\n0.082027\n0.069713\n0.353849\n-0.079919\n-0.030328\n-0.006351\n\n\nOffice\n-0.052168\n0.189803\n0.281398\n-0.267195\n-0.006059\n0.155655\n-0.150723\n0.492651\n0.223961\n-0.270637\n...\n-0.050948\n0.013481\n-0.052531\n0.079232\n0.117819\n0.127661\n0.229185\n-0.068073\n-0.027766\n-0.006161\n\n\nProduction\n-0.094307\n0.282329\n-0.285500\n0.355106\n-0.051805\n-0.168182\n-0.231386\n-0.299673\n0.060732\n0.021055\n...\n0.029883\n-0.264432\n-0.163273\n0.142330\n0.094739\n0.232051\n0.561801\n-0.105679\n-0.010326\n-0.004855\n\n\nDrive\n-0.102197\n0.406130\n-0.099229\n-0.261077\n-0.288984\n0.168809\n0.216910\n-0.042703\n0.025609\n-0.011982\n...\n-0.158782\n0.048911\n-0.029596\n-0.165462\n-0.040402\n0.103964\n0.156394\n0.603356\n0.305441\n0.008364\n\n\nCarpool\n-0.079129\n-0.063744\n-0.095696\n0.457962\n0.110105\n-0.235710\n0.182826\n0.641950\n-0.145211\n0.136891\n...\n0.037121\n-0.141038\n0.054541\n-0.065395\n-0.071457\n0.044920\n0.106103\n0.235080\n0.122904\n-0.001884\n\n\nTransit\n0.030233\n-0.101142\n0.390869\n0.052245\n0.281641\n-0.377725\n-0.019479\n-0.374445\n-0.248253\n-0.163689\n...\n-0.243862\n0.124949\n-0.119416\n0.008536\n-0.104085\n-0.005321\n0.045499\n0.341581\n0.129557\n0.001697\n\n\nOtherTransp\n-0.021871\n-0.209403\n0.139315\n0.221098\n0.318697\n0.279204\n-0.549761\n0.034251\n0.372329\n0.034315\n...\n-0.181742\n0.068611\n0.168988\n-0.132962\n-0.067493\n0.025105\n0.098007\n0.248707\n0.121794\n-0.001091\n\n\nWorkAtHome\n0.218353\n-0.331636\n-0.116068\n-0.113166\n-0.067242\n-0.168720\n-0.031647\n0.055563\n0.071404\n0.055436\n...\n0.556381\n0.076534\n0.002744\n0.289911\n-0.127684\n0.140831\n0.037033\n0.336543\n0.168037\n0.003815\n\n\nMeanCommute\n-0.097003\n0.176739\n0.135322\n-0.144408\n0.232196\n-0.590953\n0.249926\n0.093081\n0.346862\n-0.003542\n...\n-0.147735\n0.020225\n0.240347\n0.064462\n0.182459\n-0.072583\n0.061522\n-0.084455\n-0.032194\n0.002739\n\n\nEmployed\n0.345588\n0.054653\n0.157726\n0.128709\n-0.115729\n0.060930\n-0.102152\n-0.008288\n-0.190448\n0.015789\n...\n0.055605\n-0.285761\n0.169775\n-0.031261\n0.706298\n-0.305263\n-0.041239\n0.230681\n-0.001998\n0.002525\n\n\nPrivateWork\n0.035539\n0.441922\n0.158709\n0.146725\n0.033461\n-0.060590\n-0.127304\n-0.037900\n-0.101356\n-0.151123\n...\n0.450448\n0.006198\n0.291232\n-0.439397\n-0.371223\n-0.231749\n-0.013572\n-0.061566\n-0.028325\n0.012217\n\n\nSelfEmployed\n0.155300\n-0.316174\n-0.266798\n-0.104453\n-0.200415\n-0.181008\n-0.026232\n0.068297\n0.013185\n0.177112\n...\n-0.329360\n-0.016310\n-0.052208\n-0.608598\n-0.057154\n-0.122195\n0.161609\n-0.079637\n-0.014284\n0.008517\n\n\nFamilyWork\n0.085077\n-0.221137\n-0.203301\n-0.064817\n-0.213611\n-0.203811\n-0.149690\n0.048505\n-0.011095\n-0.815791\n...\n-0.006316\n-0.080816\n0.069119\n-0.052283\n0.002424\n0.012159\n0.045391\n0.005322\n-0.000437\n0.000525\n\n\nUnemployment\n-0.333420\n-0.043047\n0.069938\n-0.125235\n0.125138\n-0.132276\n-0.123131\n0.026469\n0.137090\n0.013801\n...\n0.362180\n-0.046872\n-0.665370\n-0.319148\n0.244552\n-0.142297\n-0.084088\n0.088856\n-0.035251\n-0.002350\n\n\nMinority\n-0.292461\n-0.191628\n0.282231\n0.074901\n-0.250838\n0.054750\n0.116988\n-0.058902\n0.055219\n-0.022568\n...\n-0.026109\n-0.395337\n0.045660\n0.039958\n-0.157149\n-0.074605\n-0.001124\n0.025618\n-0.010058\n-0.704021\n\n\n\n\n22 rows × 22 columns\n\n\n\nSimilarly, inspect the scores below and check your understanding; each row is an observation and the columns give the scores on each principal axis.\n\n# inspect scores\npca.scores\n\n\n\n\n\n\n\n\ncomp_00\ncomp_01\ncomp_02\ncomp_03\ncomp_04\ncomp_05\ncomp_06\ncomp_07\ncomp_08\ncomp_09\n...\ncomp_12\ncomp_13\ncomp_14\ncomp_15\ncomp_16\ncomp_17\ncomp_18\ncomp_19\ncomp_20\ncomp_21\n\n\n\n\n0\n0.000504\n0.015907\n0.008343\n-0.006795\n-0.007242\n0.005351\n0.003190\n0.001652\n0.008156\n0.006529\n...\n-0.019774\n0.000901\n-0.011845\n0.014416\n0.004541\n0.002121\n0.016706\n0.000877\n-0.011272\n-0.004851\n\n\n1\n0.005153\n0.013795\n0.011124\n-0.015817\n-0.000635\n0.004615\n-0.001805\n0.017153\n0.007027\n-0.013943\n...\n0.002524\n0.005413\n0.005880\n-0.005797\n-0.006470\n0.002276\n-0.003666\n-0.001323\n-0.005838\n-0.002150\n\n\n2\n-0.029425\n0.000688\n-0.007837\n0.002652\n0.002070\n-0.000715\n0.007475\n-0.014511\n0.033923\n0.001547\n...\n0.009447\n-0.025136\n-0.021956\n-0.024289\n0.007440\n0.009195\n0.009334\n0.021504\n-0.029051\n0.001082\n\n\n3\n-0.011412\n0.010430\n-0.021061\n0.020958\n0.015194\n-0.009843\n0.017917\n-0.014046\n0.011570\n-0.008471\n...\n-0.018034\n-0.017263\n0.010274\n-0.009092\n-0.015237\n-0.009855\n-0.008816\n0.017472\n-0.007882\n0.001266\n\n\n4\n-0.004669\n0.023879\n-0.002247\n0.001292\n-0.001606\n-0.026357\n0.006381\n0.006578\n0.021996\n-0.008525\n...\n-0.003187\n0.017533\n0.016274\n0.015243\n-0.003568\n-0.003515\n0.000466\n-0.010558\n-0.023239\n0.001167\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3213\n0.008225\n0.010000\n0.007505\n0.035786\n0.003588\n0.002305\n0.019690\n0.006378\n-0.005997\n-0.000512\n...\n-0.001672\n-0.008155\n0.002449\n-0.003349\n0.022558\n0.013136\n-0.057807\n0.016814\n0.018129\n-0.002430\n\n\n3214\n0.031668\n-0.017726\n0.037763\n0.014502\n0.037688\n0.031695\n-0.006695\n-0.025090\n-0.040650\n-0.003375\n...\n0.005698\n-0.016727\n0.018871\n-0.043735\n0.025132\n0.014618\n-0.013787\n-0.005548\n0.025438\n-0.021207\n\n\n3215\n0.007884\n0.003308\n0.003249\n0.024815\n0.004831\n0.005043\n0.010563\n-0.004429\n-0.015736\n0.006288\n...\n-0.006612\n0.021469\n0.005643\n0.005678\n0.011311\n0.000395\n-0.014083\n0.010174\n0.008824\n0.005149\n\n\n3216\n0.008614\n-0.009843\n-0.003693\n0.016797\n-0.003189\n0.019038\n-0.011354\n0.006703\n-0.016839\n0.001212\n...\n-0.001816\n0.002220\n-0.013211\n-0.001798\n-0.005951\n0.010336\n-0.042785\n0.004762\n0.010071\n0.001051\n\n\n3217\n0.016117\n-0.013507\n-0.003956\n0.015611\n0.018331\n-0.055831\n0.001351\n-0.043390\n-0.003790\n-0.025482\n...\n-0.018354\n-0.009545\n0.014021\n0.007075\n0.002762\n0.016967\n-0.043718\n0.041858\n-0.002844\n0.014264\n\n\n\n\n3218 rows × 22 columns\n\n\n\nImportantly, statsmodels rescales the scores so that they have unit inner product; in other words, so that the variances are all \\(\\frac{1}{n - 1}\\).\n\n# variance of scores\npca.scores.var()\n\ncomp_00    0.000311\ncomp_01    0.000311\ncomp_02    0.000311\ncomp_03    0.000311\ncomp_04    0.000311\ncomp_05    0.000311\ncomp_06    0.000311\ncomp_07    0.000311\ncomp_08    0.000311\ncomp_09    0.000311\ncomp_10    0.000311\ncomp_11    0.000311\ncomp_12    0.000311\ncomp_13    0.000311\ncomp_14    0.000311\ncomp_15    0.000311\ncomp_16    0.000311\ncomp_17    0.000311\ncomp_18    0.000311\ncomp_19    0.000311\ncomp_20    0.000311\ncomp_21    0.000311\ndtype: float64\n\n\n\n# for comparison\n1/(x_mx.shape[0] - 1)\n\n0.00031084861672365556\n\n\nTo change this behavior, set normalize = False when computing the principal components.\n\nQuestion 3\nCheck your understanding. Which variable contributes most to the sixth principal component? Store the variable name exactly as it appears among the original column names as pc6_most_influential_variable, and store the corresponding loading as pc6_most_influential_variable_loading. Print the variable name.\n\n# find most influential variable\npc6_most_influential_variable = pca.loadings.iloc[:, 5].abs().idxmax() # SOLUTION\n\n# find loading\npc6_most_influential_variable_loading = pca.loadings.loc[pc6_most_influential_variable, 'comp_05'] # SOLUTION\n\n# print\nprint(pc6_most_influential_variable) # SOLUTION\n\nMeanCommute\n\n\n\ngrader.check(\"q3\")"
  },
  {
    "objectID": "labs/lab5-pca/lab5-pca-soln.html#variance-ratios",
    "href": "labs/lab5-pca/lab5-pca-soln.html#variance-ratios",
    "title": "Lab 5: Principal components",
    "section": "Variance ratios",
    "text": "Variance ratios\nThe variance ratios indicate the proportions of total variance in the data captured by each principal axis. You may recall from lecture that the variance ratios are computed from the eigenvalues of the correlation (or covariance, if data are not standardized) matrix.\nWhen using statsmodels, these need to be computed manually.\n\n# compute variance ratios\nvar_ratios = pca.eigenvals/pca.eigenvals.sum()\n\n# print\nvar_ratios\n\n0     0.262856\n1     0.151574\n2     0.114128\n3     0.076665\n4     0.054345\n5     0.051541\n6     0.047318\n7     0.040208\n8     0.036687\n9     0.033641\n10    0.026326\n11    0.022018\n12    0.017596\n13    0.016841\n14    0.014282\n15    0.010239\n16    0.007834\n17    0.006307\n18    0.004719\n19    0.002662\n20    0.002101\n21    0.000112\nName: eigenvals, dtype: float64\n\n\nNote again that the principal components have been computed in order of decreasing variance.\n\n\nQuestion 4\nCheck your understanding. What proportion of variance is captured jointly by the first three components taken together? Provide a calculation to justify your answer.\nType your answer here, replacing this text.\n\nvar_ratios[0:3].sum() # SOLUTION\n\n0.5285591588625773\n\n\nSOLUTION About 53%."
  },
  {
    "objectID": "labs/lab5-pca/lab5-pca-soln.html#selecting-a-subset-of-pcs",
    "href": "labs/lab5-pca/lab5-pca-soln.html#selecting-a-subset-of-pcs",
    "title": "Lab 5: Principal components",
    "section": "Selecting a subset of PCs",
    "text": "Selecting a subset of PCs\nPCA generally consists of choosing a small subset of components. The basic strategy for selecting this subset is to determine how many are needed to capture some analyst-chosen minimum portion of total variance in the original data.\nMost often this assessment is made graphically by inspecting the variance ratios and their cumulative sum, i.e., the amount of total variation captured jointly by subsets of successive components. We’ll store these quantities in a data frame.\n\n# store proportion of variance explained as a dataframe\npca_var_explained = pd.DataFrame({\n    'Component': np.arange(1, 23),\n    'Proportion of variance explained': var_ratios})\n\n# add cumulative sum\npca_var_explained['Cumulative variance explained'] = var_ratios.cumsum()\n\n# print\npca_var_explained.head()\n\n\n\n\n\n\n\n\nComponent\nProportion of variance explained\nCumulative variance explained\n\n\n\n\n0\n1\n0.262856\n0.262856\n\n\n1\n2\n0.151574\n0.414431\n\n\n2\n3\n0.114128\n0.528559\n\n\n3\n4\n0.076665\n0.605224\n\n\n4\n5\n0.054345\n0.659569\n\n\n\n\n\n\n\nNow we’ll make a dual-axis plot showing, on one side, the proportion of variance explained (y) as a function of component (x), and on the other side, the cumulative variance explained (y) also as a function of component (x). Make sure that you’ve completed Q1(a) before running the next cell.\n\n# encode component axis only as base layer\nbase = alt.Chart(pca_var_explained).encode(\n    x = 'Component')\n\n# make a base layer for the proportion of variance explained\nprop_var_base = base.encode(\n    y = alt.Y('Proportion of variance explained',\n              axis = alt.Axis(titleColor = '#57A44C'))\n)\n\n# make a base layer for the cumulative variance explained\ncum_var_base = base.encode(\n    y = alt.Y('Cumulative variance explained', axis = alt.Axis(titleColor = '#5276A7'))\n)\n\n# add points and lines to each base layer\nprop_var = prop_var_base.mark_line(stroke = '#57A44C') + prop_var_base.mark_point(color = '#57A44C')\ncum_var = cum_var_base.mark_line() + cum_var_base.mark_point()\n\n# layer the layers\nvar_explained_plot = alt.layer(prop_var, cum_var).resolve_scale(y = 'independent')\n\n# display\nvar_explained_plot\n\n\n\n\n\n\nThe purpose of making this plot is to quickly determine the fewest number of principal components that capture a considerable portion of variation and covariation. ‘Considerable’ here is a bit subjective.\n\nQuestion 5\nHow many principal components explain more than 6% of total variation individually? Store this number as num_pc, and store the proportion of variation that they capture jointly as var_explained.\n\n# number of selected components\nnum_pc = 4 #SOLUTION\n\n# variance explained\nvar_explained = var_ratios[0:num_pc].sum() # SOLUTION\n\n#print\nprint('number selected: ', num_pc)\nprint('proportion of variance captured: ', var_explained)\n\nnumber selected:  4\nproportion of variance captured:  0.6052243442775455\n\n\n\ngrader.check(\"q5\")"
  },
  {
    "objectID": "labs/lab5-pca/lab5-pca-soln.html#interpreting-loadings",
    "href": "labs/lab5-pca/lab5-pca-soln.html#interpreting-loadings",
    "title": "Lab 5: Principal components",
    "section": "Interpreting loadings",
    "text": "Interpreting loadings\nNow that you’ve chosen the number of components to work with, the next step is to examine loadings to understand just which variables the components combine with significant weight.\nWe’ll store the scores for the components you selected as a dataframe.\n\n# subset loadings\nloading_df = pca.loadings.iloc[:, 0:num_pc]\n\n# rename columns\nloading_df = loading_df.rename(columns = dict(zip(loading_df.columns, ['PC' + str(i) for i in range(1, num_pc + 1)])))\n\n# print\nloading_df.head()\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\n\n\n\n\nWomen\n-0.020055\n0.139958\n0.187600\n-0.176614\n\n\nWhite\n0.289614\n0.196549\n-0.288902\n-0.078059\n\n\nCitizen\n0.050698\n0.064994\n-0.281904\n-0.467986\n\n\nIncomePerCap\n0.334863\n0.020432\n0.284074\n-0.022197\n\n\nPoverty\n-0.365212\n-0.120172\n-0.040170\n-0.128231\n\n\n\n\n\n\n\nAgain, the loadings are the weights with which the variables are combined to form the principal components. For example, the PC1 column tells us that this component is equal to:\n\\[(-0.020055\\times\\text{women}) + (0.289614\\times\\text{white}) + (0.050698\\times\\text{citizen}) + \\dots\\]\nSince the components together capture over half the total variation, the heavily weighted variables in the selected components are the ones that drive variation in the original data.\nBy visualizing the loadings, we can see which variables are most influential for each component, and thereby also which variables seem to drive total variation in the data.\n\n# melt from wide to long\nloading_plot_df = loading_df.reset_index().melt(\n    id_vars = 'index',\n    var_name = 'Principal Component',\n    value_name = 'Loading'\n).rename(columns = {'index': 'Variable'})\n\n# add a column of zeros to encode for x = 0 line to plot\nloading_plot_df['zero'] = np.repeat(0, len(loading_plot_df))\n\n# create base layer\nbase = alt.Chart(loading_plot_df)\n\n# create lines + points for loadings\nloadings = base.mark_line(point = True).encode(\n    y = alt.X('Variable', title = ''),\n    x = 'Loading',\n    color = 'Principal Component'\n)\n\n# create line at zero\nrule = base.mark_rule().encode(x = alt.X('zero', title = 'Loading'), size = alt.value(0.05))\n\n# layer\nloading_plot = (loadings + rule).properties(width = 120)\n\n# show\nloading_plot.facet(column = alt.Column('Principal Component', title = ''))\n\n\n\n\n\n\nLook first at PC1: the variables with the largest loadings (points farthest in either direction from the zero line) are Child Poverty (positive), Employed (negative), Income per capita (negative), Poverty (positive), and Unemployment (positive). We know from exploring the correlation matrix that employment rate, unemployment rate, and income per capita are all related, and similarly child poverty rate and poverty rate are related. Therefore, the positively-loaded variables are all measuring more or less the same thing, and likewise for the negatively-loaded variables.\nEssentially, then, PC1 is predominantly (but not entirely) a representation of income and poverty. In particular, counties have a higher value for PC1 if they have lower-than-average income per capita and higher-than-average poverty rates, and a smaller value for PC1 if they have higher-than-average income per capita and lower-than-average poverty rates.\n\nA system for loading interpretation\nOften interpreting principal components can be difficult, and sometimes there’s no clear interpretation available! That said, it helps to have a system instead of staring at the plot and scratching our heads. Here is a semi-systematic approach to interpreting loadings:\n\nDivert your attention away from the zero line.\nFind the largest positive loading, and list all variables with similar loadings.\nFind the largest negative loading, and list all variables with similar loadings.\nThe principal component represents the difference between the average of the first set and the average of the second set.\nTry to come up with a description of less than 4 words.\n\nThis system is based on the following ideas: * a high loading value (negative or positive) indicates that a variable strongly influences the principal component; * a negative loading value indicates that + increases in the value of a variable decrease the value of the principal component + and decreases in the value of a variable increase the value of the principal component; * a positive loading value indicates that + increases in the value of a variable increase the value of the principal component + and decreases in the value of a variable decrease the value of the principal component; * similar loadings between two or more variables indicate that the principal component reflects their average; * divergent loadings between two sets of variables indicates that the principal component reflects their difference.\n\n\n\nQuestion 6\nWork with your neighbor to interpret PC2. Come up with a name for the component and explain which variables are most influential.\nType your answer here, replacing this text.\nSOLUTION PC2 seems to measure employment type; high values come from high self employment rate combined with high work-at-home rate and low private sector workers."
  },
  {
    "objectID": "labs/lab5-pca/lab5-pca-soln.html#standardization",
    "href": "labs/lab5-pca/lab5-pca-soln.html#standardization",
    "title": "Lab 5: Principal components",
    "section": "Standardization",
    "text": "Standardization\nData are typically standardized because otherwise the variables on the largest scales tend to dominate the principal components, and most of the time PC1 will capture the majority of the variation. However, that is artificial. In the census data, income per capita has the largest magnitudes, and thus, the highest variance.\n\n# three largest variances\nx_mx.var().sort_values(ascending = False).head(3)\n\nIncomePerCap    3.804072e+07\nMinority        5.265263e+02\nWhite           5.264985e+02\ndtype: float64\n\n\nWhen PCs are computed without normalization, the total variation is mostly just the variance of income per capita because it is orders of magnitude larger than the variance of any other variable. But that’s just because of the scale of the variable – incomes per capita are large numbers – not a reflection that it varies more or less than the other variables.\nRun the cell below to see what happens to the variance ratios if the data are not normalized.\n\n# recompute pcs without normalization\npca_unscaled = PCA(data = x_mx, standardize = False)\n\n# show variance ratios for first three pcs\npca_unscaled.eigenvals[0:3]/pca_unscaled.eigenvals.sum()\n\n0    0.999965\n1    0.000025\n2    0.000003\nName: eigenvals, dtype: float64\n\n\nFurther, let’s look at the loadings when data are not standardized:\n\n# subset loadings\nunscaled_loading_df = pca_unscaled.loadings.iloc[:, 0:2]\n\n# rename columns\nunscaled_loading_df = unscaled_loading_df.rename(\n    columns = dict(zip(unscaled_loading_df.columns, ['PC' + str(i) for i in range(1, 3)]))\n)\n\n# melt from wide to long\nunscaled_loading_plot_df = unscaled_loading_df.reset_index().melt(\n    id_vars = 'index',\n    var_name = 'Principal Component',\n    value_name = 'Loading'\n).rename(\n    columns = {'index': 'Variable'}\n)\n\n# add a column of zeros to encode for x = 0 line to plot\nunscaled_loading_plot_df['zero'] = np.repeat(0, len(unscaled_loading_plot_df))\n\n# create base layer\nbase = alt.Chart(unscaled_loading_plot_df)\n\n# create lines + points for loadings\nloadings = base.mark_line(point = True).encode(\n    y = alt.X('Variable', title = ''),\n    x = 'Loading',\n    color = 'Principal Component'\n)\n\n# create line at zero\nrule = base.mark_rule().encode(x = alt.X('zero', title = 'Loading'), size = alt.value(0.05))\n\n# layer\nloading_plot = (loadings + rule).properties(width = 120, title = 'Loadings from unscaled PCA')\n\n# show\nloading_plot.facet(column = alt.Column('Principal Component', title = ''))\n\n\n\n\n\n\nNotice that the variables with nonzero loadings in unscaled PCA are simply the three variables with the largest variances.\n\n# three largest variances\nx_mx.var().sort_values(ascending = False).head(3)\n\nIncomePerCap    3.804072e+07\nMinority        5.265263e+02\nWhite           5.264985e+02\ndtype: float64"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing-soln.html",
    "href": "labs/lab4-smoothing/lab4-smoothing-soln.html",
    "title": "Lab 4: Smoothing",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab4-smoothing.ipynb\")\nimport numpy as np\nimport pandas as pd\nimport altair as alt\npd.options.mode.chained_assignment = None  # default='warn'\n# disable row limit for plotting\nalt.data_transformers.disable_max_rows()\n# uncomment to ensure graphics display with pdf export\n# alt.renderers.enable('mimetype')\n\nDataTransformerRegistry.enable('default')\nSo far, you’ve encountered a number of visualization techniques for displaying tidy data. In those visualizations, all graphic elements represent the values of a dataset – they are visual displays of actual data.\nIn general, smoothing means evening out. Visualizations of actual data are often irregular – points are distributed widely in scatterplots, line plots are jagged, bars are discontinuous. When we look at such visuals, we tend to attempt to look past these irregularities in order to discern patterns – for example, the overall shape of a histogram or the general trend in a scatterplot. Showing what a graphic might look like with irregularities evened out often aids the eye in detecting pattern. This is what smoothing is: evening out irregularities in graphical displays of actual data.\nFor our purposes, usually smoothing will consist in drawing a line or a curve on top of an existing statistical graphic. From a technical point of view, this amounts to adding derived geometric objects to a graphic that have fewer irregularities than the displays of actual data.\nIn this lab, you’ll learn some basic smoothing techniques – kernel density estimation, LOESS, and linear smoothing via regression – and how to implement them in Altair.\nIn Altair, smoothing is implemented via what Altair describes as transforms – operations that modify a dataset. Try not to get too attached to this terminology – ‘transform’ and ‘transformation’ are used to mean a variety of things in other contexts. You’ll begin with a brief introduction to Altair transforms before turning to smoothing techniques.\nThe sections of the lab are divided as follows:\nAnd our main goals are:\nYou’ll use the same data as last week to stick to a familiar example:\n# import tidied lab 3 data\ndata = pd.read_csv('data/lab3-data.csv')\ndata.head()\n\n\n\n\n\n\n\n\nCountry Name\nYear\nLife Expectancy\nMale Life Expectancy\nFemale Life Expectancy\nGDP per capita\nregion\nsub-region\nPopulation\n\n\n\n\n0\nAfghanistan\n2019\n63.2\n63.3\n63.2\n507.103432\nAsia\nSouthern Asia\n38041754.0\n\n\n1\nAfghanistan\n2015\n61.7\n61.0\n62.3\n578.466353\nAsia\nSouthern Asia\n34413603.0\n\n\n2\nAfghanistan\n2010\n59.9\n59.6\n60.3\n543.303042\nAsia\nSouthern Asia\n29185507.0\n\n\n3\nAlbania\n2019\n78.0\n76.3\n79.9\n5353.244856\nEurope\nSouthern Europe\n2854191.0\n\n\n4\nAlbania\n2015\n77.8\n76.1\n79.7\n3952.801215\nEurope\nSouthern Europe\n2880703.0"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing-soln.html#filter-transform",
    "href": "labs/lab4-smoothing/lab4-smoothing-soln.html#filter-transform",
    "title": "Lab 4: Smoothing",
    "section": "Filter transform",
    "text": "Filter transform\nLast week you saw a way to make histograms. As a quick refresher, to make a histogram of life expectancies across the globe in 2010, one can filter the data and then plot using the following commands:\n\n# filter\ndata2010 = data[data.Year == 2010]\n\n# plot\nalt.Chart(data2010).mark_bar().encode(\n    x = alt.X('Life Expectancy', \n              bin = alt.Bin(step = 2), \n              title = 'Life Expectancy at Birth'),\n    y = 'count()'\n)\n\n\n\n\n\n\nHowever, the filtering step can be handled within the plotting commands using .transform_filter().\nThis uses a helper command to specify the filtering condition – in the above example, the filtering condition is that Year is equal to 2010. A filtering condition is referred to in Altair as a ‘field predicate’. In the above example: * filtering field: Year * field predicate: equals 2010\nThere are different helpers for different types of field predicates – you can find a complete list in the documentation.\nHere is how to use .transform_filter() to make the same histogram shown above, but skipping the step of storing a subset of the data under a separate name:\n\n# filter and plot\nalt.Chart(data).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', \n                            equal = 2010)\n).mark_bar().encode(\n    x = alt.X('Life Expectancy', \n              bin = alt.Bin(step = 2), \n              title = 'Life Expectancy at Birth'),\n    y = 'count()'\n)\n\n\n\n\n\n\n\n\nQuestion 1: Filter transform\nConstruct a histogram of life expectancies across the globe in 2019 using a filter transform as shown above to filter the appropriate rows of the dataset. Use a bin size of three (not two) years.\n\n# filter and plot\nalt.Chart(data).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', equal = 2019) # SOLUTION\n).mark_bar().encode(\n    x = alt.X('Life Expectancy', bin = alt.Bin(step = 3), title = 'Life Expectancy at Birth'), # SOLUTION\n    y = 'count()' # SOLUTION\n)"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing-soln.html#bin-transform",
    "href": "labs/lab4-smoothing/lab4-smoothing-soln.html#bin-transform",
    "title": "Lab 4: Smoothing",
    "section": "Bin transform",
    "text": "Bin transform\nThe codes above provide a sleek way to construct the histogram that handles binning via arguments to alt.X(...). However, binning actually involves an operation: creating a new variable that is a discretization of an existing variable into contiguous intervals of a specified width.\nTo illustrate, have a look at how the histogram could be constructed ‘manually’ by the following operations. 1. Bin life expectancies 2. Count values in each bin 3. Make a bar plot of counts against bin centers.\nHere’s step 1:\n\n# bin life expectancies into 20 contiguous intervals\ndata2010['Bin'] = pd.cut(data2010[\"Life Expectancy\"], bins = 20)\ndata2010.head()\n\n\n\n\n\n\n\n\nCountry Name\nYear\nLife Expectancy\nMale Life Expectancy\nFemale Life Expectancy\nGDP per capita\nregion\nsub-region\nPopulation\nBin\n\n\n\n\n2\nAfghanistan\n2010\n59.9\n59.6\n60.3\n543.303042\nAsia\nSouthern Asia\n29185507.0\n(59.57, 62.14]\n\n\n5\nAlbania\n2010\n76.2\n74.2\n78.3\n4094.350334\nEurope\nSouthern Europe\n2913021.0\n(74.99, 77.56]\n\n\n9\nAlgeria\n2010\n75.9\n75.0\n76.8\n4479.341720\nAfrica\nNorthern Africa\n35977455.0\n(74.99, 77.56]\n\n\n13\nAngola\n2010\n58.1\n55.8\n60.5\n3587.883798\nAfrica\nSub-Saharan Africa\n23356246.0\n(57.0, 59.57]\n\n\n17\nAntigua and Barbuda\n2010\n75.9\n73.6\n78.2\n13049.257050\nAmericas\nLatin America and the Caribbean\n88028.0\n(74.99, 77.56]\n\n\n\n\n\n\n\nHere’s step 2:\n\n# count values in each bin and store midpoints\nhistdata = data2010.loc[:, ['Life Expectancy', 'Bin']].groupby('Bin').count()\nhistdata['Bin midpoint'] = histdata.index.values.categories.mid.values\nhistdata\n\n\n\n\n\n\n\n\nLife Expectancy\nBin midpoint\n\n\nBin\n\n\n\n\n\n\n(31.249, 33.87]\n1\n32.5595\n\n\n(33.87, 36.44]\n0\n35.1550\n\n\n(36.44, 39.01]\n0\n37.7250\n\n\n(39.01, 41.58]\n0\n40.2950\n\n\n(41.58, 44.15]\n0\n42.8650\n\n\n(44.15, 46.72]\n0\n45.4350\n\n\n(46.72, 49.29]\n3\n48.0050\n\n\n(49.29, 51.86]\n1\n50.5750\n\n\n(51.86, 54.43]\n1\n53.1450\n\n\n(54.43, 57.0]\n6\n55.7150\n\n\n(57.0, 59.57]\n11\n58.2850\n\n\n(59.57, 62.14]\n11\n60.8550\n\n\n(62.14, 64.71]\n8\n63.4250\n\n\n(64.71, 67.28]\n9\n65.9950\n\n\n(67.28, 69.85]\n12\n68.5650\n\n\n(69.85, 72.42]\n16\n71.1350\n\n\n(72.42, 74.99]\n24\n73.7050\n\n\n(74.99, 77.56]\n24\n76.2750\n\n\n(77.56, 80.13]\n10\n78.8450\n\n\n(80.13, 82.7]\n20\n81.4150\n\n\n\n\n\n\n\nAnd finally, step 3:\n\n# plot histogram\nalt.Chart(histdata).mark_bar(width = 10).encode(\n    x = 'Bin midpoint',\n    y = alt.Y('Life Expectancy', title = 'Count')\n)\n\n\n\n\n\n\nThese operations can be articulated as a transform in Altair using .bin_transform():\n\n# filter, bin, and plot\nalt.Chart(\n    data\n).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', \n                            equal = 2010)\n).transform_bin(\n    'Life Expectancy at Birth', # name to give binned variable\n    field = 'Life Expectancy', # variable to bin\n    bin = alt.Bin(step = 2) # binning parameters\n).mark_bar(size = 10).encode(\n    x = 'Life Expectancy at Birth:Q', \n    y = 'count()'\n)\n\n\n\n\n\n\nThe plotting codes are a little more verbose, but they’re much more efficient than performing the manipulations separately in pandas.\n\n\nQuestion 2: Bin transform\nFollow the example above and make a histogram of life expectancies across the globe in 2019 using an explicit bin transform to create bins spanning three years.\n\n# filter, bin, and plot\nalt.Chart(\n    data\n).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', \n                            equal = 2019)\n).transform_bin(\n    'Life Expectancy at Birth', # SOLUTION\n    field = 'Life Expectancy', # SOLUTION\n    bin = alt.Bin(step = 3) # SOLUTION\n).mark_bar(size = 10).encode(\n    x = 'Life Expectancy at Birth:Q', # SOLUTION\n    y = 'count()'\n)"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing-soln.html#aggregate-transform",
    "href": "labs/lab4-smoothing/lab4-smoothing-soln.html#aggregate-transform",
    "title": "Lab 4: Smoothing",
    "section": "Aggregate transform",
    "text": "Aggregate transform\nNow, the counting of observations in each bin (implemented via y = count()) is also an under-the-hood operation in constructing the histogram. You already saw how this was done ‘manually’ in the example above before introducing the bin transform.\nGrouped counting is a form of aggregation in the sense discussed in lecture: it produces output that has fewer values than the input by combining multiple values (in this case rows) into one value (in this case a count of the number of rows).\nThis operation can also be made explicit using .transform_aggregate(). This makes use of Altair’s aggregation shorthands for common aggregation functions; see the documentation on Altair encodings for a full list of shorthands.\nHere is how .transform_aggregate() would be used to perform the counting:\n\n# filter, bin, count, and plot\nalt.Chart(\n    data\n).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', \n                            equal = 2010)\n).transform_bin(\n    'Life Expectancy at Birth', \n    field = 'Life Expectancy',\n    bin = alt.Bin(step = 2) \n).transform_aggregate(\n    Count = 'count()', # altair shorthand operation -- see docs for full list\n    groupby = ['Life Expectancy at Birth'] # grouping variable(s)\n).mark_bar(size = 10).encode(\n    x = 'Life Expectancy at Birth:Q', \n    y = 'Count:Q'\n)"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing-soln.html#calculate-transform",
    "href": "labs/lab4-smoothing/lab4-smoothing-soln.html#calculate-transform",
    "title": "Lab 4: Smoothing",
    "section": "Calculate transform",
    "text": "Calculate transform\nBy default, Altair’s histograms are displayed on the count scale rather than the density scale.\nThe count scale means that the y-axis shows counts of observations in each bin.\nBy contrast, on the density scale, the y-axis would show proportions of total bar area (so that the area of plotted bars sums to 1).\nIt might seem like a silly distinction – after all, the two scales differ simply by a proportionality constant (the sample size times the bin width) – but as you will see shortly, the density scale is more useful for statistical thinking about the distribution of values and for direct comparisons of distributions approximated from samples of different sizes.\nThe scale conversion can be done using .transform_calculate(), which computes derived variables using arithmetic operations. In this case, one only needs to divide the count by the total number of observations.\n\n# filter, bin, count, convert scale, and plot\nalt.Chart(\n    data\n).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', \n                            equal = 2010)\n).transform_bin(\n    'Life Expectancy at Birth', \n    field = 'Life Expectancy', \n    bin = alt.Bin(step = 2)\n).transform_aggregate(\n    Count = 'count()',\n    groupby = ['Life Expectancy at Birth']\n).transform_calculate(\n    Density = 'datum.Count/(2*157)' # divide counts by sample size x binwidth\n).mark_bar(size = 10).encode(\n    x = 'Life Expectancy at Birth:Q', \n    y = 'Density:Q'\n)\n\n\n\n\n\n\n\nQuestion 3: Density scale histogram\nFollow the example above and convert your histogram from Question 2 (with the year 2019, the step size of 3, and the usage of .transform_bin(...)) to the density scale.\n\n\nFirst, calculate the sample size and store the value as sample_size. Store the desired step size as bin_width.\n\n\nConvert your histogram from Question 2 (with the year 2019, the step size of 3, and the usage of .transform_bin(...)) to the density scale. First calculate the count explicitly using .transform_aggregate(...) and then convert to a proportion using .transform_calculate(...). Multiply sample_size with bin_width to obtain the scaling constant and hardcode it into your implementation.\n\n\n\n# find scaling factor\nsample_size = np.sum(data.Year == 2019) # SOLUTION\nbin_width = 3 # SOLUTION\nprint('scaling factor = ', sample_size*bin_width)\n\n# construct histogram\nalt.Chart(data).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', \n                            equal = 2019)\n).transform_bin(\n    'Life Expectancy at Birth', # SOLUTION\n    field = 'Life Expectancy', # SOLUTION\n    bin = alt.Bin(step = 3) # SOLUTION\n).transform_aggregate(\n    Count = 'count()', # SOLUTION\n    groupby = ['Life Expectancy at Birth'] # SOLUTION\n).transform_calculate(\n    # use sample_size*bin_width to rescale - you will need to hardcode this value\n    Density = 'datum.Count/(459)' # SOLUTION\n).mark_bar(size = 20).encode(\n    x = 'Life Expectancy at Birth:Q', # SOLUTIOON\n    y = 'Density:Q' # SOLUTION\n)\n\nscaling factor =  459\n\n\n\n\n\n\n\n\ngrader.check(\"q3\")"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing-soln.html#comparing-distributions",
    "href": "labs/lab4-smoothing/lab4-smoothing-soln.html#comparing-distributions",
    "title": "Lab 4: Smoothing",
    "section": "Comparing distributions",
    "text": "Comparing distributions\nThe visual advantage of a kernel density estimate for discerning shape is even more apparent when comparing distributions.\nA major task in exploratory analysis is understanding how the distribution of a variable of interest changes depending on other variables – for example, you have already seen in the last lab that life expectancy seems to change over time. We can explore this phenomenon from a different angle by comparing distributions in different years.\nMultiple density estimates can be displayed on the same plot by passing a grouping variable (or set of variables) to .transform_density(...). For example, the cell below computes density estimates of life expectancies for each of two years.\n\nalt.Chart(data).transform_filter(\n    alt.FieldOneOfPredicate(field = 'Year',\n                            oneOf = [2010, 2019])\n).transform_density(\n    density = 'Life Expectancy',\n    groupby = ['Year'],\n    as_ = ['Life Expectancy at Birth', 'Estimated Density'],\n    bandwidth = 1.8, \n    extent = [25, 90],\n    steps = 1000 \n).mark_line().encode(\n    x = 'Life Expectancy at Birth:Q',\n    y = 'Estimated Density:Q',\n    color = 'Year:N'\n)\n\n\n\n\n\n\nOften the area beneath each density estimate is filled in. This can be done by simply appending a .mark_area() call at the end of the plot.\n\np = alt.Chart(data).transform_filter(\n    alt.FieldOneOfPredicate(field = 'Year',\n                            oneOf = [2010, 2019])\n).transform_density(\n    density = 'Life Expectancy',\n    groupby = ['Year'],\n    as_ = ['Life Expectancy at Birth', 'Estimated Density'],\n    bandwidth = 1.8, \n    extent = [25, 90],\n    steps = 1000 \n).mark_line().encode(\n    x = 'Life Expectancy at Birth:Q',\n    y = 'Estimated Density:Q',\n    color = 'Year:N'\n)\n\np + p.mark_area(opacity = 0.1)\n\n\n\n\n\n\nNotice that this makes it much easier to compare the distributions between years – you can see a pronounced rightward shift of the smooth for 2019 compared with 2010.\nWe could make the same comparison based on the histograms, but the shift is a lot harder to make out. Overlaid histograms should be avoided.\n\nalt.Chart(data).transform_filter(\n    alt.FieldOneOfPredicate(field = 'Year',\n                            oneOf = [2010, 2019])\n).mark_bar(opacity = 0.5).encode(\n    x = alt.X('Life Expectancy', bin = alt.Bin(maxbins = 30), title = 'Life Expectancy at Birth'),\n    y = alt.Y('count()', stack = None),\n    color = 'Year:N'\n)\n\n\n\n\n\n\n\n\nQuestion 5: Multiple density estimates\nFollow the example above to construct a plot showing separate density estimates of life expectancy for each region in the 2010. You can choose whether you prefer to fill in the area beneath the smooth curves, or not. Be sure to play with the bandwidth parameter and choose a value that seems sensible to you.\n\n# construct density estimates\np = alt.Chart(data).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', equal = 2010) # SOLUTION\n).transform_density(\n    density = 'Life Expectancy', # SOLUTION\n    groupby = ['region'], # SOLUTION\n    as_ = ['Life Expectancy at Birth', 'Estimated density'], # SOLUTION\n    bandwidth = 3, # SOLUTION\n    extent = [25, 90], # SOLUTION\n    steps = 1000 # SOLUTION\n).mark_line(\n).encode(\n    x = 'Life Expectancy at Birth:Q', # SOLUTION\n    y = 'Estimated density:Q', # SOLUTION\n    color = 'region:N' # SOLUTION\n)\n\n# add shaded area underneath curves\np + p.mark_area(opacity = 0.1) # SOLUTION\n\n\n\n\n\n\n\n\n\n\nQuestion 6: Interpretation\nDo the distributions of life expectancies seem to differ by region? If so, what is one difference that you notice? Answer in 1-2 sentences.\nType your answer here, replacing this text.\nSOLUTION\nYes – most distributions are bimodal, with two peaks, but focusing on the largest peaks, they are ordered from lowest to highest as: Africa, Oceania, Asia, Americas, Europe.\n\n\n\nQuestion 7: Outlier\nNotice that little peak way off to the left in the distribution of life expectancies in the Americas. That’s an outlier.\n\n\nWhich country is it? Check by filtering data appropriately and using .sort_values(...) to find the lowest life expectancy in the Americas. Save the outlying observation as a one-row dataframe called lowest_Americas and print the row.\n\n\nWhat was the life expectancy for that country in other years? Filter the data to examine the life expectancy in the country you identified as the outlier in all y. Save the resulting data frame as outlier_country.\n\n\nWhat Happened in 2010? Can you explain why the life expectancy was so low in that country for that particular year?(Hint: if you don’t remember, Google the country name and year in question.)\n\n\nType your answer here, replacing this text.\n\n# examine outlier\nlowest_Americas = data[\n    (data.region == 'Americas') & (data.Year == 2010) # SOLUTION\n    ].sort_values(\n    by = 'Life Expectancy', axis = 0, ascending = True # SOLUTION\n    ).head(1)\nlowest_Americas\n\n\n\n\n\n\n\n\nCountry Name\nYear\nLife Expectancy\nMale Life Expectancy\nFemale Life Expectancy\nGDP per capita\nregion\nsub-region\nPopulation\n\n\n\n\n246\nHaiti\n2010\n31.3\n28.0\n35.4\n1172.098543\nAmericas\nLatin America and the Caribbean\n9949322.0\n\n\n\n\n\n\n\n\n# show all obsrvations for country of interest\noutlier_country = data[data['Country Name'] == 'Haiti'] #SOLUTION\noutlier_country\n\n\n\n\n\n\n\n\nCountry Name\nYear\nLife Expectancy\nMale Life Expectancy\nFemale Life Expectancy\nGDP per capita\nregion\nsub-region\nPopulation\n\n\n\n\n244\nHaiti\n2019\n64.1\n63.3\n64.8\n1272.490925\nAmericas\nLatin America and the Caribbean\n11263077.0\n\n\n245\nHaiti\n2015\n62.6\n62.1\n63.1\n1389.119520\nAmericas\nLatin America and the Caribbean\n10695542.0\n\n\n246\nHaiti\n2010\n31.3\n28.0\n35.4\n1172.098543\nAmericas\nLatin America and the Caribbean\n9949322.0\n\n\n247\nHaiti\n2000\n57.0\n57.0\n57.2\n811.533974\nAmericas\nLatin America and the Caribbean\n8463806.0\n\n\n\n\n\n\n\nSolution\nThere was an earthquake that killed over a hundred thousand people (over 1% of the population).\n\ngrader.check(\"q7\")"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing-soln.html#loess",
    "href": "labs/lab4-smoothing/lab4-smoothing-soln.html#loess",
    "title": "Lab 4: Smoothing",
    "section": "LOESS",
    "text": "LOESS\nLocally weighted scatterplot smoothing (LOESS) is a flexible smoothing technique for visualizing trends in scatterplots. The technical details are a little involved but quite similar conceptually to kernel density estimation; we’ll just look at the implementation for now.\nTo illustrate, consider the scatterplots you made in lab 3 showing the relationship between life expectancy and GDP per capita. The plot for 2010 looked like this:\n\n# log transform gdp explicitly\ndata_mod1['log(GDP per capita)'] = np.log(data_mod1['GDP per capita'])\n\n# scatterplot\nscatter = alt.Chart(data_mod1).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', equal = 2000)\n).mark_circle(opacity = 0.5).encode(\n    x = alt.X('log(GDP per capita)', scale = alt.Scale(zero = False)),\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False)),\n    size = alt.Size('Population', scale = alt.Scale(type = 'sqrt'))\n)\n\n# show\nscatter\n\n\n\n\n\n\nTo add a LOESS curve, simply append .transform_loess() to the base plot:\n\n# compute smooth\nsmooth = scatter.transform_loess(\n    on = 'log(GDP per capita)', # x variable\n    loess = 'Life Expectancy', # y variable\n    bandwidth = 0.25 # how smooth?\n).mark_line(color = 'black')\n\n# add as a layer to the scatterplot\nscatter + smooth\n\n\n\n\n\n\nJust as with kernel density estimates, LOESS curves have a bandwidth parameter that controls how smooth or wiggly the curve is. In Altair, the LOESS bandwidth is a unitless parameter between 0 and 1.\n\n\nQuestion 8: LOESS bandwidth selection\nTinker with the bandwidth parameter to see its effect in the cell below. Then choose a value that produces a smoothing you find appropriate for indicating the general trend shown in the scatter.\n\n# compute smooth\nsmooth = scatter.transform_loess(\n    on = 'log(GDP per capita)', \n    loess = 'All', \n    bandwidth = 0.75 # SOLUTION\n).mark_line(color = 'black')\n\n# add as a layer to the scatterplot\nscatter + smooth\n\n\n\n\n\n\n\nLOESS curves can also be computed groupwise. For instance, to display separate curves for each region, one need only pass a groupby = ... argument to .transform_loess():\n\n# scatterplot\nscatter = alt.Chart(data_mod1).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', equal = 2000)\n).mark_circle(opacity = 0.5).encode(\n    x = alt.X('log(GDP per capita)', scale = alt.Scale(zero = False)),\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False)),\n    size = alt.Size('Population', scale = alt.Scale(type = 'sqrt')),\n    color = 'region'\n)\n\n# compute smooth\nsmooth = scatter.transform_loess(\n    groupby = ['region'], # add groupby\n    on = 'log(GDP per capita)', \n    loess = 'Life Expectancy', \n    bandwidth = 0.8 \n).mark_line(color = 'black')\n\n# add as a layer to the scatterplot\nscatter + smooth\n\n\n\n\n\n\nThe curves are a little jagged because there aren’t very many countries in each region.\n\ndata_mod1[data_mod1.Year == 2000].groupby('region').count().iloc[:, [0]]\n\n\n\n\n\n\n\n\nCountry Name\n\n\nregion\n\n\n\n\n\nAfrica\n45\n\n\nAmericas\n27\n\n\nAsia\n38\n\n\nEurope\n35\n\n\nOceania\n9"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing-soln.html#regression",
    "href": "labs/lab4-smoothing/lab4-smoothing-soln.html#regression",
    "title": "Lab 4: Smoothing",
    "section": "Regression",
    "text": "Regression\nYou will be learning more about linear regression later in the course, but we can introduce regression lines now as a visualization technique. As with LOESS, you don’t need to concern yourself with the mathematical details (yet). From this perspective, regression is a form of linear smoothing – a regression smooth is a straight line. By contrast, LOESS smooths have curvature – they are not straight lines.\nIn the example above, the LOESS curves don’t have much curvature. So it may be a cleaner choice visually to show linear smooths. This can be done using .transform_regression(...) with a similar argument structure.\n\n# scatterplot\nscatter = alt.Chart(data_mod1).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', equal = 2000)\n).mark_circle(opacity = 0.5).encode(\n    x = alt.X('log(GDP per capita)', scale = alt.Scale(zero = False)),\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False)),\n    size = alt.Size('Population', scale = alt.Scale(type = 'sqrt')),\n    color = 'region'\n)\n\n# compute smooth\nsmooth = scatter.transform_regression(\n    groupby = ['region'],\n    on = 'log(GDP per capita)', \n    regression = 'Life Expectancy'\n).mark_line(color = 'black')\n\n# add as a layer to the scatterplot\nscatter + smooth\n\n\n\n\n\n\n\n\nQuestion 9: Simple regression line\nBased on the example immediately above, construct a scatterplot of life expectancy against log GDP per capita in 2010 with points sized according to population (and no distinction between regions). Layer a single linear smooth on the scatterplot using .transform_regression(...).\n(Hint: remove the color aesthetic and grouping from the previous plot.)\n\n# construct scatterplot\nscatter = alt.Chart(data_mod1).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', equal = 2010) # SOLUTION\n).mark_circle(opacity = 0.5).encode(\n    x = alt.X('log(GDP per capita)', scale = alt.Scale(zero = False)), # SOLUTION\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False)), # SOLUTION\n    size = alt.Size('Population', scale = alt.Scale(type = 'sqrt')) # SOLUTION\n)\n\n# construct smooth\nsmooth = scatter.transform_regression(\n    on = 'log(GDP per capita)', # SOLUTION\n    regression = 'Life Expectancy' # SOLUTION\n).mark_line(color = 'black')\n\n# layer\nscatter + smooth # SOLUTION"
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization-soln.html",
    "href": "labs/lab3-visualization/lab3-visualization-soln.html",
    "title": "Lab 3: Data visualization",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab3-visualization.ipynb\")\nimport pandas as pd\nimport numpy as np\nimport altair as alt\n# disable row limit for plotting\nalt.data_transformers.disable_max_rows()\n# uncomment to ensure graphics display with pdf export\n# alt.renderers.enable('mimetype')\nData visualizations are graphics that represent quantitative or qualitative data. In PSTAT100 you’ll be using the python visualization library Altair, which is built around the pandas dataframe. Altair creates visualizations by mapping columns of a dataframe to the various elements of a graphic: axes, geometric objects, and aesthetics.\nVisualizations are essential tools in exploratory analysis as well as presentation. They can help an analyst identify and understand structure and patterns in a dataset at a high level and provide guidance for model development. They can be used to check assumptions and visualize model outputs. And they can be an effective means for conveying results to a general audience.\nConstructing effective visualization is usually an iterative process: plot-think-revise-repeat. In exploratory visualization often it is useful to produce a large quantity of plots in order to look at data from multiple angles; in this context, speed is helpful and details can be overlooked. By contrast, presentation graphics are typically highly refined versions of one or two exploratory plots that serve as communication tools; developing them involves attention to fine detail.\nObjectives\nIn this lab you’ll become familiar with the basic functionality of Altair – the basic kinds of graphics it generates and how to construct these graphics from a dataframe – and get a taste of the process of constructing good graphics.\nIn Altair, plots are constructed by: 1. creating a chart 2. specifying marks and encodings 3. adding various aesthetics, and 4. resolving display issues through customization.\nTechnical tutorial. You’ll get an introduction to each of these steps: * Creating a chart object from a dataframe * Encodings: mapping columns to graphical elements * Marks: geometric objects displayed on a plot (e.g., points, lines, polygons) * Aesthetics: display attributes of geometric objects (e.g., color, shape, transparency) * Customization: adjusting axes, labels, scales.\nVisualization process. In addition, our goal is to model for you the process of constructing a good visualization through iterative revisions. * Identifying and fixing display problems * Discerning informative from non-informative graphical elements * Designing efficient displays"
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization-soln.html#axes",
    "href": "labs/lab3-visualization/lab3-visualization-soln.html#axes",
    "title": "Lab 3: Data visualization",
    "section": "Axes",
    "text": "Axes\nAxes establish a reference system for a graphic: they define a space within which the graphic will be constructed. Usually these are coordinate systems defined at a particular scale, like Cartesian coordinates on the region (0, 100) x (0, 100), or polar coordinates on the unit circle, or geographic coordinates for the globe.\nIn Altair, axes are automatically determined based on encodings, but are customizable to an extent."
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization-soln.html#geometric-objects",
    "href": "labs/lab3-visualization/lab3-visualization-soln.html#geometric-objects",
    "title": "Lab 3: Data visualization",
    "section": "Geometric objects",
    "text": "Geometric objects\nGeometric objects are any objects superimposed on a set of axes: points, lines, polygons, circles, bars, arcs, curves, and the like. Often, visualizations are characterized according to the type of object used to display data – for example, the scatterplot consists of points, a bar plot consists of bars, a line plot consists of one or more lines, and so on.\nIn Altair, geometric objects are called marks."
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization-soln.html#aesthetic-attributes",
    "href": "labs/lab3-visualization/lab3-visualization-soln.html#aesthetic-attributes",
    "title": "Lab 3: Data visualization",
    "section": "Aesthetic attributes",
    "text": "Aesthetic attributes\nThe word ‘aesthetics’ is used in a variety of ways in relation to graphics; you will see this in your reading. For us, ‘aesthetic attirbutes’ will refer to attributes of geometric objects like color. The primary aesthetics in statistical graphics are color, opacity, shape, and size.\nIn Altair, aesthetic attributes are called mark properties."
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization-soln.html#text",
    "href": "labs/lab3-visualization/lab3-visualization-soln.html#text",
    "title": "Lab 3: Data visualization",
    "section": "Text",
    "text": "Text\nText is used in graphics to label axes, geometric objects, and legends for aesthetic mappings. Text specification is usually a step in customization for presentation graphics, but often skipped in exploratory graphics. Carefully chosen text is very important in this context, because it provides essential information that a general reader needs to interpret a plot.\nIn Altair, text is usually controlled as part of encoding specification."
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization-soln.html#basic-scatterplots",
    "href": "labs/lab3-visualization/lab3-visualization-soln.html#basic-scatterplots",
    "title": "Lab 3: Data visualization",
    "section": "Basic scatterplots",
    "text": "Basic scatterplots\nThe following cell constructs a scatterplot of life expectancy at birth against GDP per capita; each point corresponds to one country in one year. The syntax works as follows: * alt.Chart() begins by constructing a ‘chart’ object constructed from the dataframe; * the result is passed to .mark_circle(), which specifies a geometric object (circles) to add to the chart; * the result is passed to .encode(), which specifies which columns should be used to determine the coordinates of the circles.\n\n# basic scatterplot\nalt.Chart(data).mark_circle().encode(\n    x = 'GDP per capita',\n    y = 'Life Expectancy'\n)\n\n\n\n\n\n\n\n\nQuestion 1: Different marks\nThe cell below is a copy of the previous cell. Have a look at the documentation on marks for a list of the possible mark types. Try out a few alternatives to see what they look like. Once you’re satisfied, change the mark to points.\n\n# BEGIN SOLUTION NO PROMPT\n# basic scatterplot\nfig_q1 = alt.Chart(data).mark_point().encode( # change made here from circle to point\n    x = 'GDP per capita',\n    y = 'Life Expectancy'\n)\n# END SOLUTION\n\n\"\"\" # BEGIN PROMPT\n# basic scatterplot\nalt.Chart(data).mark_circle().encode( # tinker here with different marks\n    x = 'GDP per capita',\n    y = 'Life Expectancy'\n)\n\"\"\"; # END PROMPT\n\nfig_q1 # SOLUTION NO PROMPT\n\n\n\n\n\n\n\n\n\n\nQuestion 2: Mark properties\nWhat is the difference between points and circles, according to the documentation?\nType your answer here, replacing this text.\nSOLUTION: Points can have many shapes; circles can not."
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization-soln.html#axis-adjustments-with-alt.x-and-alt.y",
    "href": "labs/lab3-visualization/lab3-visualization-soln.html#axis-adjustments-with-alt.x-and-alt.y",
    "title": "Lab 3: Data visualization",
    "section": "Axis adjustments with alt.X() and alt.Y()",
    "text": "Axis adjustments with alt.X() and alt.Y()\nAn initial problem that would be good to resolve before continuing is that the y axis label isn’t informative. Let’s change that by wrapping the column to encode in alt.Y() and specifying the title manually.\n\n# change axis label\nalt.Chart(data).mark_circle().encode(\n    x = 'GDP per capita',\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth')\n)\n\n\n\n\n\n\nalt.Y() and alt.X() are helper functions that modify encoding specifications. The cell below adjusts the scale of the y axis as well; since above there are no life expectancies below 30, starting the y axis at 0 adds whitespace.\n\n# don't start y axis at zero\nalt.Chart(data).mark_circle().encode(\n    x = 'GDP per capita',\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False))\n)\n\n\n\n\n\n\nIn the plot above, there are a lot of points squished together near \\(x = 0\\). It will make it easier to see the pattern of scatter in that region to adjust the x axis so that values are not displayed on a linear scale. Using alt.Scale() allows for efficient axis rescaling; the cell below puts GDP per capita on a log scale.\n\n# log scale for x axis\nalt.Chart(data).mark_circle().encode(\n    x = alt.X('GDP per capita', scale = alt.Scale(type = 'log')),\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False))\n)\n\n\n\n\n\n\n\n\nQuestion 3: Changing axis scale\nTry a different scale by modifying the type = ... argument of alt.Scale in the cell below. Look at the altair documentation for a list of the possible types.\n\n# try another axis scale\nalt.Chart(data).mark_circle().encode(\n    x = alt.X('GDP per capita', scale = alt.Scale(type = 'sqrt')), # SOLUTION\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False))\n)"
  },
  {
    "objectID": "labs/lab7-classification/lab7-classification.html",
    "href": "labs/lab7-classification/lab7-classification.html",
    "title": "Lab 7: Classification",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab7-classification.ipynb\")\nimport numpy as np\nimport pandas as pd\nimport altair as alt\nimport statsmodels.api as sm\n\n# disable row limit for plotting\nalt.data_transformers.disable_max_rows()\n# uncomment to ensure graphics display with pdf export\n# alt.renderers.enable('mimetype')\nThis lab covers binary regression and classification using logistic regression models. The logistic regression model for a binary outcome \\(y \\in \\{0, 1\\}\\) posits that the probability of the outcome of interest follows a logistic function of the explanatory variable \\(x\\):\n\\[\nP(Y = 1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}}\n\\]\nMore commonly, the model is written in terms of the log-odds of the outcome of interest:\n\\[\n\\log\\left[\\frac{P(Y = 1)}{P(Y = 0)}\\right]\n= \\beta_0 + \\beta_1 x\n\\]\nAdditional explanatory variables can be included in the model by specifying a linear predictor with additional \\(\\beta_j x_j\\) terms.\nLogistic regression models represent the probability of an outcome as a function of one or more explanatory variables; fitted probabilities can be coerced to hard classifications by thresholding.\nFor this lab, we’ll revisit the SEDA data from an earlier assignment. Below are the log median incomes and estimated achievement gaps on math and reading tests for 625 school districts in California:\nseda = pd.read_csv('data/seda.csv').dropna()\nseda.head()\nThe estimated achievement gap is positive if boys outperform girls, and negative if girls outperform boys. We can therefore define a binary indicator of the direction of the achievement gap:\nseda['favors_boys'] = (seda.gap &gt; 0)\nseda.head()\nYou may recall having calculated the proportion of districts in various income brackets with a math gap favoring boys.\nWe will now consider the closely related problem of estimating the probability that a district has a math gap favoring boys based on the median income of the district.\nSince we’re only considering math gaps, we’ll filter out the gap estimates on reading tests.\nreg_data = seda[seda.subject == 'math'].loc[:, ['log_income', 'favors_boys']]\nLet’s set aside the data for 100 randomly chosen districts to use later in quantifying the classification accuracy of the model."
  },
  {
    "objectID": "labs/lab7-classification/lab7-classification.html#exploratory-analysis",
    "href": "labs/lab7-classification/lab7-classification.html#exploratory-analysis",
    "title": "Lab 7: Classification",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis\nPreviously you had binned income into brackets and constructed a table of the proportion of districts in each income bracket with a math gap favoring boys. It turns out that binning and aggregation is a useful exploratory strategy for binary regression. Your table from before would have been something like this:\n\n# define income bracket\ntrain['income_bracket'] = pd.cut(train.log_income, 10)\n\n# compute mean and standard deviation of each variable by bracket\ntbl = train.groupby('income_bracket').agg(func = ['mean', 'std'])\n\n# fix column indexing and remove 'artificial' brackets containing only min and max values\ntbl.columns = [\"_\".join(a) for a in tbl.columns.to_flat_index()]\ntbl = tbl[tbl.favors_boys_std &gt; 0]\n\n# display\ntbl\n\nWe can plot these proportions, with standard deviations, as functions of income. Since standard deviations are fairly high, the variability bands only show 0.4 standard deviations in either direction.\n\n\ntrend = alt.Chart(tbl).mark_line(point = True).encode(\n    x = alt.X('log_income_mean', title = 'log income'),\n    y = alt.Y('favors_boys_mean', title = 'Pr(math gap favors boys)')\n)\n\nband = alt.Chart(tbl).transform_calculate(\n    lwr = 'datum.favors_boys_mean - 0.4*datum.favors_boys_std',\n    upr = 'datum.favors_boys_mean + 0.4*datum.favors_boys_std'\n).mark_area(opacity = 0.3).encode(\n    x = 'log_income_mean',\n    y = alt.Y('lwr:Q', scale = alt.Scale(domain = [0, 1])),\n    y2 = 'upr:Q'\n)\n\ntrend + band\n\nWe can regard these proportions as estimates of the probability that the achievement gap in math favors boys. Thus, the figure above displays the exact relationship we will attempt to model, only as a continuous function of income rather than at 8 discrete points.\n\n\nQuestion 2: model assumptions\nThe logistic regression model assumes that the probability of the outcome of interest is a monotonic function of the explanatory variable(s). Examine the plot above and discuss with your neighbor. Does this monotinicity assumption seem to be true? Why or why not?\nType your answer here, replacing this text."
  },
  {
    "objectID": "labs/lab7-classification/lab7-classification.html#model-fitting",
    "href": "labs/lab7-classification/lab7-classification.html#model-fitting",
    "title": "Lab 7: Classification",
    "section": "Model fitting",
    "text": "Model fitting\nWe’ll fit a simple model of the probility that the math gap favors boys as a logistic function of log income:\n\\[\n\\log\\left[\\frac{P(\\text{gap favors boys})}{1 - P(\\text{gap favors boys})}\\right] = \\beta_0 + \\beta_1 \\log(\\text{median income})\n\\]\nThe data preparations are exactly the same as in linear regression: we’ll obtain a vector of the response outcome and an explanatory variable matrix containing log median income and a constant (for the intercept).\n\n# explanatory variable matrix\nx = sm.add_constant(train.log_income)\n\n# response\ny = train.favors_boys\n\nThe model is fit using statsmodels.Logit(). Note that the endogenous variable (the response) can be either Boolean (take values True and False) or integer (take values 0 or 1).\n\n# fit the model\nmodel = sm.Logit(endog = y, exog = x)\n\n# store the fitted model\nfit = model.fit()\n\n# display parameter estimates\nfit.params\n\nA coefficient table remains useful for logistic regression:\n\ncoef_tbl = pd.DataFrame(\n    {'estimate': fit.params,\n    'standard error': np.sqrt(fit.cov_params().values.diagonal())},\n    index = x.columns\n)\ncoef_tbl\n\n\nQuestion 3: confidence intervals\nCompute 99% confidence intervals for the model parameters. Store the result as a dataframe called param_ci.\nHint: the syntax is identical to that based on sm.OLS; this is also mentioned in the lecture slides.\n\n# compute 99% confidence intervals\nparam_ci = ...\n\n# display\nparam_ci.rename(columns = {0: 'lwr', 1: 'upr'}, inplace = True)\nparam_ci\n\n\ngrader.check(\"q3\")\n\nWe can superimpose the predicted probabilities for a fine grid of log median incomes on the data figure we had made previously to compare the fitted model with the observed values:\n\n# grid of log income values\ngrid_df = pd.DataFrame({\n    'log_income': np.linspace(9, 14, 200)\n})\n\n# add predictions\ngrid_df['pred'] = fit.predict(sm.add_constant(grid_df))\n\n# plot predictions against income\nmodel_viz = alt.Chart(grid_df).mark_line(color = 'red', opacity = 0.5).encode(\n    x = 'log_income',\n    y = 'pred'\n)\n\n# superimpose on data figure\ntrend + band + model_viz\n\nDepending on your training sample, the model may or may not align well with the computed proportions, but it should be mostly or entirely within the 0.4-standard-deviation band.\nTo interpret the estimated relationship, recall that if median income is doubled, the log-odds changes by:\n\\[\n\\hat{\\beta}_1\\log(2\\times\\text{median income}) - \\hat{\\beta}_1 \\log(\\text{median income}) = \\hat{\\beta}_1 \\log(2)\n\\]\nNow, exponentiating gives the estimated multiplicative change in odds:\n\\[\n\\exp\\left\\{\\log(\\text{baseline odds}) + \\hat{\\beta}_1 \\log(2)\\right\\} = \\text{baseline odds} \\times e^{\\hat{\\beta}_1 \\log(2)}\n\\]\nSo computing \\(e^{\\hat{\\beta}_1 \\log(2)}\\) gives a quantity we can readily interpret:\n\nnp.exp(fit.params.log_income*np.log(2))\n\nThe exact number will depend a little bit on the data partition you used to compute the estimate, but the answer should be roughly consistent with the following interpretation:\n\nEach doubling of median income is associated with an estimated four-fold increase in the odds that a school district has a math gap favoring boys."
  },
  {
    "objectID": "labs/lab7-classification/lab7-classification.html#classification",
    "href": "labs/lab7-classification/lab7-classification.html#classification",
    "title": "Lab 7: Classification",
    "section": "Classification",
    "text": "Classification\nNow we’ll consider the task of classifying new school districts by the predicted direction of their math achievement gap.\nA straightforward classification rule would be:\n\\[\n\\text{gap predicted to favor boys} \\quad\\Longleftrightarrow\\quad \\widehat{Pr}(\\text{gap favors boys}) &gt; 0.5\n\\]\nWe can obtain the estimated probabilities using .predict(), and construct the classifier manually. To assess the accuracy, we’ll want to arrange the classifications side-by-side with the observed outcomes:\n\n# compute predicted probabilities on test set\npreds = fit.predict(sm.add_constant(test.log_income))\n\n# construct classifier\npred_df = pd.DataFrame({\n    'observation': test.favors_boys,\n    'prediction': preds &gt; 0.5\n})\n\n# preview\npred_df.head()\n\nNote that the testing partition was used here – to get an unbiased estimate of the classification accuracy, we need data that were not used in fitting the model.\nCross-tabulating observed and predicted outcomes gives a detailed view of the accuracy and error:\n\n# cross-tabulate classifications with observed outcomes\npred_tbl = pred_df.groupby(['observation', 'prediction']).size()\npred_tbl\n\nThe entries where observation and prediction have the same value are counts of the number of districts correctly classified; those where they do not match are counts of errors.\n\nQuestion 4: overall classification accuracy\nCompute the overall classification accuracy – the proportion of districts that were correctly classified.\n\naccuracy = ...\n\n\ngrader.check(\"q4\")\n\nOften class-wise accuracy rates are more informative, because there are two possible types of error:\n\nA district that has a math gap favoring girls is classified as having a math gap favoring boys\nA district that has a math gap favoring boys is classified as having a math gap favoring girls\n\nYou may notice that there were more errors of one type than another in your result above. This is not conveyed by reporting the overall accuracy rate.\nFor a clearer picture, we can find the proportion of errors among by outcome:\n\npred_df['error'] = (pred_df.observation != pred_df.prediction)\nfnr = pred_df[pred_df.observation == True].error.mean()\nfpr = pred_df[pred_df.observation == False].error.mean()\ntpr = 1 - fpr\ntnr = 1 - fnr\n\nprint('false positive rate: ', fpr)\nprint('false negative rate: ', fnr)\nprint('true positive rate (sensitivity): ', tpr)\nprint('true negative rate (specificity): ', tnr)\n\n\n\nQuestion 5: make your own classifier\nDefine a new classifier by adjusting the probability threshold. Compute and print the false positive, false negative, true positive, and true negative rates. Experiment until you achieve a better balance between errors of each type.\n\n# construct classifier\nnew_pred_df = pd.DataFrame({\n    ...\n    ...\n})\n\n# compute error rates\nnew_pred_df['error'] = ...\nnew_fnr = ...\nnew_fpr = ...\nnew_tpr = ...\nnew_tnr = ...\n\n# print\nprint('false positive rate: ', new_fpr)\nprint('false negative rate: ', new_fnr)\nprint('true positive rate (sensitivity): ', new_tpr)\nprint('true negative rate (specificity): ', new_tnr)\n\n\ngrader.check(\"q5\")"
  },
  {
    "objectID": "misc/week5-correlation-simulations.html",
    "href": "misc/week5-correlation-simulations.html",
    "title": "PSTAT100",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport altair as alt\n\n\nn = 100\nx = np.random.uniform(low = 0, high = 1, size = n)\nsim_df = pd.DataFrame({'x': x})\n\n\n## linear relationship\n\n# intercept, slope\na, b = 1, -2\n\n# noise\nnoise_sd = 0.5\nnoise = np.random.normal(loc = 0, scale = noise_sd, size = 100)\n\n# simulate y\nsim_df['y'] = a + b*x + noise\n\n# plot\nscatter = alt.Chart(\n    sim_df\n).mark_point().encode(\n    x = 'x',\n    y = 'y'\n)\n\n# compute correlation\nprint('correlation: ', sim_df.corr().loc['x', 'y'])\nscatter\n\ncorrelation:  -0.7751417785628631\n\n\n\n\n\n\n\n\n## outlier sensitivity\n\nsim_df.loc[100] = [3, 3]\n\n# plot\nscatter = alt.Chart(\n    sim_df\n).mark_point().encode(\n    x = 'x',\n    y = 'y'\n)\n\n# compute correlation\nprint('correlation: ', sim_df.corr().loc['x', 'y'])\nsim_df = sim_df.loc[0:99].copy()\nscatter\n\ncorrelation:  -0.3019971006847511\n\n\n\n\n\n\n\n\n## quadratic relationship\n\n# center x, center y, scale\na, b, c = 0.5, 0.5, 3\n\n# noise\nnoise_sd = 0.1\nnoise = np.random.normal(loc = 0, scale = noise_sd, size = 100)\n\n# simulate y\nsim_df['y'] = c*(x - a)*(x - b) + noise\n\n# plot\nscatter = alt.Chart(\n    sim_df\n).mark_point().encode(\n    x = 'x',\n    y = 'y'\n)\n\n# compute correlation\nprint('correlation: ', sim_df.corr().loc['x', 'y'])\nscatter\n\ncorrelation:  -0.038293509416259405\n\n\n\n\n\n\n\n\n## log relationship\n\n# offset, scale\na, b = 0.5, 0.5\n\n# noise\nnoise_sd = 0.1\nnoise = np.random.normal(loc = 0, scale = noise_sd, size = 100)\n\n# simulate y\nsim_df['y'] = a + b*np.log(x) + noise\n\n# plot\nscatter = alt.Chart(\n    sim_df\n).mark_point().encode(\n    x = 'x',\n    y = 'y'\n)\n\n# compute correlation\nprint('correlation: ', sim_df.corr().loc['x', 'y'])\nscatter\n\ncorrelation:  0.8699655736164486"
  },
  {
    "objectID": "misc/week6-slr-codes.html",
    "href": "misc/week6-slr-codes.html",
    "title": "PSTAT100",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport altair as alt\nfrom sklearn.linear_model import LinearRegression\nfrom scipy.stats import norm\nfrom scipy import stats\n\nalt.data_transformers.disable_max_rows()\n\nDataTransformerRegistry.enable('default')\n\n\n\ngrid_df = pd.DataFrame({'n_meteorites': np.arange(50)})\ngrid_df['negative binomial'] = stats.nbinom.pmf(k = grid_df.n_meteorites, n = 12, p = 0.4)\ngrid_df['poisson'] = stats.poisson.pmf(k = grid_df.n_meteorites, mu = 15)\n\n\ngrid_df = grid_df.melt(\n    id_vars = 'n_meteorites',\n    var_name = 'distribution',\n    value_name = 'pmf'\n)\n\ndists = alt.Chart(grid_df).mark_line(point = True).encode(\n    x = 'n_meteorites',\n    y = 'pmf',\n    color = 'distribution'\n)\n\nnp.random.seed(80621)\ntoy_data = pd.DataFrame({'n_meteorites': np.random.negative_binomial(12, 0.4, 225)})\n\nhist = alt.Chart(toy_data).transform_bin(\n    as_ = 'bin', \n    field = 'n_meteorites', \n    bin = alt.Bin(step = 3)\n).transform_aggregate(\n    Count = 'count()',\n    groupby = ['bin']\n).transform_calculate(\n    density = 'datum.Count/(3*225)',\n    binshift = 'datum.bin + 1.5'\n).mark_bar(size = 12, opacity = 0.8).encode(\n    x = alt.X('binshift:Q', title = 'number of meteorites'),\n    y = 'density:Q'\n)\n\ndists + hist\n\n\n\n\n\n\n\ntoy_data.sum()\n\nn_meteorites    3907\ndtype: int64\n\n\n\n# import grade-aggregated seda data from hw2\nseda = pd.read_csv('data/seda.csv')\nseda.head()\n\n\n\n\n\n\n\n\nid\nlog_income\nsubject\ngap\n\n\n\n\n0\n600001\n11.392048\nmath\n-0.562855\n\n\n1\n600006\n11.607236\nmath\n0.061163\n\n\n2\n600011\n10.704570\nmath\n-0.015417\n\n\n3\n600012\n10.589787\nmath\nNaN\n\n\n4\n600013\n11.399662\nmath\n0.054454\n\n\n\n\n\n\n\n\n# plot from hw2\nbase = alt.Chart(seda).mark_point(opacity = 0.5).encode(\n    y = alt.Y('gap', title = 'estimated gender gap (m - f)'),\n    x = alt.X('log_income', scale = alt.Scale(zero = False), title = 'log(median income)'),\n    color = 'subject'\n)\n\nhw2_plot = base + base.transform_regression('log_income', 'gap', groupby = ['subject']).mark_line()\n\nhw2_plot\n\n\n\n\n\n\n\n# filter to math and remove NaNs\nregdata = seda[seda.subject == 'math'].dropna()\nregdata.head()\n\n\n\n\n\n\n\n\nid\nlog_income\nsubject\ngap\n\n\n\n\n0\n600001\n11.392048\nmath\n-0.562855\n\n\n1\n600006\n11.607236\nmath\n0.061163\n\n\n2\n600011\n10.704570\nmath\n-0.015417\n\n\n4\n600013\n11.399662\nmath\n0.054454\n\n\n5\n600014\n10.826107\nmath\n0.020526\n\n\n\n\n\n\n\n\n# simple scatterplot of math gap vs district income\nscatter = alt.Chart(regdata).mark_point().encode(\n    x = alt.X('log_income', scale = alt.Scale(zero = False)),\n    y = 'gap'\n)\n\n# show\nscatter\n\n\n\n\n\n\n\n# save explanatory variable and response variable separately as arrays\nx = regdata.log_income.values\ny = regdata.gap.values\n\n# check dimensions of x -- must be n x 1 for regression\nx.shape\n\n(625,)\n\n\n\n# add axis\nx = x[:, np.newaxis]\nx.shape\n\n(625, 1)\n\n\n\n# configure regression module\nslr = LinearRegression()\n\n# fit slr model\nslr.fit(x, y)\n\nLinearRegression()\n\n\n\n# store estimates\nslope, intercept = slr.coef_, slr.intercept_\n\nestimates = np.append(intercept, slope)\nestimates\n\narray([-1.35616996,  0.12105696])\n\n\n\n# ols solution, by hand\nx_mx = np.vstack([np.repeat(1, len(x)), x[:, 0]]).transpose() # X\nxtx = x_mx.transpose().dot(x_mx) # X'X\nxtx_inv = np.linalg.inv(xtx) # (X'X)^{-1}\nxtx_inv.dot(x_mx.transpose()).dot(y) # (X'X)^{-1} X'y\n\narray([-1.35616996,  0.12105696])\n\n\n\n# fitted values\nfitted = slr.predict(x)\n\n\n# residuals\nresid = y - fitted\n\n\n# store data with fitted values and residuals\nfit_df = pd.DataFrame({'log_income': x[:, 0], \n              'gap': y,\n              'fitted': fitted,\n              'residuals': resid})\n\n# base chart\nbase = alt.Chart(fit_df).encode(\n    x = alt.X('log_income', scale = alt.Scale(zero = False))\n)\n\n# data scatter\npoints = base.mark_point(opacity = 0.5).encode(y = 'gap')\n\n# grid of values along regression line\nline_df = pd.DataFrame({'log_income': np.linspace(x.min(), x.max(), 500)})\nline_df['gap'] = line_df.log_income*slope + intercept\n\n# plot line\nline = alt.Chart(line_df).mark_line(\n    color = 'red',\n    opacity = 0.4\n).encode(\n    x = 'log_income',\n    y = 'gap'\n)\n\n# show residuals as vertical lines\nresids = base.mark_errorbar(opacity = 0.3).encode(\n    y = 'gap',\n    y2 = 'fitted'\n)\n\n# display\n(points + line).properties(title = 'line + scatter') | (points + line + resids).properties(title = 'residuals shown in grey')\n\n\n\n\n\n\n\n(points + line).properties(title = 'by hand') | (points.transform_regression('log_income', 'gap').mark_line() + points).properties(title = 'using Altair')\n\n\n\n\n\n\n\nfit_df.shape\n\n(625, 4)\n\n\n\n# plot distribution of residuals\nhist = alt.Chart(fit_df).transform_bin(\n    as_ = 'bin', \n    field = 'residuals', \n    bin = alt.Bin(step = 0.05)\n).transform_aggregate(\n    Count = 'count()',\n    groupby = ['bin']\n).transform_calculate(\n    density = 'datum.Count/(0.05*625)',\n    binshift = 'datum.bin + 0.025'\n).mark_bar(size = 12, opacity = 0.8).encode(\n    x = alt.X('binshift:Q', title = 'residuals'),\n    y = 'density:Q'\n)\n\nsmooth = alt.Chart(fit_df).transform_density(\n    density = 'residuals',\n    as_ = ['residuals', 'density'],\n    bandwidth = 0.05,\n    extent = [-0.8, 0.6],\n    steps = 500\n).mark_line(color = 'black', opacity = 0.6).encode(\n    x = 'residuals:Q',\n    y = 'density:Q'\n)\n\nhist + smooth\n\n\n\n\n\n\n\n# residual mean, variance\nresid.var(), resid.mean()\n\n(0.013129022870165687, -1.744382416291046e-16)\n\n\n\n# residual SE\nn = len(x)\np = 2\nresid_se = np.sqrt(resid.var()*(n - 1)/(n - p))\n\nresid_se\n\n0.11467387123120727\n\n\n\npdf_df = pd.DataFrame({'residual': np.linspace(-0.8, 0.6, 500)})\npdf_df['density'] = norm.pdf(pdf_df.residual, loc = 0, scale = resid_se)\n\nnormal_density = alt.Chart(pdf_df).mark_line(\n    color = 'red',\n    opacity = 0.4\n).encode(\n    y = 'density', \n    x = 'residual'\n)\n\n(hist + smooth + normal_density).properties(title = 'normal density in red')\n\n\n\n\n\n\n\n# coefficient variances/covariances\nx_mx = np.vstack([np.repeat(1, n), x[:, 0]]).transpose()\ncoef_vcov = np.linalg.inv(x_mx.transpose().dot(x_mx))*(resid_se**2)\n\n# coefficient standard errors\ncoef_se = np.sqrt(coef_vcov.diagonal())\n\n# coefficient intervals\nnp.vstack([estimates + 2*coef_se, estimates - 2*coef_se])\n\narray([[-1.09498451,  0.14472448],\n       [-1.61735541,  0.09738944]])\n\n\n\ndef subsample_reg():\n    # subsample data\n    subsamp = regdata.sample(n = 200)\n\n    # save explanatory variable and response variable separately as arrays\n    x = subsamp.log_income.values\n    y = subsamp.gap.values\n\n    # add axis\n    x = x[:, np.newaxis]\n\n    # configure regression module\n    slr = LinearRegression()\n\n    # fit slr model\n    slr.fit(x, y)\n\n    # store estimates\n    slope, intercept = slr.coef_, slr.intercept_\n\n    estimates = np.append(intercept, slope)\n\n    # fitted values\n    fitted = slr.predict(x)\n\n    # residuals\n    resid = y - fitted\n\n    # store data with fitted values and residuals\n    fit_df = pd.DataFrame({'log_income': x[:, 0], \n                  'gap': y,\n                  'fitted': fitted,\n                  'residuals': resid})\n\n    # base chart\n    base = alt.Chart(fit_df).encode(\n        x = alt.X('log_income', scale = alt.Scale(domain = (10, 12.4)))\n    )\n\n    # data scatter\n    points = base.mark_point(opacity = 0.5).encode(y = alt.Y('gap', scale = alt.Scale(domain = (-1, 0.5))))\n\n    # grid of values along regression line\n    line_df = pd.DataFrame({'log_income': np.linspace(10, 12.4, 500)})\n    line_df['gap'] = line_df.log_income*slope + intercept\n\n    # plot line\n    line = alt.Chart(line_df).mark_line(\n        color = 'red',\n        opacity = 0.4\n    ).encode(\n        x = 'log_income',\n        y = 'gap'\n    )\n\n\n    # display\n    plot = (points + line)\n\n    return plot\n\n\n(subsample_reg() + subsample_reg() + subsample_reg() + subsample_reg() + subsample_reg()).properties(title = 'lines fit to 5 subsamples')\n\n\n\n\n\n\n\n# fitted variances\nxtx_inv = np.linalg.inv(x_mx.transpose().dot(x_mx))\nx0_mx = np.vstack([np.repeat(1, len(line_df)), np.linspace(x.min(), x.max(), 500)]).transpose()\nfit_var = x0_mx.dot(xtx_inv).dot(x0_mx.transpose()).diagonal()*(resid_se**2)\nfit_se = np.sqrt(fit_var)\n\nline_df['lwr'] = line_df.gap - 2*fit_se\nline_df['upr'] = line_df.gap + 2*fit_se\n\nband = alt.Chart(line_df).mark_errorband(color = 'grey').encode(\n    x = 'log_income',\n    y = alt.Y('lwr', title = 'gap'),\n    y2 = 'upr'\n)\n\n(points + line + band).properties(title = 'regression line with uncertainty band') \n\n\n\n\n\n\n\n# prediction\nnewobs = np.array([1, np.log(86000)])\npred = estimates.dot(newobs)\n\npred_se = np.sqrt((resid_se**2)*(1 + newobs.dot(xtx_inv).dot(newobs)))\n\n\npred_df = pd.DataFrame({'log_income': [np.log(86000)], 'lwr': [pred - 2*pred_se], 'upr': [pred + 2*pred_se], 'gap': pred})\npred_bar = alt.Chart(pred_df).mark_errorbar(opacity = 0.6).encode(\n    x = 'log_income',\n    y = alt.Y('lwr', title = 'gap'),\n    y2 = 'upr'\n)\n\npred_pt = alt.Chart(pred_df).mark_circle(color = 'black', opacity = 0.6, size = 50).encode(\n    x = 'log_income',\n    y = 'gap'\n)\n\npoints + line + band + pred_bar + pred_pt"
  },
  {
    "objectID": "labs/lab7-classification/lab7-classification-soln.html",
    "href": "labs/lab7-classification/lab7-classification-soln.html",
    "title": "Lab 7: Classification",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab7-classification.ipynb\")\nimport numpy as np\nimport pandas as pd\nimport altair as alt\nimport statsmodels.api as sm\n\n# disable row limit for plotting\nalt.data_transformers.disable_max_rows()\n# uncomment to ensure graphics display with pdf export\n# alt.renderers.enable('mimetype')\n\nDataTransformerRegistry.enable('default')\nThis lab covers binary regression and classification using logistic regression models. The logistic regression model for a binary outcome \\(y \\in \\{0, 1\\}\\) posits that the probability of the outcome of interest follows a logistic function of the explanatory variable \\(x\\):\n\\[\nP(Y = 1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}}\n\\]\nMore commonly, the model is written in terms of the log-odds of the outcome of interest:\n\\[\n\\log\\left[\\frac{P(Y = 1)}{P(Y = 0)}\\right]\n= \\beta_0 + \\beta_1 x\n\\]\nAdditional explanatory variables can be included in the model by specifying a linear predictor with additional \\(\\beta_j x_j\\) terms.\nLogistic regression models represent the probability of an outcome as a function of one or more explanatory variables; fitted probabilities can be coerced to hard classifications by thresholding.\nFor this lab, we’ll revisit the SEDA data from an earlier assignment. Below are the log median incomes and estimated achievement gaps on math and reading tests for 625 school districts in California:\nseda = pd.read_csv('data/seda.csv').dropna()\nseda.head()\n\n\n\n\n\n\n\n\nid\nlog_income\nsubject\ngap\n\n\n\n\n0\n600001\n11.392048\nmath\n-0.562855\n\n\n1\n600006\n11.607236\nmath\n0.061163\n\n\n2\n600011\n10.704570\nmath\n-0.015417\n\n\n4\n600013\n11.399662\nmath\n0.054454\n\n\n5\n600014\n10.826107\nmath\n0.020526\nThe estimated achievement gap is positive if boys outperform girls, and negative if girls outperform boys. We can therefore define a binary indicator of the direction of the achievement gap:\nseda['favors_boys'] = (seda.gap &gt; 0)\nseda.head()\n\n\n\n\n\n\n\n\nid\nlog_income\nsubject\ngap\nfavors_boys\n\n\n\n\n0\n600001\n11.392048\nmath\n-0.562855\nFalse\n\n\n1\n600006\n11.607236\nmath\n0.061163\nTrue\n\n\n2\n600011\n10.704570\nmath\n-0.015417\nFalse\n\n\n4\n600013\n11.399662\nmath\n0.054454\nTrue\n\n\n5\n600014\n10.826107\nmath\n0.020526\nTrue\nYou may recall having calculated the proportion of districts in various income brackets with a math gap favoring boys.\nWe will now consider the closely related problem of estimating the probability that a district has a math gap favoring boys based on the median income of the district.\nSince we’re only considering math gaps, we’ll filter out the gap estimates on reading tests.\nreg_data = seda[seda.subject == 'math'].loc[:, ['log_income', 'favors_boys']]\nLet’s set aside the data for 100 randomly chosen districts to use later in quantifying the classification accuracy of the model."
  },
  {
    "objectID": "labs/lab7-classification/lab7-classification-soln.html#exploratory-analysis",
    "href": "labs/lab7-classification/lab7-classification-soln.html#exploratory-analysis",
    "title": "Lab 7: Classification",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis\nPreviously you had binned income into brackets and constructed a table of the proportion of districts in each income bracket with a math gap favoring boys. It turns out that binning and aggregation is a useful exploratory strategy for binary regression. Your table from before would have been something like this:\n\n# define income bracket\ntrain['income_bracket'] = pd.cut(train.log_income, 10)\n\n# compute mean and standard deviation of each variable by bracket\ntbl = train.groupby('income_bracket').agg(func = ['mean', 'std'])\n\n# fix column indexing and remove 'artificial' brackets containing only min and max values\ntbl.columns = [\"_\".join(a) for a in tbl.columns.to_flat_index()]\ntbl = tbl[tbl.favors_boys_std &gt; 0]\n\n# display\ntbl\n\n\n\n\n\n\n\n\nlog_income_mean\nlog_income_std\nfavors_boys_mean\nfavors_boys_std\n\n\nincome_bracket\n\n\n\n\n\n\n\n\n(10.235, 10.462]\n10.348307\n0.060971\n0.190476\n0.402374\n\n\n(10.462, 10.69]\n10.583395\n0.067177\n0.238636\n0.428693\n\n\n(10.69, 10.918]\n10.800607\n0.067857\n0.234043\n0.425669\n\n\n(10.918, 11.146]\n11.019188\n0.069156\n0.333333\n0.473242\n\n\n(11.146, 11.374]\n11.251932\n0.065589\n0.450549\n0.500305\n\n\n(11.374, 11.601]\n11.452734\n0.059902\n0.614035\n0.491150\n\n\n(11.601, 11.829]\n11.683897\n0.068255\n0.851852\n0.362014\n\n\n(11.829, 12.057]\n11.931236\n0.067608\n0.888889\n0.333333\n\n\n\n\n\n\n\nWe can plot these proportions, with standard deviations, as functions of income. Since standard deviations are fairly high, the variability bands only show 0.4 standard deviations in either direction.\n\n\ntrend = alt.Chart(tbl).mark_line(point = True).encode(\n    x = alt.X('log_income_mean', title = 'log income'),\n    y = alt.Y('favors_boys_mean', title = 'Pr(math gap favors boys)')\n)\n\nband = alt.Chart(tbl).transform_calculate(\n    lwr = 'datum.favors_boys_mean - 0.4*datum.favors_boys_std',\n    upr = 'datum.favors_boys_mean + 0.4*datum.favors_boys_std'\n).mark_area(opacity = 0.3).encode(\n    x = 'log_income_mean',\n    y = alt.Y('lwr:Q', scale = alt.Scale(domain = [0, 1])),\n    y2 = 'upr:Q'\n)\n\ntrend + band\n\n\n\n\n\n\nWe can regard these proportions as estimates of the probability that the achievement gap in math favors boys. Thus, the figure above displays the exact relationship we will attempt to model, only as a continuous function of income rather than at 8 discrete points.\n\n\nQuestion 2: model assumptions\nThe logistic regression model assumes that the probability of the outcome of interest is a monotonic function of the explanatory variable(s). Examine the plot above and discuss with your neighbor. Does this monotinicity assumption seem to be true? Why or why not?\nType your answer here, replacing this text.\nSOLUTION\nYes, the probability seems to be monotonic in income, because the proportion of districts with a math gap favoring boys increases with income. The observed pattern may not be perfectly monotonic from point to point, but the general pattern should be increasing, and within the variability bands there will exist some relationship that is strictly monotonic."
  },
  {
    "objectID": "labs/lab7-classification/lab7-classification-soln.html#model-fitting",
    "href": "labs/lab7-classification/lab7-classification-soln.html#model-fitting",
    "title": "Lab 7: Classification",
    "section": "Model fitting",
    "text": "Model fitting\nWe’ll fit a simple model of the probility that the math gap favors boys as a logistic function of log income:\n\\[\n\\log\\left[\\frac{P(\\text{gap favors boys})}{1 - P(\\text{gap favors boys})}\\right] = \\beta_0 + \\beta_1 \\log(\\text{median income})\n\\]\nThe data preparations are exactly the same as in linear regression: we’ll obtain a vector of the response outcome and an explanatory variable matrix containing log median income and a constant (for the intercept).\n\n# explanatory variable matrix\nx = sm.add_constant(train.log_income)\n\n# response\ny = train.favors_boys\n\nThe model is fit using statsmodels.Logit(). Note that the endogenous variable (the response) can be either Boolean (take values True and False) or integer (take values 0 or 1).\n\n# fit the model\nmodel = sm.Logit(endog = y, exog = x)\n\n# store the fitted model\nfit = model.fit()\n\n# display parameter estimates\nfit.params\n\nOptimization terminated successfully.\n         Current function value: 0.590621\n         Iterations 6\n\n\nconst        -25.902434\nlog_income     2.301623\ndtype: float64\n\n\nA coefficient table remains useful for logistic regression:\n\ncoef_tbl = pd.DataFrame(\n    {'estimate': fit.params,\n    'standard error': np.sqrt(fit.cov_params().values.diagonal())},\n    index = x.columns\n)\ncoef_tbl\n\n\n\n\n\n\n\n\nestimate\nstandard error\n\n\n\n\nconst\n-25.902434\n3.159894\n\n\nlog_income\n2.301623\n0.285260\n\n\n\n\n\n\n\n\nQuestion 3: confidence intervals\nCompute 99% confidence intervals for the model parameters. Store the result as a dataframe called param_ci.\nHint: the syntax is identical to that based on sm.OLS; this is also mentioned in the lecture slides.\n\n# compute 99% confidence intervals\nparam_ci = fit.conf_int(alpha = 0.01) # SOLUTION\n\n# display\nparam_ci.rename(columns = {0: 'lwr', 1: 'upr'}, inplace = True)\nparam_ci\n\n\n\n\n\n\n\n\nlwr\nupr\n\n\n\n\nconst\n-34.041781\n-17.763086\n\n\nlog_income\n1.566843\n3.036403\n\n\n\n\n\n\n\n\ngrader.check(\"q3\")\n\nWe can superimpose the predicted probabilities for a fine grid of log median incomes on the data figure we had made previously to compare the fitted model with the observed values:\n\n# grid of log income values\ngrid_df = pd.DataFrame({\n    'log_income': np.linspace(9, 14, 200)\n})\n\n# add predictions\ngrid_df['pred'] = fit.predict(sm.add_constant(grid_df))\n\n# plot predictions against income\nmodel_viz = alt.Chart(grid_df).mark_line(color = 'red', opacity = 0.5).encode(\n    x = 'log_income',\n    y = 'pred'\n)\n\n# superimpose on data figure\ntrend + band + model_viz\n\n\n\n\n\n\nDepending on your training sample, the model may or may not align well with the computed proportions, but it should be mostly or entirely within the 0.4-standard-deviation band.\nTo interpret the estimated relationship, recall that if median income is doubled, the log-odds changes by:\n\\[\n\\hat{\\beta}_1\\log(2\\times\\text{median income}) - \\hat{\\beta}_1 \\log(\\text{median income}) = \\hat{\\beta}_1 \\log(2)\n\\]\nNow, exponentiating gives the estimated multiplicative change in odds:\n\\[\n\\exp\\left\\{\\log(\\text{baseline odds}) + \\hat{\\beta}_1 \\log(2)\\right\\} = \\text{baseline odds} \\times e^{\\hat{\\beta}_1 \\log(2)}\n\\]\nSo computing \\(e^{\\hat{\\beta}_1 \\log(2)}\\) gives a quantity we can readily interpret:\n\nnp.exp(fit.params.log_income*np.log(2))\n\n4.9301197639682846\n\n\nThe exact number will depend a little bit on the data partition you used to compute the estimate, but the answer should be roughly consistent with the following interpretation:\n\nEach doubling of median income is associated with an estimated four-fold increase in the odds that a school district has a math gap favoring boys."
  },
  {
    "objectID": "labs/lab7-classification/lab7-classification-soln.html#classification",
    "href": "labs/lab7-classification/lab7-classification-soln.html#classification",
    "title": "Lab 7: Classification",
    "section": "Classification",
    "text": "Classification\nNow we’ll consider the task of classifying new school districts by the predicted direction of their math achievement gap.\nA straightforward classification rule would be:\n\\[\n\\text{gap predicted to favor boys} \\quad\\Longleftrightarrow\\quad \\widehat{Pr}(\\text{gap favors boys}) &gt; 0.5\n\\]\nWe can obtain the estimated probabilities using .predict(), and construct the classifier manually. To assess the accuracy, we’ll want to arrange the classifications side-by-side with the observed outcomes:\n\n# compute predicted probabilities on test set\npreds = fit.predict(sm.add_constant(test.log_income))\n\n# construct classifier\npred_df = pd.DataFrame({\n    'observation': test.favors_boys,\n    'prediction': preds &gt; 0.5\n})\n\n# preview\npred_df.head()\n\n\n\n\n\n\n\n\nobservation\nprediction\n\n\n\n\n261\nFalse\nFalse\n\n\n600\nFalse\nFalse\n\n\n190\nFalse\nFalse\n\n\n334\nFalse\nFalse\n\n\n686\nFalse\nFalse\n\n\n\n\n\n\n\nNote that the testing partition was used here – to get an unbiased estimate of the classification accuracy, we need data that were not used in fitting the model.\nCross-tabulating observed and predicted outcomes gives a detailed view of the accuracy and error:\n\n# cross-tabulate classifications with observed outcomes\npred_tbl = pred_df.groupby(['observation', 'prediction']).size()\npred_tbl\n\nobservation  prediction\nFalse        False         56\n             True          10\nTrue         False         15\n             True          19\ndtype: int64\n\n\nThe entries where observation and prediction have the same value are counts of the number of districts correctly classified; those where they do not match are counts of errors.\n\nQuestion 4: overall classification accuracy\nCompute the overall classification accuracy – the proportion of districts that were correctly classified.\n\naccuracy = np.mean(pred_df.observation == pred_df.prediction) # SOLUTION\n\n\ngrader.check(\"q4\")\n\nOften class-wise accuracy rates are more informative, because there are two possible types of error:\n\nA district that has a math gap favoring girls is classified as having a math gap favoring boys\nA district that has a math gap favoring boys is classified as having a math gap favoring girls\n\nYou may notice that there were more errors of one type than another in your result above. This is not conveyed by reporting the overall accuracy rate.\nFor a clearer picture, we can find the proportion of errors among by outcome:\n\npred_df['error'] = (pred_df.observation != pred_df.prediction)\nfnr = pred_df[pred_df.observation == True].error.mean()\nfpr = pred_df[pred_df.observation == False].error.mean()\ntpr = 1 - fpr\ntnr = 1 - fnr\n\nprint('false positive rate: ', fpr)\nprint('false negative rate: ', fnr)\nprint('true positive rate (sensitivity): ', tpr)\nprint('true negative rate (specificity): ', tnr)\n\nfalse positive rate:  0.15151515151515152\nfalse negative rate:  0.4411764705882353\ntrue positive rate (sensitivity):  0.8484848484848485\ntrue negative rate (specificity):  0.5588235294117647\n\n\n\n\nQuestion 5: make your own classifier\nDefine a new classifier by adjusting the probability threshold. Compute and print the false positive, false negative, true positive, and true negative rates. Experiment until you achieve a better balance between errors of each type.\n\n# construct classifier\nnew_pred_df = pd.DataFrame({\n    'observation': test.favors_boys, # SOLUTION\n    'prediction': preds &gt; 0.39 # SOLUTION\n})\n\n# compute error rates\nnew_pred_df['error'] = (new_pred_df.observation != new_pred_df.prediction) # SOLUTION\nnew_fnr = new_pred_df[new_pred_df.observation == True].error.mean() # SOLUTION\nnew_fpr = new_pred_df[new_pred_df.observation == False].error.mean() # SOLUTION\nnew_tpr = 1 - new_fpr # SOLUTION\nnew_tnr = 1 - new_fnr # SOLUTION\n\n# print\nprint('false positive rate: ', new_fpr)\nprint('false negative rate: ', new_fnr)\nprint('true positive rate (sensitivity): ', new_tpr)\nprint('true negative rate (specificity): ', new_tnr)\n\nfalse positive rate:  0.3484848484848485\nfalse negative rate:  0.38235294117647056\ntrue positive rate (sensitivity):  0.6515151515151515\ntrue negative rate (specificity):  0.6176470588235294\n\n\n\ngrader.check(\"q5\")"
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization.html",
    "href": "labs/lab3-visualization/lab3-visualization.html",
    "title": "Lab 3: Data visualization",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab3-visualization.ipynb\")\nimport pandas as pd\nimport numpy as np\nimport altair as alt\n# disable row limit for plotting\nalt.data_transformers.disable_max_rows()\n# uncomment to ensure graphics display with pdf export\n# alt.renderers.enable('mimetype')\nData visualizations are graphics that represent quantitative or qualitative data. In PSTAT100 you’ll be using the python visualization library Altair, which is built around the pandas dataframe. Altair creates visualizations by mapping columns of a dataframe to the various elements of a graphic: axes, geometric objects, and aesthetics.\nVisualizations are essential tools in exploratory analysis as well as presentation. They can help an analyst identify and understand structure and patterns in a dataset at a high level and provide guidance for model development. They can be used to check assumptions and visualize model outputs. And they can be an effective means for conveying results to a general audience.\nConstructing effective visualization is usually an iterative process: plot-think-revise-repeat. In exploratory visualization often it is useful to produce a large quantity of plots in order to look at data from multiple angles; in this context, speed is helpful and details can be overlooked. By contrast, presentation graphics are typically highly refined versions of one or two exploratory plots that serve as communication tools; developing them involves attention to fine detail.\nObjectives\nIn this lab you’ll become familiar with the basic functionality of Altair – the basic kinds of graphics it generates and how to construct these graphics from a dataframe – and get a taste of the process of constructing good graphics.\nIn Altair, plots are constructed by: 1. creating a chart 2. specifying marks and encodings 3. adding various aesthetics, and 4. resolving display issues through customization.\nTechnical tutorial. You’ll get an introduction to each of these steps: * Creating a chart object from a dataframe * Encodings: mapping columns to graphical elements * Marks: geometric objects displayed on a plot (e.g., points, lines, polygons) * Aesthetics: display attributes of geometric objects (e.g., color, shape, transparency) * Customization: adjusting axes, labels, scales.\nVisualization process. In addition, our goal is to model for you the process of constructing a good visualization through iterative revisions. * Identifying and fixing display problems * Discerning informative from non-informative graphical elements * Designing efficient displays"
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization.html#axes",
    "href": "labs/lab3-visualization/lab3-visualization.html#axes",
    "title": "Lab 3: Data visualization",
    "section": "Axes",
    "text": "Axes\nAxes establish a reference system for a graphic: they define a space within which the graphic will be constructed. Usually these are coordinate systems defined at a particular scale, like Cartesian coordinates on the region (0, 100) x (0, 100), or polar coordinates on the unit circle, or geographic coordinates for the globe.\nIn Altair, axes are automatically determined based on encodings, but are customizable to an extent."
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization.html#geometric-objects",
    "href": "labs/lab3-visualization/lab3-visualization.html#geometric-objects",
    "title": "Lab 3: Data visualization",
    "section": "Geometric objects",
    "text": "Geometric objects\nGeometric objects are any objects superimposed on a set of axes: points, lines, polygons, circles, bars, arcs, curves, and the like. Often, visualizations are characterized according to the type of object used to display data – for example, the scatterplot consists of points, a bar plot consists of bars, a line plot consists of one or more lines, and so on.\nIn Altair, geometric objects are called marks."
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization.html#aesthetic-attributes",
    "href": "labs/lab3-visualization/lab3-visualization.html#aesthetic-attributes",
    "title": "Lab 3: Data visualization",
    "section": "Aesthetic attributes",
    "text": "Aesthetic attributes\nThe word ‘aesthetics’ is used in a variety of ways in relation to graphics; you will see this in your reading. For us, ‘aesthetic attirbutes’ will refer to attributes of geometric objects like color. The primary aesthetics in statistical graphics are color, opacity, shape, and size.\nIn Altair, aesthetic attributes are called mark properties."
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization.html#text",
    "href": "labs/lab3-visualization/lab3-visualization.html#text",
    "title": "Lab 3: Data visualization",
    "section": "Text",
    "text": "Text\nText is used in graphics to label axes, geometric objects, and legends for aesthetic mappings. Text specification is usually a step in customization for presentation graphics, but often skipped in exploratory graphics. Carefully chosen text is very important in this context, because it provides essential information that a general reader needs to interpret a plot.\nIn Altair, text is usually controlled as part of encoding specification."
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization.html#basic-scatterplots",
    "href": "labs/lab3-visualization/lab3-visualization.html#basic-scatterplots",
    "title": "Lab 3: Data visualization",
    "section": "Basic scatterplots",
    "text": "Basic scatterplots\nThe following cell constructs a scatterplot of life expectancy at birth against GDP per capita; each point corresponds to one country in one year. The syntax works as follows: * alt.Chart() begins by constructing a ‘chart’ object constructed from the dataframe; * the result is passed to .mark_circle(), which specifies a geometric object (circles) to add to the chart; * the result is passed to .encode(), which specifies which columns should be used to determine the coordinates of the circles.\n\n# basic scatterplot\nalt.Chart(data).mark_circle().encode(\n    x = 'GDP per capita',\n    y = 'Life Expectancy'\n)\n\n\n\nQuestion 1: Different marks\nThe cell below is a copy of the previous cell. Have a look at the documentation on marks for a list of the possible mark types. Try out a few alternatives to see what they look like. Once you’re satisfied, change the mark to points.\n\n\n# basic scatterplot\nalt.Chart(data).mark_circle().encode( # tinker here with different marks\n    x = 'GDP per capita',\n    y = 'Life Expectancy'\n)\n\n\n\n\n\nQuestion 2: Mark properties\nWhat is the difference between points and circles, according to the documentation?\nType your answer here, replacing this text."
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization.html#axis-adjustments-with-alt.x-and-alt.y",
    "href": "labs/lab3-visualization/lab3-visualization.html#axis-adjustments-with-alt.x-and-alt.y",
    "title": "Lab 3: Data visualization",
    "section": "Axis adjustments with alt.X() and alt.Y()",
    "text": "Axis adjustments with alt.X() and alt.Y()\nAn initial problem that would be good to resolve before continuing is that the y axis label isn’t informative. Let’s change that by wrapping the column to encode in alt.Y() and specifying the title manually.\n\n# change axis label\nalt.Chart(data).mark_circle().encode(\n    x = 'GDP per capita',\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth')\n)\n\nalt.Y() and alt.X() are helper functions that modify encoding specifications. The cell below adjusts the scale of the y axis as well; since above there are no life expectancies below 30, starting the y axis at 0 adds whitespace.\n\n# don't start y axis at zero\nalt.Chart(data).mark_circle().encode(\n    x = 'GDP per capita',\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False))\n)\n\nIn the plot above, there are a lot of points squished together near \\(x = 0\\). It will make it easier to see the pattern of scatter in that region to adjust the x axis so that values are not displayed on a linear scale. Using alt.Scale() allows for efficient axis rescaling; the cell below puts GDP per capita on a log scale.\n\n# log scale for x axis\nalt.Chart(data).mark_circle().encode(\n    x = alt.X('GDP per capita', scale = alt.Scale(type = 'log')),\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False))\n)\n\n\n\nQuestion 3: Changing axis scale\nTry a different scale by modifying the type = ... argument of alt.Scale in the cell below. Look at the altair documentation for a list of the possible types.\n\n# try another axis scale\nalt.Chart(data).mark_circle().encode(\n    x = ...\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False))\n)"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing.html",
    "href": "labs/lab4-smoothing/lab4-smoothing.html",
    "title": "Lab 4: Smoothing",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab4-smoothing.ipynb\")\nimport numpy as np\nimport pandas as pd\nimport altair as alt\npd.options.mode.chained_assignment = None  # default='warn'\n# disable row limit for plotting\nalt.data_transformers.disable_max_rows()\n# uncomment to ensure graphics display with pdf export\n# alt.renderers.enable('mimetype')\nSo far, you’ve encountered a number of visualization techniques for displaying tidy data. In those visualizations, all graphic elements represent the values of a dataset – they are visual displays of actual data.\nIn general, smoothing means evening out. Visualizations of actual data are often irregular – points are distributed widely in scatterplots, line plots are jagged, bars are discontinuous. When we look at such visuals, we tend to attempt to look past these irregularities in order to discern patterns – for example, the overall shape of a histogram or the general trend in a scatterplot. Showing what a graphic might look like with irregularities evened out often aids the eye in detecting pattern. This is what smoothing is: evening out irregularities in graphical displays of actual data.\nFor our purposes, usually smoothing will consist in drawing a line or a curve on top of an existing statistical graphic. From a technical point of view, this amounts to adding derived geometric objects to a graphic that have fewer irregularities than the displays of actual data.\nIn this lab, you’ll learn some basic smoothing techniques – kernel density estimation, LOESS, and linear smoothing via regression – and how to implement them in Altair.\nIn Altair, smoothing is implemented via what Altair describes as transforms – operations that modify a dataset. Try not to get too attached to this terminology – ‘transform’ and ‘transformation’ are used to mean a variety of things in other contexts. You’ll begin with a brief introduction to Altair transforms before turning to smoothing techniques.\nThe sections of the lab are divided as follows:\nAnd our main goals are:\nYou’ll use the same data as last week to stick to a familiar example:\n# import tidied lab 3 data\ndata = pd.read_csv('data/lab3-data.csv')\ndata.head()"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing.html#filter-transform",
    "href": "labs/lab4-smoothing/lab4-smoothing.html#filter-transform",
    "title": "Lab 4: Smoothing",
    "section": "Filter transform",
    "text": "Filter transform\nLast week you saw a way to make histograms. As a quick refresher, to make a histogram of life expectancies across the globe in 2010, one can filter the data and then plot using the following commands:\n\n# filter\ndata2010 = data[data.Year == 2010]\n\n# plot\nalt.Chart(data2010).mark_bar().encode(\n    x = alt.X('Life Expectancy', \n              bin = alt.Bin(step = 2), \n              title = 'Life Expectancy at Birth'),\n    y = 'count()'\n)\n\nHowever, the filtering step can be handled within the plotting commands using .transform_filter().\nThis uses a helper command to specify the filtering condition – in the above example, the filtering condition is that Year is equal to 2010. A filtering condition is referred to in Altair as a ‘field predicate’. In the above example: * filtering field: Year * field predicate: equals 2010\nThere are different helpers for different types of field predicates – you can find a complete list in the documentation.\nHere is how to use .transform_filter() to make the same histogram shown above, but skipping the step of storing a subset of the data under a separate name:\n\n# filter and plot\nalt.Chart(data).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', \n                            equal = 2010)\n).mark_bar().encode(\n    x = alt.X('Life Expectancy', \n              bin = alt.Bin(step = 2), \n              title = 'Life Expectancy at Birth'),\n    y = 'count()'\n)\n\n\n\nQuestion 1: Filter transform\nConstruct a histogram of life expectancies across the globe in 2019 using a filter transform as shown above to filter the appropriate rows of the dataset. Use a bin size of three (not two) years.\n\n# filter and plot\nalt.Chart(data).transform_filter(\n    ...\n).mark_bar().encode(\n    x = ...\n    y = ...\n)"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing.html#bin-transform",
    "href": "labs/lab4-smoothing/lab4-smoothing.html#bin-transform",
    "title": "Lab 4: Smoothing",
    "section": "Bin transform",
    "text": "Bin transform\nThe codes above provide a sleek way to construct the histogram that handles binning via arguments to alt.X(...). However, binning actually involves an operation: creating a new variable that is a discretization of an existing variable into contiguous intervals of a specified width.\nTo illustrate, have a look at how the histogram could be constructed ‘manually’ by the following operations. 1. Bin life expectancies 2. Count values in each bin 3. Make a bar plot of counts against bin centers.\nHere’s step 1:\n\n# bin life expectancies into 20 contiguous intervals\ndata2010['Bin'] = pd.cut(data2010[\"Life Expectancy\"], bins = 20)\ndata2010.head()\n\nHere’s step 2:\n\n# count values in each bin and store midpoints\nhistdata = data2010.loc[:, ['Life Expectancy', 'Bin']].groupby('Bin').count()\nhistdata['Bin midpoint'] = histdata.index.values.categories.mid.values\nhistdata\n\nAnd finally, step 3:\n\n# plot histogram\nalt.Chart(histdata).mark_bar(width = 10).encode(\n    x = 'Bin midpoint',\n    y = alt.Y('Life Expectancy', title = 'Count')\n)\n\nThese operations can be articulated as a transform in Altair using .bin_transform():\n\n# filter, bin, and plot\nalt.Chart(\n    data\n).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', \n                            equal = 2010)\n).transform_bin(\n    'Life Expectancy at Birth', # name to give binned variable\n    field = 'Life Expectancy', # variable to bin\n    bin = alt.Bin(step = 2) # binning parameters\n).mark_bar(size = 10).encode(\n    x = 'Life Expectancy at Birth:Q', \n    y = 'count()'\n)\n\nThe plotting codes are a little more verbose, but they’re much more efficient than performing the manipulations separately in pandas.\n\n\nQuestion 2: Bin transform\nFollow the example above and make a histogram of life expectancies across the globe in 2019 using an explicit bin transform to create bins spanning three years.\n\n# filter, bin, and plot\nalt.Chart(\n    data\n).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', \n                            equal = 2019)\n).transform_bin(\n    ...\n    field = ...\n    bin = ...\n).mark_bar(size = 10).encode(\n    x = ...\n    y = 'count()'\n)"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing.html#aggregate-transform",
    "href": "labs/lab4-smoothing/lab4-smoothing.html#aggregate-transform",
    "title": "Lab 4: Smoothing",
    "section": "Aggregate transform",
    "text": "Aggregate transform\nNow, the counting of observations in each bin (implemented via y = count()) is also an under-the-hood operation in constructing the histogram. You already saw how this was done ‘manually’ in the example above before introducing the bin transform.\nGrouped counting is a form of aggregation in the sense discussed in lecture: it produces output that has fewer values than the input by combining multiple values (in this case rows) into one value (in this case a count of the number of rows).\nThis operation can also be made explicit using .transform_aggregate(). This makes use of Altair’s aggregation shorthands for common aggregation functions; see the documentation on Altair encodings for a full list of shorthands.\nHere is how .transform_aggregate() would be used to perform the counting:\n\n# filter, bin, count, and plot\nalt.Chart(\n    data\n).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', \n                            equal = 2010)\n).transform_bin(\n    'Life Expectancy at Birth', \n    field = 'Life Expectancy',\n    bin = alt.Bin(step = 2) \n).transform_aggregate(\n    Count = 'count()', # altair shorthand operation -- see docs for full list\n    groupby = ['Life Expectancy at Birth'] # grouping variable(s)\n).mark_bar(size = 10).encode(\n    x = 'Life Expectancy at Birth:Q', \n    y = 'Count:Q'\n)"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing.html#calculate-transform",
    "href": "labs/lab4-smoothing/lab4-smoothing.html#calculate-transform",
    "title": "Lab 4: Smoothing",
    "section": "Calculate transform",
    "text": "Calculate transform\nBy default, Altair’s histograms are displayed on the count scale rather than the density scale.\nThe count scale means that the y-axis shows counts of observations in each bin.\nBy contrast, on the density scale, the y-axis would show proportions of total bar area (so that the area of plotted bars sums to 1).\nIt might seem like a silly distinction – after all, the two scales differ simply by a proportionality constant (the sample size times the bin width) – but as you will see shortly, the density scale is more useful for statistical thinking about the distribution of values and for direct comparisons of distributions approximated from samples of different sizes.\nThe scale conversion can be done using .transform_calculate(), which computes derived variables using arithmetic operations. In this case, one only needs to divide the count by the total number of observations.\n\n# filter, bin, count, convert scale, and plot\nalt.Chart(\n    data\n).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', \n                            equal = 2010)\n).transform_bin(\n    'Life Expectancy at Birth', \n    field = 'Life Expectancy', \n    bin = alt.Bin(step = 2)\n).transform_aggregate(\n    Count = 'count()',\n    groupby = ['Life Expectancy at Birth']\n).transform_calculate(\n    Density = 'datum.Count/(2*157)' # divide counts by sample size x binwidth\n).mark_bar(size = 10).encode(\n    x = 'Life Expectancy at Birth:Q', \n    y = 'Density:Q'\n)\n\n\nQuestion 3: Density scale histogram\nFollow the example above and convert your histogram from Question 2 (with the year 2019, the step size of 3, and the usage of .transform_bin(...)) to the density scale.\n\n\nFirst, calculate the sample size and store the value as sample_size. Store the desired step size as bin_width.\n\n\nConvert your histogram from Question 2 (with the year 2019, the step size of 3, and the usage of .transform_bin(...)) to the density scale. First calculate the count explicitly using .transform_aggregate(...) and then convert to a proportion using .transform_calculate(...). Multiply sample_size with bin_width to obtain the scaling constant and hardcode it into your implementation.\n\n\n\n# find scaling factor\nsample_size = ...\nbin_width = ...\nprint('scaling factor = ', sample_size*bin_width)\n\n# construct histogram\nalt.Chart(data).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', \n                            equal = 2019)\n).transform_bin(\n    ...\n    field = ...\n    bin = ...\n).transform_aggregate(\n    Count = ...\n    groupby = ...\n).transform_calculate(\n    # use sample_size*bin_width to rescale - you will need to hardcode this value\n    Density = ...\n).mark_bar(size = 20).encode(\n    x = 'Life Expectancy at Birth:Q', # SOLUTIOON\n    y = ...\n)\n\n\ngrader.check(\"q3\")"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing.html#comparing-distributions",
    "href": "labs/lab4-smoothing/lab4-smoothing.html#comparing-distributions",
    "title": "Lab 4: Smoothing",
    "section": "Comparing distributions",
    "text": "Comparing distributions\nThe visual advantage of a kernel density estimate for discerning shape is even more apparent when comparing distributions.\nA major task in exploratory analysis is understanding how the distribution of a variable of interest changes depending on other variables – for example, you have already seen in the last lab that life expectancy seems to change over time. We can explore this phenomenon from a different angle by comparing distributions in different years.\nMultiple density estimates can be displayed on the same plot by passing a grouping variable (or set of variables) to .transform_density(...). For example, the cell below computes density estimates of life expectancies for each of two years.\n\nalt.Chart(data).transform_filter(\n    alt.FieldOneOfPredicate(field = 'Year',\n                            oneOf = [2010, 2019])\n).transform_density(\n    density = 'Life Expectancy',\n    groupby = ['Year'],\n    as_ = ['Life Expectancy at Birth', 'Estimated Density'],\n    bandwidth = 1.8, \n    extent = [25, 90],\n    steps = 1000 \n).mark_line().encode(\n    x = 'Life Expectancy at Birth:Q',\n    y = 'Estimated Density:Q',\n    color = 'Year:N'\n)\n\nOften the area beneath each density estimate is filled in. This can be done by simply appending a .mark_area() call at the end of the plot.\n\np = alt.Chart(data).transform_filter(\n    alt.FieldOneOfPredicate(field = 'Year',\n                            oneOf = [2010, 2019])\n).transform_density(\n    density = 'Life Expectancy',\n    groupby = ['Year'],\n    as_ = ['Life Expectancy at Birth', 'Estimated Density'],\n    bandwidth = 1.8, \n    extent = [25, 90],\n    steps = 1000 \n).mark_line().encode(\n    x = 'Life Expectancy at Birth:Q',\n    y = 'Estimated Density:Q',\n    color = 'Year:N'\n)\n\np + p.mark_area(opacity = 0.1)\n\nNotice that this makes it much easier to compare the distributions between years – you can see a pronounced rightward shift of the smooth for 2019 compared with 2010.\nWe could make the same comparison based on the histograms, but the shift is a lot harder to make out. Overlaid histograms should be avoided.\n\nalt.Chart(data).transform_filter(\n    alt.FieldOneOfPredicate(field = 'Year',\n                            oneOf = [2010, 2019])\n).mark_bar(opacity = 0.5).encode(\n    x = alt.X('Life Expectancy', bin = alt.Bin(maxbins = 30), title = 'Life Expectancy at Birth'),\n    y = alt.Y('count()', stack = None),\n    color = 'Year:N'\n)\n\n\n\nQuestion 5: Multiple density estimates\nFollow the example above to construct a plot showing separate density estimates of life expectancy for each region in the 2010. You can choose whether you prefer to fill in the area beneath the smooth curves, or not. Be sure to play with the bandwidth parameter and choose a value that seems sensible to you.\n\n# construct density estimates\np = alt.Chart(data).transform_filter(\n    ...\n).transform_density(\n    density = ...\n    groupby = ...\n    as_ = ...\n    bandwidth = ...\n    extent = ...\n    steps = ...\n).mark_line(\n).encode(\n    x = ...\n    y = ...\n    color = ...\n)\n\n# add shaded area underneath curves\n...\n\n\n\n\n\nQuestion 6: Interpretation\nDo the distributions of life expectancies seem to differ by region? If so, what is one difference that you notice? Answer in 1-2 sentences.\nType your answer here, replacing this text.\n\n\n\nQuestion 7: Outlier\nNotice that little peak way off to the left in the distribution of life expectancies in the Americas. That’s an outlier.\n\n\nWhich country is it? Check by filtering data appropriately and using .sort_values(...) to find the lowest life expectancy in the Americas. Save the outlying observation as a one-row dataframe called lowest_Americas and print the row.\n\n\nWhat was the life expectancy for that country in other years? Filter the data to examine the life expectancy in the country you identified as the outlier in all y. Save the resulting data frame as outlier_country.\n\n\nWhat Happened in 2010? Can you explain why the life expectancy was so low in that country for that particular year?(Hint: if you don’t remember, Google the country name and year in question.)\n\n\nType your answer here, replacing this text.\n\n# examine outlier\nlowest_Americas = data[\n    ...\n    ].sort_values(\n    by = ...\n    ).head(1)\nlowest_Americas\n\n\n# show all obsrvations for country of interest\noutlier_country = ...\noutlier_country\n\n\ngrader.check(\"q7\")"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing.html#loess",
    "href": "labs/lab4-smoothing/lab4-smoothing.html#loess",
    "title": "Lab 4: Smoothing",
    "section": "LOESS",
    "text": "LOESS\nLocally weighted scatterplot smoothing (LOESS) is a flexible smoothing technique for visualizing trends in scatterplots. The technical details are a little involved but quite similar conceptually to kernel density estimation; we’ll just look at the implementation for now.\nTo illustrate, consider the scatterplots you made in lab 3 showing the relationship between life expectancy and GDP per capita. The plot for 2010 looked like this:\n\n# log transform gdp explicitly\ndata_mod1['log(GDP per capita)'] = np.log(data_mod1['GDP per capita'])\n\n# scatterplot\nscatter = alt.Chart(data_mod1).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', equal = 2000)\n).mark_circle(opacity = 0.5).encode(\n    x = alt.X('log(GDP per capita)', scale = alt.Scale(zero = False)),\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False)),\n    size = alt.Size('Population', scale = alt.Scale(type = 'sqrt'))\n)\n\n# show\nscatter\n\nTo add a LOESS curve, simply append .transform_loess() to the base plot:\n\n# compute smooth\nsmooth = scatter.transform_loess(\n    on = 'log(GDP per capita)', # x variable\n    loess = 'Life Expectancy', # y variable\n    bandwidth = 0.25 # how smooth?\n).mark_line(color = 'black')\n\n# add as a layer to the scatterplot\nscatter + smooth\n\nJust as with kernel density estimates, LOESS curves have a bandwidth parameter that controls how smooth or wiggly the curve is. In Altair, the LOESS bandwidth is a unitless parameter between 0 and 1.\n\n\nQuestion 8: LOESS bandwidth selection\nTinker with the bandwidth parameter to see its effect in the cell below. Then choose a value that produces a smoothing you find appropriate for indicating the general trend shown in the scatter.\n\n# compute smooth\nsmooth = scatter.transform_loess(\n    on = 'log(GDP per capita)', \n    loess = 'All', \n    bandwidth = ...\n).mark_line(color = 'black')\n\n# add as a layer to the scatterplot\nscatter + smooth\n\n\nLOESS curves can also be computed groupwise. For instance, to display separate curves for each region, one need only pass a groupby = ... argument to .transform_loess():\n\n# scatterplot\nscatter = alt.Chart(data_mod1).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', equal = 2000)\n).mark_circle(opacity = 0.5).encode(\n    x = alt.X('log(GDP per capita)', scale = alt.Scale(zero = False)),\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False)),\n    size = alt.Size('Population', scale = alt.Scale(type = 'sqrt')),\n    color = 'region'\n)\n\n# compute smooth\nsmooth = scatter.transform_loess(\n    groupby = ['region'], # add groupby\n    on = 'log(GDP per capita)', \n    loess = 'Life Expectancy', \n    bandwidth = 0.8 \n).mark_line(color = 'black')\n\n# add as a layer to the scatterplot\nscatter + smooth\n\nThe curves are a little jagged because there aren’t very many countries in each region.\n\ndata_mod1[data_mod1.Year == 2000].groupby('region').count().iloc[:, [0]]"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing.html#regression",
    "href": "labs/lab4-smoothing/lab4-smoothing.html#regression",
    "title": "Lab 4: Smoothing",
    "section": "Regression",
    "text": "Regression\nYou will be learning more about linear regression later in the course, but we can introduce regression lines now as a visualization technique. As with LOESS, you don’t need to concern yourself with the mathematical details (yet). From this perspective, regression is a form of linear smoothing – a regression smooth is a straight line. By contrast, LOESS smooths have curvature – they are not straight lines.\nIn the example above, the LOESS curves don’t have much curvature. So it may be a cleaner choice visually to show linear smooths. This can be done using .transform_regression(...) with a similar argument structure.\n\n# scatterplot\nscatter = alt.Chart(data_mod1).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', equal = 2000)\n).mark_circle(opacity = 0.5).encode(\n    x = alt.X('log(GDP per capita)', scale = alt.Scale(zero = False)),\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False)),\n    size = alt.Size('Population', scale = alt.Scale(type = 'sqrt')),\n    color = 'region'\n)\n\n# compute smooth\nsmooth = scatter.transform_regression(\n    groupby = ['region'],\n    on = 'log(GDP per capita)', \n    regression = 'Life Expectancy'\n).mark_line(color = 'black')\n\n# add as a layer to the scatterplot\nscatter + smooth\n\n\n\nQuestion 9: Simple regression line\nBased on the example immediately above, construct a scatterplot of life expectancy against log GDP per capita in 2010 with points sized according to population (and no distinction between regions). Layer a single linear smooth on the scatterplot using .transform_regression(...).\n(Hint: remove the color aesthetic and grouping from the previous plot.)\n\n# construct scatterplot\nscatter = alt.Chart(data_mod1).transform_filter(\n    ...\n).mark_circle(opacity = 0.5).encode(\n    x = ...\n    y = ...\n    size = ...\n)\n\n# construct smooth\nsmooth = scatter.transform_regression(\n    on = ...\n    regression = ...\n).mark_line(color = 'black')\n\n# layer\n..."
  },
  {
    "objectID": "labs/lab5-pca/lab5-pca.html",
    "href": "labs/lab5-pca/lab5-pca.html",
    "title": "Lab 5: Principal components",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab5-pca.ipynb\")\nimport numpy as np\nimport pandas as pd\nimport altair as alt\nfrom scipy import linalg\nfrom statsmodels.multivariate.pca import PCA\n# disable row limit for plotting\nalt.data_transformers.disable_max_rows()\n# uncomment to ensure graphics display with pdf export\n# alt.renderers.enable('mimetype')\nPrincipal components analysis (PCA) is a widely-used multivariate analysis technique. Depending on the application, PCA is variously described as:\nThe core technique of PCA is finding linear data transformations that preserve variance.\nWhat does it mean to say that ‘principal components are linear data transformations’? Suppose you have a dataset with \\(n\\) observations and \\(p\\) variables. We can represent the values as a data matrix \\(\\mathbf{X}\\) with \\(n\\) rows and \\(p\\) columns:\n\\[\n\\mathbf{X}\n= \\underbrace{\\left[\\begin{array}{cccc}\n    \\mathbf{x}_1 &\\mathbf{x}_2 &\\cdots &\\mathbf{x}_p\n    \\end{array}\\right]}_{\\text{column vectors}}\n= \\left[\\begin{array}{cccc}\n    x_{11} &x_{12} &\\cdots &x_{1p} \\\\\n    x_{21} &x_{22} &\\cdots &x_{2p} \\\\\n    \\vdots &\\vdots &\\ddots &\\vdots \\\\\n    x_{n1} &x_{n2} &\\cdots &x_{np}\n\\end{array}\\right]\n\\]\nTo say that the principal components are linear data transformations means that each principal component is of the form:\n\\[\n\\text{PC} = \\mathbf{Xv} = v_1 \\mathbf{x}_1 + v_2 \\mathbf{x}_2 + \\cdots + v_p \\mathbf{x}_p\n\\]\nfor some vector \\(\\mathbf{v}\\). In PCA, the following terminology is used:\nAs discussed in lecture, the values of the loadings are found by decomposing the correlation structure.\nObjectives\nIn this lab, you’ll focus on computing and interpreting principal components:\nYou’ll work with a selection of county summaries from the 2010 U.S. census. The first few rows of the dataset are shown below:\n# import tidy county-level 2010 census data\ncensus = pd.read_csv('data/census2010.csv', encoding = 'latin1')\ncensus.head()\nThe observational units are U.S. counties, and each row is an observation on one county. The values are, for the most part, percentages of the county population. You can find variable descriptions in the metadata file census2010metadata.csv in the data directory."
  },
  {
    "objectID": "labs/lab5-pca/lab5-pca.html#loadings-and-scores",
    "href": "labs/lab5-pca/lab5-pca.html#loadings-and-scores",
    "title": "Lab 5: Principal components",
    "section": "Loadings and scores",
    "text": "Loadings and scores\nIn statsmodels, the module multivariate.pca contains an easy-to-use implementation.\n\n# compute principal components\npca = PCA(data = x_mx, standardize = True) \n\nMost quantities you might want to use in PCA can be retrieved as attributes of pca. In particular:\n\n.loadings contains the loadings\n.scores contains the scores\n.eigenvals contains the variances along each principal axis (see lecture notes)\n\nExamine the loadings below. Each column gives the loadings for one principal component; components are ordered from largest to smallest variance.\n\n# inspect loadings\npca.loadings\n\nSimilarly, inspect the scores below and check your understanding; each row is an observation and the columns give the scores on each principal axis.\n\n# inspect scores\npca.scores\n\nImportantly, statsmodels rescales the scores so that they have unit inner product; in other words, so that the variances are all \\(\\frac{1}{n - 1}\\).\n\n# variance of scores\npca.scores.var()\n\n\n# for comparison\n1/(x_mx.shape[0] - 1)\n\nTo change this behavior, set normalize = False when computing the principal components.\n\nQuestion 3\nCheck your understanding. Which variable contributes most to the sixth principal component? Store the variable name exactly as it appears among the original column names as pc6_most_influential_variable, and store the corresponding loading as pc6_most_influential_variable_loading. Print the variable name.\n\n# find most influential variable\npc6_most_influential_variable = ...\n\n# find loading\npc6_most_influential_variable_loading = ...\n\n# print\n...\n\n\ngrader.check(\"q3\")"
  },
  {
    "objectID": "labs/lab5-pca/lab5-pca.html#variance-ratios",
    "href": "labs/lab5-pca/lab5-pca.html#variance-ratios",
    "title": "Lab 5: Principal components",
    "section": "Variance ratios",
    "text": "Variance ratios\nThe variance ratios indicate the proportions of total variance in the data captured by each principal axis. You may recall from lecture that the variance ratios are computed from the eigenvalues of the correlation (or covariance, if data are not standardized) matrix.\nWhen using statsmodels, these need to be computed manually.\n\n# compute variance ratios\nvar_ratios = pca.eigenvals/pca.eigenvals.sum()\n\n# print\nvar_ratios\n\nNote again that the principal components have been computed in order of decreasing variance.\n\n\nQuestion 4\nCheck your understanding. What proportion of variance is captured jointly by the first three components taken together? Provide a calculation to justify your answer.\nType your answer here, replacing this text.\n\n..."
  },
  {
    "objectID": "labs/lab5-pca/lab5-pca.html#selecting-a-subset-of-pcs",
    "href": "labs/lab5-pca/lab5-pca.html#selecting-a-subset-of-pcs",
    "title": "Lab 5: Principal components",
    "section": "Selecting a subset of PCs",
    "text": "Selecting a subset of PCs\nPCA generally consists of choosing a small subset of components. The basic strategy for selecting this subset is to determine how many are needed to capture some analyst-chosen minimum portion of total variance in the original data.\nMost often this assessment is made graphically by inspecting the variance ratios and their cumulative sum, i.e., the amount of total variation captured jointly by subsets of successive components. We’ll store these quantities in a data frame.\n\n# store proportion of variance explained as a dataframe\npca_var_explained = pd.DataFrame({\n    'Component': np.arange(1, 23),\n    'Proportion of variance explained': var_ratios})\n\n# add cumulative sum\npca_var_explained['Cumulative variance explained'] = var_ratios.cumsum()\n\n# print\npca_var_explained.head()\n\nNow we’ll make a dual-axis plot showing, on one side, the proportion of variance explained (y) as a function of component (x), and on the other side, the cumulative variance explained (y) also as a function of component (x). Make sure that you’ve completed Q1(a) before running the next cell.\n\n# encode component axis only as base layer\nbase = alt.Chart(pca_var_explained).encode(\n    x = 'Component')\n\n# make a base layer for the proportion of variance explained\nprop_var_base = base.encode(\n    y = alt.Y('Proportion of variance explained',\n              axis = alt.Axis(titleColor = '#57A44C'))\n)\n\n# make a base layer for the cumulative variance explained\ncum_var_base = base.encode(\n    y = alt.Y('Cumulative variance explained', axis = alt.Axis(titleColor = '#5276A7'))\n)\n\n# add points and lines to each base layer\nprop_var = prop_var_base.mark_line(stroke = '#57A44C') + prop_var_base.mark_point(color = '#57A44C')\ncum_var = cum_var_base.mark_line() + cum_var_base.mark_point()\n\n# layer the layers\nvar_explained_plot = alt.layer(prop_var, cum_var).resolve_scale(y = 'independent')\n\n# display\nvar_explained_plot\n\nThe purpose of making this plot is to quickly determine the fewest number of principal components that capture a considerable portion of variation and covariation. ‘Considerable’ here is a bit subjective.\n\nQuestion 5\nHow many principal components explain more than 6% of total variation individually? Store this number as num_pc, and store the proportion of variation that they capture jointly as var_explained.\n\n# number of selected components\nnum_pc = ...\n\n# variance explained\nvar_explained = ...\n\n#print\nprint('number selected: ', num_pc)\nprint('proportion of variance captured: ', var_explained)\n\n\ngrader.check(\"q5\")"
  },
  {
    "objectID": "labs/lab5-pca/lab5-pca.html#interpreting-loadings",
    "href": "labs/lab5-pca/lab5-pca.html#interpreting-loadings",
    "title": "Lab 5: Principal components",
    "section": "Interpreting loadings",
    "text": "Interpreting loadings\nNow that you’ve chosen the number of components to work with, the next step is to examine loadings to understand just which variables the components combine with significant weight.\nWe’ll store the scores for the components you selected as a dataframe.\n\n# subset loadings\nloading_df = pca.loadings.iloc[:, 0:num_pc]\n\n# rename columns\nloading_df = loading_df.rename(columns = dict(zip(loading_df.columns, ['PC' + str(i) for i in range(1, num_pc + 1)])))\n\n# print\nloading_df.head()\n\nAgain, the loadings are the weights with which the variables are combined to form the principal components. For example, the PC1 column tells us that this component is equal to:\n\\[(-0.020055\\times\\text{women}) + (0.289614\\times\\text{white}) + (0.050698\\times\\text{citizen}) + \\dots\\]\nSince the components together capture over half the total variation, the heavily weighted variables in the selected components are the ones that drive variation in the original data.\nBy visualizing the loadings, we can see which variables are most influential for each component, and thereby also which variables seem to drive total variation in the data.\n\n# melt from wide to long\nloading_plot_df = loading_df.reset_index().melt(\n    id_vars = 'index',\n    var_name = 'Principal Component',\n    value_name = 'Loading'\n).rename(columns = {'index': 'Variable'})\n\n# add a column of zeros to encode for x = 0 line to plot\nloading_plot_df['zero'] = np.repeat(0, len(loading_plot_df))\n\n# create base layer\nbase = alt.Chart(loading_plot_df)\n\n# create lines + points for loadings\nloadings = base.mark_line(point = True).encode(\n    y = alt.X('Variable', title = ''),\n    x = 'Loading',\n    color = 'Principal Component'\n)\n\n# create line at zero\nrule = base.mark_rule().encode(x = alt.X('zero', title = 'Loading'), size = alt.value(0.05))\n\n# layer\nloading_plot = (loadings + rule).properties(width = 120)\n\n# show\nloading_plot.facet(column = alt.Column('Principal Component', title = ''))\n\nLook first at PC1: the variables with the largest loadings (points farthest in either direction from the zero line) are Child Poverty (positive), Employed (negative), Income per capita (negative), Poverty (positive), and Unemployment (positive). We know from exploring the correlation matrix that employment rate, unemployment rate, and income per capita are all related, and similarly child poverty rate and poverty rate are related. Therefore, the positively-loaded variables are all measuring more or less the same thing, and likewise for the negatively-loaded variables.\nEssentially, then, PC1 is predominantly (but not entirely) a representation of income and poverty. In particular, counties have a higher value for PC1 if they have lower-than-average income per capita and higher-than-average poverty rates, and a smaller value for PC1 if they have higher-than-average income per capita and lower-than-average poverty rates.\n\nA system for loading interpretation\nOften interpreting principal components can be difficult, and sometimes there’s no clear interpretation available! That said, it helps to have a system instead of staring at the plot and scratching our heads. Here is a semi-systematic approach to interpreting loadings:\n\nDivert your attention away from the zero line.\nFind the largest positive loading, and list all variables with similar loadings.\nFind the largest negative loading, and list all variables with similar loadings.\nThe principal component represents the difference between the average of the first set and the average of the second set.\nTry to come up with a description of less than 4 words.\n\nThis system is based on the following ideas: * a high loading value (negative or positive) indicates that a variable strongly influences the principal component; * a negative loading value indicates that + increases in the value of a variable decrease the value of the principal component + and decreases in the value of a variable increase the value of the principal component; * a positive loading value indicates that + increases in the value of a variable increase the value of the principal component + and decreases in the value of a variable decrease the value of the principal component; * similar loadings between two or more variables indicate that the principal component reflects their average; * divergent loadings between two sets of variables indicates that the principal component reflects their difference.\n\n\n\nQuestion 6\nWork with your neighbor to interpret PC2. Come up with a name for the component and explain which variables are most influential.\nType your answer here, replacing this text."
  },
  {
    "objectID": "labs/lab5-pca/lab5-pca.html#standardization",
    "href": "labs/lab5-pca/lab5-pca.html#standardization",
    "title": "Lab 5: Principal components",
    "section": "Standardization",
    "text": "Standardization\nData are typically standardized because otherwise the variables on the largest scales tend to dominate the principal components, and most of the time PC1 will capture the majority of the variation. However, that is artificial. In the census data, income per capita has the largest magnitudes, and thus, the highest variance.\n\n# three largest variances\nx_mx.var().sort_values(ascending = False).head(3)\n\nWhen PCs are computed without normalization, the total variation is mostly just the variance of income per capita because it is orders of magnitude larger than the variance of any other variable. But that’s just because of the scale of the variable – incomes per capita are large numbers – not a reflection that it varies more or less than the other variables.\nRun the cell below to see what happens to the variance ratios if the data are not normalized.\n\n# recompute pcs without normalization\npca_unscaled = PCA(data = x_mx, standardize = False)\n\n# show variance ratios for first three pcs\npca_unscaled.eigenvals[0:3]/pca_unscaled.eigenvals.sum()\n\nFurther, let’s look at the loadings when data are not standardized:\n\n# subset loadings\nunscaled_loading_df = pca_unscaled.loadings.iloc[:, 0:2]\n\n# rename columns\nunscaled_loading_df = unscaled_loading_df.rename(\n    columns = dict(zip(unscaled_loading_df.columns, ['PC' + str(i) for i in range(1, 3)]))\n)\n\n# melt from wide to long\nunscaled_loading_plot_df = unscaled_loading_df.reset_index().melt(\n    id_vars = 'index',\n    var_name = 'Principal Component',\n    value_name = 'Loading'\n).rename(\n    columns = {'index': 'Variable'}\n)\n\n# add a column of zeros to encode for x = 0 line to plot\nunscaled_loading_plot_df['zero'] = np.repeat(0, len(unscaled_loading_plot_df))\n\n# create base layer\nbase = alt.Chart(unscaled_loading_plot_df)\n\n# create lines + points for loadings\nloadings = base.mark_line(point = True).encode(\n    y = alt.X('Variable', title = ''),\n    x = 'Loading',\n    color = 'Principal Component'\n)\n\n# create line at zero\nrule = base.mark_rule().encode(x = alt.X('zero', title = 'Loading'), size = alt.value(0.05))\n\n# layer\nloading_plot = (loadings + rule).properties(width = 120, title = 'Loadings from unscaled PCA')\n\n# show\nloading_plot.facet(column = alt.Column('Principal Component', title = ''))\n\nNotice that the variables with nonzero loadings in unscaled PCA are simply the three variables with the largest variances.\n\n# three largest variances\nx_mx.var().sort_values(ascending = False).head(3)"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas-soln.html",
    "href": "labs/lab1-pandas/lab1-pandas-soln.html",
    "title": "Lab 1: Pandas Overview",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab1-pandas.ipynb\")\nPandas is one of the most widely used Python libraries in data science. In this lab, you will learn commonly used data tidying operations/tools in Pandas."
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas-soln.html#creating-dataframes-basic-manipulations",
    "href": "labs/lab1-pandas/lab1-pandas-soln.html#creating-dataframes-basic-manipulations",
    "title": "Lab 1: Pandas Overview",
    "section": "Creating DataFrames & Basic Manipulations",
    "text": "Creating DataFrames & Basic Manipulations\nA dataframe is a table in which each column has a type; there is an index over the columns (typically string labels) and an index over the rows (typically ordinal numbers). An index is represented by a series object, which is a one-dimensional labeled array. Here you’ll cover:\n\ncreating dataframes from scratch;\nretrieving attributes;\ndataframe indexing;\nadding, removing, and renaming columns.\n\n\nCreating dataframes from scratch\nThe documentation for the pandas DataFrame class provide two primary syntaxes to create a data frame from scratch:\n\nfrom a dictionary\nrow-wise tuples\n\nSyntax 1 (dictionary): You can create a data frame by specifying the columns and values using a dictionary (a concatenation of named lists) as shown below.\nThe keys of the dictionary are the column names, and the values of the dictionary are lists containing the row entries.\n\n# define a dataframe using dictionary syntax\nfruit_info = pd.DataFrame( \n    data = { 'fruit': ['apple', 'orange', 'banana', 'raspberry'],\n             'color': ['red', 'orange', 'yellow', 'pink']\n           })\n\n# print\nfruit_info\n\n\n\n\n\n\n\n\nfruit\ncolor\n\n\n\n\n0\napple\nred\n\n\n1\norange\norange\n\n\n2\nbanana\nyellow\n\n\n3\nraspberry\npink\n\n\n\n\n\n\n\nSyntax 2 (row tuples): You can also define a dataframe by specifying the rows as tuples.\nEach row corresponds to a distinct tuple, and the column indices are specified separately.\n\n# define the same dataframe using tuple syntax \nfruit_info2 = pd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"]\n)\n\n# print\nfruit_info2\n\n\n\n\n\n\n\n\nfruit\ncolor\n\n\n\n\n0\napple\nred\n\n\n1\norange\norange\n\n\n2\nbanana\nyellow\n\n\n3\nraspberry\npink\n\n\n\n\n\n\n\n\n\nDataframe Attributes\nDataFrames have several basic attributes:\n\n.shape contains dimensions;\n.dtypes contains data types (float, integer, object, etc.)\n.size first (row) dimension;\n.values contains an array comprising each entry in the dataframe.\n.columns contains the column index;\n.index contains the row index.\n\nYou can obtain these attributes by appending the attribute name to the dataframe name. For instance, the dimensions of a dataframe df can be retrieved by df.shape.\n\n# dimensions\nfruit_info.shape\n\n(4, 2)\n\n\nTo retrieve a two-dimensional numpy array with the values of the dataframe, use df.values. It is sometimes useful to extract this data structure in order to perform vectorized operations, linear algebra, and the like.\n\n# as array\nfruit_info.values\n\narray([['apple', 'red'],\n       ['orange', 'orange'],\n       ['banana', 'yellow'],\n       ['raspberry', 'pink']], dtype=object)\n\n\n\n\nDataframe Indexing\nThe entries in a dataframe are indexed. Indices for rows and columns are stored as the .index. and .columns attributes, respectively.\n\nfruit_info.columns\n\nIndex(['fruit', 'color'], dtype='object')\n\n\n\nfruit_info.index\n\nRangeIndex(start=0, stop=4, step=1)\n\n\nBy default, the row indexing is simply numbering by consecutive integers.\n\nfruit_info.index.values\n\narray([0, 1, 2, 3], dtype=int64)\n\n\nHowever, rows can alternatively be indexed by labels:\n\n# define with a row index\npd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"],\n    index = [\"fruit 1\", \"fruit 2\", \"fruit 3\", 'fruit 4']\n)\n\n\n\n\n\n\n\n\nfruit\ncolor\n\n\n\n\nfruit 1\napple\nred\n\n\nfruit 2\norange\norange\n\n\nfruit 3\nbanana\nyellow\n\n\nfruit 4\nraspberry\npink\n\n\n\n\n\n\n\nUnlike data frames in R, the row index label figures prominently in certain operations. The elements of the dataframe can be retrived using .loc[ROW-INDEX, COL-INDEX] which specifies the location of data values by name (not by position).\n\n# retrieve row 0, column 'fruit'\nfruit_info.loc[0, 'fruit']\n\n'apple'\n\n\nMost of the time rows are indexed numerically, and somewhat confusingly, the syntax for .loc does not require putting the row index 0 in quotes, even though it refers to the row label and not the row number. This is important to remember, because often operations will scramble the order of rows. To see the difference, consider the following:\n\n# non-consecutive row index\npd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"],\n    index = np.array([8, 6, 4, 2])\n)\n\n\n\n\n\n\n\n\nfruit\ncolor\n\n\n\n\n8\napple\nred\n\n\n6\norange\norange\n\n\n4\nbanana\nyellow\n\n\n2\nraspberry\npink\n\n\n\n\n\n\n\nNow adding .loc[2, 'color'] will retrieve pink, the last row of the dataframe.\n\n# subset\npd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"],\n    index = np.array([8, 6, 4, 2])\n).loc[2, 'color']\n\n'pink'\n\n\nTo retrieve values by position, use .iloc. For many, this is more intuitive, as it is most similar to matrix or array indexing in mathematical notation.\n\n# retrieve 0, 0 entry\nfruit_info.iloc[0, 0]\n\n'apple'\n\n\n\n\nAdding, removing, and renaming columns\nThere are two ways to add new columns:\n\ndirect specification;\nusing .loc[].\n\nDirect specification: For a dataFrame df, you can add a column with df['new column name'] = ... and assign a list or array of values to the column.\nUsing .loc[]: For a dataframe df, you can add a column with df.loc[:, 'new column name'] = ... and assign a list or array of values to the column.\nBoth accomplish the same task – adding a new column index and populating values for each row – but .loc[] is a little faster.\n\n\nQuestion 1\nUsing direct specification, add to the fruit_info table a new column called rank1 containing integers 1, 2, 3, and 4, which express your personal preference about the taste ordering for each fruit (1 is tastiest; 4 is least tasty). Make sure that the numbers utilized are unique - no ties are allowed.\n\n\nfruit_info['rank1'] = [1, 3, 4, 2] # SOLUTION\n\n# print\nfruit_info\n\n\n\n\n\n\n\n\nfruit\ncolor\nrank1\n\n\n\n\n0\napple\nred\n1\n\n\n1\norange\norange\n3\n\n\n2\nbanana\nyellow\n4\n\n\n3\nraspberry\npink\n2\n\n\n\n\n\n\n\n\ngrader.check(\"q1\")\n\nNow, create a new dataframe fruit_info_mod1 with the same information as fruit_info_original, but has the additional column rank2. Let’s start off with making fruit_info_mod1 as a copy of fruit_info:\n\nfruit_info_mod1 = fruit_info.copy()\n\n\n\nQuestion 2\nUsing .loc[], add a column called rank2 to the fruit_info_mod1 table that contains the same values in the same order as the rank1 column.\nHint: .loc will parse : as shorthand for ‘all indices’.\n\n\nfruit_info_mod1.loc[:, 'rank2']  = [1, 3, 4, 2] #SOLUTION\n\n# print\nfruit_info_mod1\n\n\n\n\n\n\n\n\nfruit\ncolor\nrank1\nrank2\n\n\n\n\n0\napple\nred\n1\n1\n\n\n1\norange\norange\n3\n3\n\n\n2\nbanana\nyellow\n4\n4\n\n\n3\nraspberry\npink\n2\n2\n\n\n\n\n\n\n\n\ngrader.check(\"q2\")\n\nWhen using the .loc[] approach, the : specifies that values are assigned to all rows of the data frame, so the array assigned to the new variable must be the same length as the data frame. What if we only assign values to certain rows? Try running the cell below.\n\n# define new variable just for rows 1 and 2\nfruit_info_mod1.loc[1:2, 'rank3'] = [1, 2]\n\n# check result\nfruit_info_mod1\n\n\n\n\n\n\n\n\nfruit\ncolor\nrank1\nrank2\nrank3\n\n\n\n\n0\napple\nred\n1\n1\nNaN\n\n\n1\norange\norange\n3\n3\n1.0\n\n\n2\nbanana\nyellow\n4\n4\n2.0\n\n\n3\nraspberry\npink\n2\n2\nNaN\n\n\n\n\n\n\n\nThe remaining rows are assigned missing values. Notice what this does to the data type:\n\n# check data types\nfruit_info_mod1.dtypes\n\nfruit     object\ncolor     object\nrank1      int64\nrank2      int64\nrank3    float64\ndtype: object\n\n\nWe can detect these missing values using .isna():\n\n# returns a logical data frame indicating whether each entry is missing or not\nfruit_info_mod1.isna()\n\n\n\n\n\n\n\n\nfruit\ncolor\nrank1\nrank2\nrank3\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n\n\n\n\n\nIt would be more helpful to simply see by column whether there are missing values. Appending a .any() to the above command will do the trick:\n\n# detects whether any column has missing entries\nfruit_info_mod1.isna().any()\n\nfruit    False\ncolor    False\nrank1    False\nrank2    False\nrank3     True\ndtype: bool\n\n\nNow that we’ve had a bit of fun let’s remove those rank variables. Columns can be removed using .drop() with a list of column names to drop as its argument. For example:\n\n# first syntax for .drop()\nfruit_info_mod1.drop(columns = 'color')\n\n\n\n\n\n\n\n\nfruit\nrank1\nrank2\nrank3\n\n\n\n\n0\napple\n1\n1\nNaN\n\n\n1\norange\n3\n3\n1.0\n\n\n2\nbanana\n4\n4\n2.0\n\n\n3\nraspberry\n2\n2\nNaN\n\n\n\n\n\n\n\nThere is an alternate syntax to that shown above, which involves specifying the axis (row vs. column) and index name to drop:\n\n# second syntax for .drop()\nfruit_info_mod1.drop('color', axis = 1)\n\n\n\n\n\n\n\n\nfruit\nrank1\nrank2\nrank3\n\n\n\n\n0\napple\n1\n1\nNaN\n\n\n1\norange\n3\n3\n1.0\n\n\n2\nbanana\n4\n4\n2.0\n\n\n3\nraspberry\n2\n2\nNaN\n\n\n\n\n\n\n\n\n\nQuestion 3\nUse the .drop() method to drop all rank columns you created in fruit_info_mod1. Note that drop does not change the table, but instead returns a new table with fewer columns or rows. To store the result, assign a new name (or write over the old dataframe). Here, assign the result to fruit_info_original.\nHint: Look through the documentation to see how you can drop multiple columns of a Pandas dataframe at once using a list of column names.\n\n\nfruit_info_original = fruit_info_mod1.drop(columns = ['rank1', 'rank2', 'rank3']) #SOLUTION\n\n# print\nfruit_info_original\n\n\n\n\n\n\n\n\nfruit\ncolor\n\n\n\n\n0\napple\nred\n\n\n1\norange\norange\n\n\n2\nbanana\nyellow\n\n\n3\nraspberry\npink\n\n\n\n\n\n\n\n\ngrader.check(\"q3\")\n\nNifty trick: Use df.columns[df.columns.str.startswith('STRING')] to retrieve all indices starting with STRING and ix.values.tolist() to convert an index to an array of index names to obtain a list of column names to drop. Combining these gives df.columns[df.columns.str.startswith('STRING')].values.tolist(), and will return a list of all column names starting with STRING. This can be used in conjunction with the hint to remove all columns starting with rank.\n\n# try the nifty trick here\n\nNow create a new dataframe fruit_info_mod2with the same information as fruit_info_original, but has the column names capitalized. Begin by creating a copy fruit_info_mod2 of fruit_info_original:\n\nfruit_info_mod2 = fruit_info_original.copy()\n\n\n\nQuestion 4\nReview the documentation for .rename(). Based on the examples, rename the columns of fruit_info_mod2 so they begin with capital letters.\nFor many operations, you can change the dataframe ‘in place’ without reassigning the result of the operation to a new name by setting the inplace parameter to True. Use that strategy here.\n\n\nfruit_info_mod2.rename(columns = {'fruit': 'Fruit', 'color': 'Color'}, inplace = True) #SOLUTION\n\n# print\nfruit_info_mod2\n\n\n\n\n\n\n\n\nFruit\nColor\n\n\n\n\n0\napple\nred\n\n\n1\norange\norange\n\n\n2\nbanana\nyellow\n\n\n3\nraspberry\npink\n\n\n\n\n\n\n\n\ngrader.check(\"q4\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas-soln.html#operations-on-data-frames",
    "href": "labs/lab1-pandas/lab1-pandas-soln.html#operations-on-data-frames",
    "title": "Lab 1: Pandas Overview",
    "section": "Operations on Data Frames",
    "text": "Operations on Data Frames\nWith some basics in place, here you’ll see how to perform subsetting operations on data frames that are useful for tidying up datasets.\n\nSlicing: selecting columns or rows in chunks or by position.\n\nOften imported data contain columns that are either superfluous or not of interest for a particular project.\nYou may also want to examine particular portions of a data frame.\n\nFiltering: selecting rows that meet certain criteria\n\nOften you’ll want to remove duplicate rows, filter missing observations, or select a structured subset of a data frame.\nAlso helpful for inspection.\n\n\nTo illustrate these operations, you’ll use a dataset comprising counts of the given names of babies born in California each year from 1990 - 2018. The cell below imports the baby names data as a data frame from a .csv file. .head() prints the first few rows of the dataset.\n\n# import baby names data\nbaby_names = pd.read_csv('data/baby_names.csv')\n\n# preview first few rows\nbaby_names.head()\n\n\n\n\n\n\n\n\nState\nSex\nYear\nName\nCount\n\n\n\n\n0\nCA\nF\n1990\nJessica\n6635\n\n\n1\nCA\nF\n1990\nAshley\n4537\n\n\n2\nCA\nF\n1990\nStephanie\n4001\n\n\n3\nCA\nF\n1990\nAmanda\n3856\n\n\n4\nCA\nF\n1990\nJennifer\n3611\n\n\n\n\n\n\n\nYour focus here isn’t on analyzing this data, so we won’t ask you to spend too much effort getting acquainted with it. However, a brief inspection is always a good idea. Let’s check:\n\ndimensions (number of rows and columns);\nhow many distinct states, sexes, and years.\n\nNote that the above dataframe displayed is a preview of the full dataframe.\n\nQuestion 5\nYou’ve already seen how to examine dimensions using dataframe attributes. Check the dimensions of baby_names and store them in dimensions_baby_names.\n\n\ndimensions_baby_names = baby_names.shape #SOLUTION\n\n\ngrader.check(\"q5\")\n\nYou haven’t yet seen how to retrieve the distinct values of an array or series, without duplicates. There are a few different ways to go about this, but one is to count the number of occurrences of each distinct entry in a column. This can be done by retrieving the column as a series using syntax of the form df.colname, and then pass the result to .value_counts():\n\n# count distinct values\nbaby_names.Sex.value_counts()\n\nF    112196\nM     78566\nName: Sex, dtype: int64\n\n\n\n\nQuestion 6\nCount the number of occurences of each distinct year. Create a series occur_per_year that displays the number of occurrences, ordered by year (so that the years are displayed in order). If you add sort = False as an argument to value_counts, the distinct values will be displayed in the order they appear in the dataset.\nHow many years are represented in the dataset? Store your answer as num_years.\n\n\noccur_per_year = baby_names.Year.value_counts(sort = False) #SOLUTION \n\nnum_years = len(occur_per_year) #SOLUTION\n\nprint(occur_per_year)\nprint(num_years)\n\n1990    6261\n1991    6226\n1992    6304\n1993    6314\n1994    6241\n1995    6092\n1996    6036\n1997    5961\n1998    5976\n1999    6052\n2000    6284\n2001    6333\n2002    6414\n2003    6533\n2004    6708\n2005    6874\n2006    7075\n2007    7250\n2008    7158\n2009    7119\n2010    7010\n2011    6880\n2012    7007\n2013    6861\n2014    6952\n2015    6871\n2016    6770\n2017    6684\n2018    6516\nName: Year, dtype: int64\n29\n\n\n\ngrader.check(\"q6\")\n\n\n\nSlicing: selecting rows and columns\nThere are two fast and simple ways to slice dataframes:\n\nusing .loc to specify rows and columns by index;\nusing .iloc to specify rows and columns by position.\n\nYou have seen simple examples of both of these above. Here we’ll show how to use these two commands to retrieve multiple rows and columns.\n\nSlicing with .loc: specifying index names\nThis method retrieves entries by specifying row and column indexes using syntax of the form df.loc[rows, cols]. The rows and columns can be single indices, a list of indices, or a set of adjacent indices using a colon :. Examples of these usages are shown below.\n\n# single indices -- small slice\nbaby_names.loc[2, 'Name']\n\n'Stephanie'\n\n\n\n# a list of indices -- larger slice\nbaby_names.loc[[2, 3], ['Name', 'Count']]\n\n\n\n\n\n\n\n\nName\nCount\n\n\n\n\n2\nStephanie\n4001\n\n\n3\nAmanda\n3856\n\n\n\n\n\n\n\n\n# consecutive indices -- a chunk\nbaby_names.loc[2:10, 'Year':'Count']\n\n\n\n\n\n\n\n\nYear\nName\nCount\n\n\n\n\n2\n1990\nStephanie\n4001\n\n\n3\n1990\nAmanda\n3856\n\n\n4\n1990\nJennifer\n3611\n\n\n5\n1990\nElizabeth\n3170\n\n\n6\n1990\nSarah\n2843\n\n\n7\n1990\nBrittany\n2737\n\n\n8\n1990\nSamantha\n2720\n\n\n9\n1990\nMichelle\n2453\n\n\n10\n1990\nMelissa\n2442\n\n\n\n\n\n\n\n\n\n\nSlicing with .iloc: specifying entry positions\nAn alternative to specifying the indices in order to slice a dataframe is to specify the entry positions using .iloc (‘integer location’). You have seen an example of this too. As with .loc, .iloc can be used to select multiple rows/columns using either lists of positions or a consecutive set with from:to syntax.\n\n# single position\nbaby_names.iloc[2, 3]\n\n'Stephanie'\n\n\n\n# a list of positions\nbaby_names.iloc[[2, 3], [3, 4]]\n\n\n\n\n\n\n\n\nName\nCount\n\n\n\n\n2\nStephanie\n4001\n\n\n3\nAmanda\n3856\n\n\n\n\n\n\n\n\n# consecutive positions\nbaby_names.iloc[2:11, 2:5]\n\n\n\n\n\n\n\n\nYear\nName\nCount\n\n\n\n\n2\n1990\nStephanie\n4001\n\n\n3\n1990\nAmanda\n3856\n\n\n4\n1990\nJennifer\n3611\n\n\n5\n1990\nElizabeth\n3170\n\n\n6\n1990\nSarah\n2843\n\n\n7\n1990\nBrittany\n2737\n\n\n8\n1990\nSamantha\n2720\n\n\n9\n1990\nMichelle\n2453\n\n\n10\n1990\nMelissa\n2442\n\n\n\n\n\n\n\nWhile these commands may look very similar to their .loc analogs, there are some subtle but important differences. The row selection looks nearly identical, but recall that .loc uses the index and .iloc uses the position; they look so similar because typically index and position coincide.\nHowever, sorting the baby_names dataframe helps to reveal how the position of a row is not necessarily equal to the index of a row. For example, the first row is not necessarily the row associated with index 1. This distinction is important in understanding the difference between .loc[] and .iloc[].\n\n# sort and display\nsorted_baby_names = baby_names.sort_values(by=['Name'])\nsorted_baby_names.head()\n\n\n\n\n\n\n\n\nState\nSex\nYear\nName\nCount\n\n\n\n\n160797\nCA\nM\n2008\nAadan\n7\n\n\n178791\nCA\nM\n2014\nAadan\n5\n\n\n163914\nCA\nM\n2009\nAadan\n6\n\n\n171112\nCA\nM\n2012\nAaden\n38\n\n\n179928\nCA\nM\n2015\nAaden\n34\n\n\n\n\n\n\n\nHere is an example of how we would get the 2nd, 3rd, and 4th rows with only the Name column of the baby_names dataframe using both iloc[] and loc[]. Observe the difference, especially after sorting baby_names by name.\n\n# example iloc usage\nsorted_baby_names.iloc[1:4, 3]\n\n178791    Aadan\n163914    Aadan\n171112    Aaden\nName: Name, dtype: object\n\n\nNotice that using loc[] with 1:4 gives different results, since it selects using the index. The index gets moved around when you perform an operation like sort on the dataframe.\n\n# same syntax, different result\nsorted_baby_names.loc[1:4, \"Name\"]\n\n1           Ashley\n22219       Ashley\n138598      Ashley\n151978      Ashley\n120624      Ashley\n            ...   \n74380       Jennie\n19395       Jennie\n23061       Jennie\n91825       Jennie\n4         Jennifer\nName: Name, Length: 68640, dtype: object\n\n\nAbove, the .loc method retrieves all indexes between index 1 and index 4 in the order they appear in the sorted dataset. If instead we want to retrieve the same rows returned by the .iloc command, we need to specify the row indices explicitly as a list:\n\n# retrieve the same rows as iloc using loc\nsorted_baby_names.loc[[178791, 163914, 171112], 'Name']\n\n178791    Aadan\n163914    Aadan\n171112    Aaden\nName: Name, dtype: object\n\n\nSometimes it’s useful for slicing (and other operations) to set one of the columns to be a row index, effectively treating one column as a collection of row labels. This can be accomplished using set_index.\n\n# change the (row) index from 0,1,2,... to the name column\nbaby_names_nameindexed = baby_names.set_index(\"Name\") \nbaby_names_nameindexed.head()\n\n\n\n\n\n\n\n\nState\nSex\nYear\nCount\n\n\nName\n\n\n\n\n\n\n\n\nJessica\nCA\nF\n1990\n6635\n\n\nAshley\nCA\nF\n1990\n4537\n\n\nStephanie\nCA\nF\n1990\n4001\n\n\nAmanda\nCA\nF\n1990\n3856\n\n\nJennifer\nCA\nF\n1990\n3611\n\n\n\n\n\n\n\nWe can now slice by name directly:\n\n# slice rows for ashley and jennifer\nbaby_names_nameindexed.loc[['Ashley', 'Jennifer'], :]\n\n\n\n\n\n\n\n\nState\nSex\nYear\nCount\n\n\nName\n\n\n\n\n\n\n\n\nAshley\nCA\nF\n1990\n4537\n\n\nAshley\nCA\nF\n1991\n4233\n\n\nAshley\nCA\nF\n1992\n3966\n\n\nAshley\nCA\nF\n1993\n3591\n\n\nAshley\nCA\nF\n1994\n3202\n\n\n...\n...\n...\n...\n...\n\n\nJennifer\nCA\nM\n1998\n10\n\n\nJennifer\nCA\nM\n1999\n12\n\n\nJennifer\nCA\nM\n2000\n10\n\n\nJennifer\nCA\nM\n2001\n8\n\n\nJennifer\nCA\nM\n2002\n7\n\n\n\n\n88 rows Ã— 4 columns\n\n\n\n\n\nQuestion 7\nLook up the name of a friend! Store the name as friend_name. Use the name-indexed data frame to slice rows for the name of your choice and the Count, Sex, and Year columns in that order, and store the data frame as friend_slice.\n\n\n# if your friend's name is not in the database, use another name\n\nfriend_name = \"Trevor\" # SOLUTION\nfriend_slice = baby_names_nameindexed.loc[\"Trevor\", ['Count', 'Sex', 'Year']] #SOLUTION\n\n#print\nfriend_slice\n\n\n\n\n\n\n\n\nCount\nSex\nYear\n\n\nName\n\n\n\n\n\n\n\nTrevor\n5\nF\n1990\n\n\nTrevor\n823\nM\n1990\n\n\nTrevor\n836\nM\n1991\n\n\nTrevor\n897\nM\n1992\n\n\nTrevor\n737\nM\n1993\n\n\nTrevor\n675\nM\n1994\n\n\nTrevor\n682\nM\n1995\n\n\nTrevor\n609\nM\n1996\n\n\nTrevor\n590\nM\n1997\n\n\nTrevor\n647\nM\n1998\n\n\nTrevor\n673\nM\n1999\n\n\nTrevor\n545\nM\n2000\n\n\nTrevor\n535\nM\n2001\n\n\nTrevor\n488\nM\n2002\n\n\nTrevor\n425\nM\n2003\n\n\nTrevor\n369\nM\n2004\n\n\nTrevor\n372\nM\n2005\n\n\nTrevor\n335\nM\n2006\n\n\nTrevor\n302\nM\n2007\n\n\nTrevor\n281\nM\n2008\n\n\nTrevor\n252\nM\n2009\n\n\nTrevor\n219\nM\n2010\n\n\nTrevor\n194\nM\n2011\n\n\nTrevor\n161\nM\n2012\n\n\nTrevor\n142\nM\n2013\n\n\nTrevor\n126\nM\n2014\n\n\nTrevor\n114\nM\n2015\n\n\nTrevor\n103\nM\n2016\n\n\nTrevor\n90\nM\n2017\n\n\nTrevor\n78\nM\n2018\n\n\n\n\n\n\n\n\ngrader.check(\"q7\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas-soln.html#filtering",
    "href": "labs/lab1-pandas/lab1-pandas-soln.html#filtering",
    "title": "Lab 1: Pandas Overview",
    "section": "Filtering",
    "text": "Filtering\nFiltering is sifting out rows according to a criterion, and can be accomplished using an array or series of Trues and Falses defined by a comparison. To take a simple example, say you wanted to filter out all names with fewer than 1000 occurrences. First you could define a logical series:\n\n# true if filtering criterion is met, false otherwise\narr = baby_names.Count &gt; 1000\n\nThen you can filter using that array:\n\n# filter\nbaby_names_filtered = baby_names[arr]\nbaby_names_filtered.head()\n\n\n\n\n\n\n\n\nState\nSex\nYear\nName\nCount\n\n\n\n\n0\nCA\nF\n1990\nJessica\n6635\n\n\n1\nCA\nF\n1990\nAshley\n4537\n\n\n2\nCA\nF\n1990\nStephanie\n4001\n\n\n3\nCA\nF\n1990\nAmanda\n3856\n\n\n4\nCA\nF\n1990\nJennifer\n3611\n\n\n\n\n\n\n\nNotice that the filtered array is much smaller than the overall array – only about 2000 rows correspond to a name occurring more than 1000 times in a year for a gender.\n\n# compare dimensions\nprint(baby_names_filtered.shape)\nprint(baby_names.shape)\n\n(2517, 5)\n(190762, 5)\n\n\nYou have already encountered this concept in lab 0 when subsetting an array. For your reference, some commonly used comparison operators are given below.\n\n\n\nSymbol\nUsage\nMeaning\n\n\n\n\n==\na == b\nDoes a equal b?\n\n\n&lt;=\na &lt;= b\nIs a less than or equal to b?\n\n\n&gt;=\na &gt;= b\nIs a greater than or equal to b?\n\n\n&lt;\na &lt; b\nIs a less than b?\n\n\n&gt;\na &gt; b\nIs a greater than b?\n\n\n~\n~p\nReturns negation of p\n\n\n|\np | q\np OR q\n\n\n&\np & q\np AND q\n\n\n^\np ^ q\np XOR q (exclusive or)\n\n\n\nWhat if instead you wanted to filter using multiple conditions? Here’s an example of retrieving rows with counts exceeding 1000 for only the year 2001:\n\n# filter using two conditions\nbaby_names[(baby_names.Year == 2000) & (baby_names.Count &gt; 1000)]\n\n\n\n\n\n\n\n\nState\nSex\nYear\nName\nCount\n\n\n\n\n36416\nCA\nF\n2000\nEmily\n2958\n\n\n36417\nCA\nF\n2000\nAshley\n2831\n\n\n36418\nCA\nF\n2000\nSamantha\n2579\n\n\n36419\nCA\nF\n2000\nJessica\n2484\n\n\n36420\nCA\nF\n2000\nJennifer\n2263\n\n\n...\n...\n...\n...\n...\n...\n\n\n137298\nCA\nM\n2000\nOscar\n1089\n\n\n137299\nCA\nM\n2000\nThomas\n1061\n\n\n137300\nCA\nM\n2000\nCameron\n1052\n\n\n137301\nCA\nM\n2000\nAustin\n1010\n\n\n137302\nCA\nM\n2000\nRichard\n1001\n\n\n\n\n98 rows Ã— 5 columns\n\n\n\n\nQuestion 8\nSelect the girl names in 2010 that were given more than 3000 times, and store them as common_girl_names_2010.\nNote: Any time you use p & q to filter the dataframe, make sure to use df[df[(p) & (q)]] or df.loc[df[(p) & (q)]]). That is, make sure to wrap conditions with parentheses to ensure the intended order of operations.\n\n\ncommon_girl_names_2010 = baby_names[(baby_names.Sex == 'F') & (baby_names.Year == 2010) & (baby_names.Count &gt; 3000)] #SOLUTION\n\ncommon_girl_names_2010\n\n\n\n\n\n\n\n\nState\nSex\nYear\nName\nCount\n\n\n\n\n76793\nCA\nF\n2010\nIsabella\n3368\n\n\n76794\nCA\nF\n2010\nSophia\n3361\n\n\n\n\n\n\n\n\ngrader.check(\"q8\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas-soln.html#grouping-and-aggregation",
    "href": "labs/lab1-pandas/lab1-pandas-soln.html#grouping-and-aggregation",
    "title": "Lab 1: Pandas Overview",
    "section": "Grouping and aggregation",
    "text": "Grouping and aggregation\nGrouping and aggregation are useful in generating data summaries, which are often important starting points in exploring a dataset.\n\nAggregation\nAggregation literally means ‘putting together’ (etymologically the word means ‘joining the herd’) – in statistics and data science, this refers to data summaries like an average, a minimum, or a measure of spread such as the sample variance or mean absolute deviation (data herding!). From a technical point of view, operations that take multiple values as inputs and return a single output are considered summaries – in other words, statistics. Some of the most common aggregations are:\n\nsum\nproduct\ncount\nnumber of distinct values\nmean\nmedian\nvariance\nstandard deviation\nminimum/maximum\nquantiles\n\nPandas has built-in dataframe operations that compute most of these summaries across either axis (column-wise or row-wise):\n\n.sum()\n.prod()\n.mean()\n.median()\n.var()\n.std()\n.nunique()\n.min() and .max()\n.quantile()\n\nTo illustrate these operations, let’s filter out all names in 1995.\n\n# filter 1995 names\nnames_95 = baby_names[baby_names.Year == 1995]\n\nHow many individuals were counted in total in 1995? We can address that by computing a sum of the counts:\n\n# n for 1995\nnames_95.Count.sum()\n\n494580\n\n\nWhat is the typical frequency of all names in 1995? We can address that by computing the average count:\n\n# average count for a name in 1995\nnames_95.Count.mean()\n\n81.18516086671043\n\n\n\n\nQuestion 9\nFind how often the most common name 1995 was given and store this as names_95_max_count. Use this value to filter names_95 and find which name was most common that year. Store the filtered dataframe as names_95_most_common_name.\n\n\nnames_95_max_count = names_95.Count.max() #SOLUTION\nnames_95_most_common_name = (names_95.loc[names_95.Count == names_95.Count.max(),  'Name']) #SOLUTION\n\n\nprint(\"Number of people with the most frequent name in 1995 is :\", names_95_max_count, \"people\")\nprint(\"Most frequent name in 1995 is:\", names_95_most_common_name.values[0])\n\nNumber of people with the most frequent name in 1995 is : 5003 people\nMost frequent name in 1995 is: Daniel\n\n\n\ngrader.check(\"q9\")\n\nCaution! If applied to the entire dataframe, the operation df.max() (or any other aggregation) will return the maximum of each column. Notice that the cell below does not return the row you found just now, but could easily be misinterpreted as such. The cell does tell you that the maximum value of sex (alphabetically last) is M and the maximum name (alphabetically last) is Zyanya and the maximum count is 5003; it does not tell you that 5003 boys were named Zyanya in 1995.\n\n# maximum of each variable\nnames_95.max()\n\nState        CA\nSex           M\nYear       1995\nName     Zyanya\nCount      5003\ndtype: object\n\n\n\n\nGrouping\nWhat if you want to know the most frequent male and female names? If so, you’ll need to repeat the above operations group-wise by sex.\nIn general, any variable in a dataframe can be used to define a grouping structure on the rows (or, less commonly, columns). After grouping, any dataframe operations will be executed within each group, but not across groups. This can be used to generate grouped summaries, such as the maximum count for boys and girls; as a point of terminology, we’d describe this summary as ‘maximum count by sex’.\nThe .groupby() function defines such a structure; here is the documentation. The cell below groups the names_95 dataframe by sex. Notice that when the grouped dataframe is previewed with .head(), the first few rows are returned for each group.\n\n# grouped dataframe\nnames_95_bysex = names_95.groupby('Sex')\n\n# print\nnames_95_bysex.head(2)\n\n\n\n\n\n\n\n\nState\nSex\nYear\nName\nCount\n\n\n\n\n18604\nCA\nF\n1995\nJessica\n4620\n\n\n18605\nCA\nF\n1995\nAshley\n2903\n\n\n124938\nCA\nM\n1995\nDaniel\n5003\n\n\n124939\nCA\nM\n1995\nMichael\n4783\n\n\n\n\n\n\n\nAny aggregation operations applied to the grouped dataframe will be applied separately to the rows where Sex == M and the rows where Sex == F. For example, computing .sum() on the grouped dataframe will show the total number of individuals in the data for 1995 by sex:\n\n# number of individuals by sex\nnames_95_bysex.Count.sum()\n\nSex\nF    234552\nM    260028\nName: Count, dtype: int64\n\n\nThe most frequent boy and girl names can be found using .idxmax() groupwise to obtain the index of the first occurence of the maximum count for each sex, and then slicing with .loc:\n\n# first most common names by sex\nnames_95.loc[names_95_bysex.Count.idxmax(), :]\n\n\n\n\n\n\n\n\nState\nSex\nYear\nName\nCount\n\n\n\n\n18604\nCA\nF\n1995\nJessica\n4620\n\n\n124938\nCA\nM\n1995\nDaniel\n5003\n\n\n\n\n\n\n\nSince .idxmax() gives the index of the first occurrence, these are the alphabetically first most common names; there could be ties. You know from your work so far that there are no ties for the male names; another filtering step can be used to check for ties among the female names.\n\n# ties?\nnames_95[names_95_bysex.Count.max().values[0] == names_95['Count']]\n\n\n\n\n\n\n\n\nState\nSex\nYear\nName\nCount\n\n\n\n\n18604\nCA\nF\n1995\nJessica\n4620\n\n\n\n\n\n\n\nSo, no ties.\n\n\nQuestion 10\nAre there more girl names or boy names in 1995? Use the grouped dataframe names_95_bysex with the .count() aggregation to find the total number of names for each sex. Store the female and male counts as girl_name_count and boy_name_count, respectfully.\n\n\ngirl_name_count = names_95_bysex.Count.count()['F'] #SOLUTION\nboy_name_count = names_95_bysex.Count.count()['M'] #SOLUTION\n\n#print\nprint(girl_name_count)\nprint(boy_name_count)\n\n3614\n2478\n\n\n\ngrader.check(\"q10\")\n\n\n\nChaining operations\nYou have already seen examples of this, but pandas and numpy operations can be chained together in sequence. For example, names_95.Count.max() is a chain with two steps: first select the Count column (.count); then compute the maximum (.max()).\nGrouped summaries are often convenient to compute in a chained fashion, rather than by assigning the grouped dataframe a new name and performing operations on the resulting object. For example, finding the total number of boys and girls recorded in the 1995 data can be done with the following chain:\n\n# repeating previous calculation, more streamlined\nnames_95.groupby('Sex').Count.sum()\n\nSex\nF    234552\nM    260028\nName: Count, dtype: int64\n\n\nWe can take this even one step further and also perform the filtering in sequence as part of the chain:\n\n# longer chain\nbaby_names[baby_names.Year == 1995].groupby('Sex').Count.sum()\n\nSex\nF    234552\nM    260028\nName: Count, dtype: int64\n\n\nChains can get somewhat long, but they have the advantage of making codes more efficient, and often more readable. We did above in one step what took several lines before. Further, this chain can almost be read aloud:\n“Take baby names, filter on year, then group by sex, then select name counts, then compute the sum.”\nLet’s now consider computing the average counts of boy and girl names for each year 1990-1995. This can be accomplished by the following chain (notice it is possible to group by multiple variables).\n\n# average counts by sex and year\nbaby_names[baby_names.Year &lt;= 1995].groupby(['Year', 'Sex']).mean(numeric_only = True)\n\n\n\n\n\n\n\n\n\nCount\n\n\nYear\nSex\n\n\n\n\n\n1990\nF\n70.085760\n\n\nM\n115.231930\n\n\n1991\nF\n70.380888\n\n\nM\n114.608124\n\n\n1992\nF\n68.744510\n\n\nM\n110.601556\n\n\n1993\nF\n66.330675\n\n\nM\n107.896552\n\n\n1994\nF\n66.426301\n\n\nM\n102.967966\n\n\n1995\nF\n64.900941\n\n\nM\n104.934625\n\n\n\n\n\n\n\nThis display is not ideal. We can ‘pivot’ the table into a wide format by adding a few extra steps in the chain: change the indices to columns; then define a new shape by specifying which column should be the new row index, which should be the new column index, and which values should populate the table.\n\n# average counts by sex and year\nbaby_names[baby_names.Year &lt;= 1995].groupby(\n    ['Year', 'Sex']\n    ).mean(\n    numeric_only = True\n    ).reset_index().pivot(\n    index = 'Sex', columns = 'Year', values = 'Count'\n    )\n\n\n\n\n\n\n\nYear\n1990\n1991\n1992\n1993\n1994\n1995\n\n\nSex\n\n\n\n\n\n\n\n\n\n\nF\n70.08576\n70.380888\n68.744510\n66.330675\n66.426301\n64.900941\n\n\nM\n115.23193\n114.608124\n110.601556\n107.896552\n102.967966\n104.934625\n\n\n\n\n\n\n\nStyle comment: break long chains over multiple lines with indentation. The above chain is too long to be readable in one line. To balance the readability of codes with the efficiency of chaining, it is good practice to break long chains over several lines, with appropriate indentations.\nHere are some rules of thumb on style.\n\nSeparate comparisons by spaces (a&lt;b as a &lt; b)\nSplit chains longer than 30-40 characters over multiple lines\nSplit lines between delimiters (, )\nIncrease indentation for lines between delimiters\nFor chained operations, try to get each step in the chain shown on a separate line\nFor functions with multiple arguments, split lines so that each argument is on its own line\n\n\n\nQuestion 11\nWrite a chain with appropriate style to display the (first) most common boy and girl names in each of the years 2005-2015. Do this in two steps:\n\nFirst filter baby_names by year, then group by year and sex, and then find the indices of first occurence of the largest counts. Store these indices as ind.\nThen use .loc[] with your stored indices to slice baby_names so as to retrieve the rows corresponding to each most frequent name each year and for each sex; then pivot this table so that the columns are years, the rows are sexes, and the entries are names. Store this as pivot_names.\n\n\n\n# BEGIN SOLUTION\nind =  baby_names[(baby_names.Year &lt;= 2015) & (baby_names.Year &gt;= 2005)].groupby(\n    ['Sex', 'Year']\n).Count.idxmax(\n).values\n\npivot_names = baby_names.loc[ind, :].pivot(\n    index = 'Sex', \n    columns = 'Year', \n    values = 'Name'\n)\n\n# END SOLUTION\nprint(ind)\npivot_names\n\n[ 55767  59866  64073  68355  72602  76793  80890  84883  88981  92944\n  96958 150164 152939 155807 158775 161686 164614 167527 170414 173323\n 176221 179159]\n\n\n\n\n\n\n\n\nYear\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n\n\nSex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF\nEmily\nEmily\nEmily\nIsabella\nIsabella\nIsabella\nSophia\nSophia\nSophia\nSophia\nSophia\n\n\nM\nDaniel\nDaniel\nDaniel\nDaniel\nDaniel\nJacob\nJacob\nJacob\nJacob\nNoah\nNoah\n\n\n\n\n\n\n\n\ngrader.check(\"q11\")"
  },
  {
    "objectID": "labs/lab6-regression/lab6-regression-soln.html",
    "href": "labs/lab6-regression/lab6-regression-soln.html",
    "title": "Lab 6: Regression",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab6-regression.ipynb\")\nimport numpy as np\nimport pandas as pd\nimport altair as alt\nimport statsmodels.api as sm\n\n# disable row limit for plotting\nalt.data_transformers.disable_max_rows()\n# uncomment to ensure graphics display with pdf export\n# alt.renderers.enable('mimetype')\n\nDataTransformerRegistry.enable('default')\nThis lab covers the nuts and bolts of fitting linear models. The linear model expresses a response variable, \\(y\\), as a linear function of \\(p - 1\\) explanatory variables \\(x_1, \\dots, x_{p - 1}\\) and a random error \\(\\epsilon\\). Its general form is:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_{p - 1} x_{p - 1} + \\epsilon \\qquad \\epsilon \\sim N(0, \\sigma^2)\\]\nUsually, the response and explanatory variables and error term are indexed by observation \\(i = 1, \\dots, n\\) so that the model describes a dataset comprising \\(n\\) values of each variable:\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_{p - 1} x_{i, p - 1} + \\epsilon_i \\qquad\\begin{cases} \\epsilon_i \\sim N(0, \\sigma^2) \\\\ i = 1, \\dots, n\\end{cases}\\]\nBecause the indices get confusing to keep track of, it is much easier to express the model in matrix form as\n\\[\\mathbf{y} = \\mathbf{X}\\beta + \\epsilon\\]\nwhere:\n\\[\\mathbf{y} = \\left[\\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array}\\right]_{\\;n \\times 1} \\qquad\n    \\mathbf{X} = \\left[\\begin{array}{cccc}\n        1 &x_{11} &\\cdots &x_{1, p - 1} \\\\\n        1 &x_{21} &\\cdots &x_{2, p - 1} \\\\\n        \\vdots &\\vdots &\\ddots &\\vdots \\\\\n        1 &x_{n1} &\\cdots &x_{n, p - 1}\n        \\end{array}\\right]_{\\;n \\times p} \\qquad\n    \\beta = \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{p - 1} \\end{array} \\right]_{\\;p \\times 1} \\qquad\n    \\epsilon = \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right]_{\\;n \\times 1}\\]\nFitting a model of this form means estimating the parameters \\(\\beta_0, \\beta_1, \\dots, \\beta_{p - 1}\\) and \\(\\sigma^2\\) from a set of data.\nWhen fitting a linear model, it is also of interest to quantify uncertainty by estimating the variability of \\(\\hat{\\beta}\\) and measure overall quality of fit. This lab illustrates that process and the computations involved.\nObjectives\nIn this lab, you’ll learn how to:\nThroughout you’ll use simple visualizations to help make the connection between fitted models and the aspects of a dataset that model features describe."
  },
  {
    "objectID": "labs/lab6-regression/lab6-regression-soln.html#estimation",
    "href": "labs/lab6-regression/lab6-regression-soln.html#estimation",
    "title": "Lab 6: Regression",
    "section": "Estimation",
    "text": "Estimation\n‘Fitting’ a model refers to computing estimates; statsmodels.OLS() will fit a linear regression model based on the response vector and explanatory variable matrix. Note that the model structure is implicit – OLS will fit \\(y = X\\beta + \\epsilon\\) no matter what, so you need to be sure you have arranged \\(X\\) and \\(y\\) correctly to fit the model that you intend.\n\n# fit model\nslr = sm.OLS(endog = y, exog = x)\n\nThis returns an object of a distinct model class specific to OLS:\n\ntype(slr)\n\nstatsmodels.regression.linear_model.OLS\n\n\nAssociated with the class are various attributes and methods. From the model instance, .fit() retrieves the model results:\n\ntype(slr.fit())\n\nstatsmodels.regression.linear_model.RegressionResultsWrapper\n\n\nNote, however, that slr.fit() will not produce any interesting output:\n\nslr.fit()\n\n&lt;statsmodels.regression.linear_model.RegressionResultsWrapper at 0x1c8be960c10&gt;\n\n\nWhat the .fit() method does is create a results object that contains parameter estimates and other quantities we might want to retrieve.\n\nrslt = slr.fit()\n\nThe coeffient estimates \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\) are:\n\nrslt.params\n\nconst                  7.511423\neduc_expected_yrs_f   -0.427472\ndtype: float64\n\n\nThe error variance estimate \\(\\hat{\\sigma}^2\\) is:\n\nrslt.scale\n\n0.43785919814090585\n\n\nIt was noted in lecture that the variances and covariances of \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\) are given by the matrix:\n\\[\\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1}\n  = \\left[\\begin{array}{cc}\n        \\text{var}\\hat{\\beta}_0 & \\text{cov}\\left(\\hat{\\beta}_0, \\hat{\\beta}_1\\right) \\\\\n        \\text{cov}\\left(\\hat{\\beta}_1, \\hat{\\beta}_0\\right) & \\text{var}\\hat{\\beta}_1\n        \\end{array}\\right]\\]\nSo we can estimate these quantities, which quantify the variation and covariation of the estimated coefficients, by plugging in the estimated error variance and computing \\(\\hat{\\sigma}^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\). This estimate is:\n\nrslt.cov_params()\n\n\n\n\n\n\n\n\nconst\neduc_expected_yrs_f\n\n\n\n\nconst\n0.068834\n-0.005782\n\n\neduc_expected_yrs_f\n-0.005782\n0.000509\n\n\n\n\n\n\n\nStandard errors for the coefficient estimates are obtained from the diagonal entries. We might create a nice summary of all the estimates as follows:\n\ncoef_tbl = pd.DataFrame({'estimate': rslt.params.values,\n              'standard error': np.sqrt(rslt.cov_params().values.diagonal())},\n              index = x.columns)\ncoef_tbl.loc['error variance', 'estimate'] = rslt.scale\n\ncoef_tbl\n\n\n\n\n\n\n\n\nestimate\nstandard error\n\n\n\n\nconst\n7.511423\n0.262362\n\n\neduc_expected_yrs_f\n-0.427472\n0.022560\n\n\nerror variance\n0.437859\nNaN\n\n\n\n\n\n\n\nLastly, a standard metric often reported with linear models is the \\(R^2\\) score, which is interpreted as the proportion of variation in the response captured by the model.\n\n# compute R-squared\nrslt.rsquared\n\n0.723814308777671\n\n\nSo, the expected years of education for women in a country explains 72% of variability in fertility rates, and furthermore, according to the fitted model:\n\nfor a country in which women are entirely uneducated, the estimated mean fertility rate is 7.5 children on average by the end of a woman’s reproductive period\neach additional year of education for women is associated with a decrease in a country’s fertility rate of an estimated 0.43\nafter accounting for women’s education levels, fertility rates vary by a standard deviation of \\(0.66 = \\sqrt{0.438}\\) across countries\n\n\nQuestion 3: center the explanatory variable\nNote that no countries report an expected zero years of education for women, so the meaning of the intercept is artificial. As we saw in lecture, centering the explanatory variable can improve interpretability of the intercept. Center the expected years of education for women and refit the model by following the steps outlined below. Display the coefficient estimates and standard errors.\n\n# center the education column by subtracting its mean from each value\neduc_ctr = reg_data.educ_expected_yrs_f - reg_data.educ_expected_yrs_f.mean() # SOLUTION\n\n# reconstruct the explanatory variable matrix\nx_ctr = sm.tools.add_constant(educ_ctr) # SOLUTION\n\n# fit new model\nslr_ctr = sm.OLS(endog = y, exog = x_ctr) # SOLUTION\nrslt_ctr = slr_ctr.fit() # SOLUTION\n\n# arrange estimates and se's in a dataframe and display\n# BEGIN SOLUTION\ncoef_tbl_ctr = pd.DataFrame({'estimate': rslt_ctr.params.values,\n              'standard error': np.sqrt(rslt_ctr.cov_params().values.diagonal())},\n              index = x_ctr.columns)\ncoef_tbl_ctr.loc['error variance', 'estimate'] = rslt_ctr.scale\n\ncoef_tbl_ctr\n# END SOLUTION\n\n\n\n\n\n\n\n\nestimate\nstandard error\n\n\n\n\nconst\n2.655168\n0.056125\n\n\neduc_expected_yrs_f\n-0.427472\n0.022560\n\n\nerror variance\n0.437859\nNaN\n\n\n\n\n\n\n\n\ngrader.check(\"q3\")"
  },
  {
    "objectID": "labs/lab6-regression/lab6-regression-soln.html#fitted-values-and-residuals",
    "href": "labs/lab6-regression/lab6-regression-soln.html#fitted-values-and-residuals",
    "title": "Lab 6: Regression",
    "section": "Fitted values and residuals",
    "text": "Fitted values and residuals\nThe fitted value for \\(y_i\\) is the value along the line specified by the model that corresponds to the matching explanatory variable \\(x_i\\). In other words:\n\\[\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\]\nThese can be obtained directly from rslt:\n\n# fitted values\nrslt.fittedvalues\n\nCountry\nAfghanistan            4.606441\nAlbania                1.868041\nAlgeria                2.335168\nAngola                 4.530275\nAntigua and Barbuda    1.990860\n                         ...   \nUruguay                2.177590\nUzbekistan             2.379686\nVanuatu                3.179536\nZambia                 3.773122\nZimbabwe               2.819952\nLength: 139, dtype: float64\n\n\nThe result is an array with length matching the number of rows in x; note the index for the pandas series – the fitted values are returned in the same order as the observations used to fit the model.\n\n(rslt.fittedvalues.index == x.index).all()\n\nTrue\n\n\nRecall that model residuals are the difference between observed and fitted values:\n\\[e_i = y_i - \\hat{y}_i\\]\nThese are similarly retrievable as an attribute of the regression results:\n\n# residuals\nrslt.resid\n\nCountry\nAfghanistan           -0.133441\nAlbania               -0.251041\nAlgeria                0.687832\nAngola                 0.988725\nAntigua and Barbuda    0.003140\n                         ...   \nUruguay               -0.204590\nUzbekistan             0.224314\nVanuatu                0.602464\nZambia                 0.859878\nZimbabwe               0.795048\nLength: 139, dtype: float64\n\n\nNote again that these are returned in the same order as the original observations.\n\nQuestion 4: calculations ‘by hand’\nCalculate the fitted values and residuals manually. Store the results as arrays fitted_manual and resid_manual, respectively.\nHint: use matrix-vector multiplication.\n\nfitted_manual = x.dot(rslt.params.values) # SOLUTION\nresid_manual = y - fitted_manual # SOLUTION\n\n\ngrader.check(\"q4\")\n\nIt is often convenient to add the fitted values and residuals as new columns in reg_data:\n\n# append fitted values and residuals\nreg_data['fitted_slr'] = rslt.fittedvalues\nreg_data['resid_slr'] = rslt.resid\n\nreg_data.head(3) \n\n\n\n\n\n\n\n\nfertility_total\neduc_expected_yrs_f\nhdi\nfitted_slr\nresid_slr\n\n\nCountry\n\n\n\n\n\n\n\n\n\nAfghanistan\n4.473\n6.795722\n0.509\n4.606441\n-0.133441\n\n\nAlbania\n1.617\n13.201755\n0.792\n1.868041\n-0.251041\n\n\nAlgeria\n3.023\n12.108990\n0.746\n2.335168\n0.687832\n\n\n\n\n\n\n\nWe can use this augmented dataframe to visualize the deterministic part of the model:\n\n# construct line plot\nslr_line = alt.Chart(reg_data).mark_line().encode(\n    x = 'educ_expected_yrs_f',\n    y = 'fitted_slr'\n)\n\n# layer\nscatter_educ + slr_line\n\n\n\n\n\n\nTo obtain uncertainty bands about the estimated mean, we’ll compute predictions at each observed value using .get_prediction() – this method by default returns standard errors associated with each prediction.\n\npreds = rslt.get_prediction(x)\n\nThe predictions are stored as .predicted_mean. Since we computed predictions at the observed values, the predictions should match the fitted values:\n\n(preds.predicted_mean == rslt.fittedvalues).all()\n\nTrue\n\n\nStandard errors are stored as .se_mean. Uncertainty bands are typically drawn \\(2SE\\) in either direction from the fitted values; so we’ll append those values to the original data.\n\nreg_data['lwr_mean'] = preds.predicted_mean - 2*preds.se_mean\nreg_data['upr_mean'] = preds.predicted_mean + 2*preds.se_mean\n\nreg_data.head()\n\n\n\n\n\n\n\n\nfertility_total\neduc_expected_yrs_f\nhdi\nfitted_slr\nresid_slr\nlwr_mean\nupr_mean\n\n\nCountry\n\n\n\n\n\n\n\n\n\n\n\nAfghanistan\n4.473\n6.795722\n0.509\n4.606441\n-0.133441\n4.371882\n4.841001\n\n\nAlbania\n1.617\n13.201755\n0.792\n1.868041\n-0.251041\n1.728389\n2.007693\n\n\nAlgeria\n3.023\n12.108990\n0.746\n2.335168\n0.687832\n2.217945\n2.452390\n\n\nAngola\n5.519\n6.973901\n0.582\n4.530275\n0.988725\n4.302742\n4.757808\n\n\nAntigua and Barbuda\n1.994\n12.914441\n0.772\n1.990860\n0.003140\n1.858509\n2.123210\n\n\n\n\n\n\n\nWe can use these to shade in the area between the lower and upper limits.\n\nband = alt.Chart(reg_data).mark_area(opacity = 0.2).encode(\n    x = 'educ_expected_yrs_f',\n    y = 'lwr_mean',\n    y2 = 'upr_mean'\n)\n\n# layer\nscatter_educ + slr_line + band\n\n\n\n\n\n\nAs discussed in lecture, we can also compute and display uncertainty bounds for predicted observations (rather than the mean). These will be wider, because there is more uncertainty associated with predicting observations compared with estimating the mean.\n\n\nQuestion 5: prediction intervals\nThe standard error for predictions is stored with the output of .get_prediction() as the attribute .se_obs – standard error for observations. Use this and follow the example above to compute 95% uncertainty bounds for the observations. Add the lower and upper bounds as new columns of reg_data named lwr_obs and upr_obs, respectively. Construct a plot showing data scatter, the model predictions, and prediction uncertainty bands.\n\n# compute prediction uncertainty bounds\nreg_data['lwr_obs'] = preds.predicted_mean - 2*preds.se_obs # SOLUTION\nreg_data['upr_obs'] = preds.predicted_mean + 2*preds.se_obs # SOLUTION\n\n# construct plot showing prediction uncertainty\n# BEGIN SOLUTION\nband_pred = alt.Chart(reg_data).mark_area(opacity = 0.2).encode(\n    x = 'educ_expected_yrs_f',\n    y = 'lwr_obs',\n    y2 = 'upr_obs'\n)\n\n# layer\nscatter_educ + slr_line + band_pred\n# END SOLUTION\n\n\n\n\n\n\n\ngrader.check(\"q5\")\n\nRecall that the interpretation of the prediction band is that 95% of the time, the band will cover the observed values.\n\n\nQuestion 6: coverage\nWhat proportion of observed values are within the prediction bands? Compute and store this value as coverage_prop.\n\ncoverage_prop = np.mean((reg_data.lwr_obs &lt; reg_data.fertility_total) & (reg_data.fertility_total &lt; reg_data.upr_obs)) # SOLUTION\n\n\ngrader.check(\"q6\")"
  },
  {
    "objectID": "labs/lab6-regression/data/data-preprocessing.html",
    "href": "labs/lab6-regression/data/data-preprocessing.html",
    "title": "HDR preprocessing",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n# import hdi data\nhdi = pd.read_csv('raw/hdi_1990-2019.csv', \n                  encoding = 'latin1', \n                  na_values = '..'\n).loc[:, ['Country', '2018']].rename(\n    columns = {'2018': 'hdi'}\n)\n\n# import gdi data\ngdi = pd.read_csv('raw/gdi_1995-2019.csv', \n                  encoding = 'latin1', \n                  na_values = '..'\n).loc[:, ['Country', '2018']].rename(\n    columns = {'2018': 'gdi'}\n)\n\n# merge hdi and ihdi and gdi\nhdr_data = pd.merge(hdi, gdi, how = 'inner', on = 'Country')\n\n# trim whitespace from country names\nhdr_data['Country'] = hdr_data.Country.str.strip()\n\n# preview\nhdr_data.head()\n\n\n\n\n\n\n\n\nCountry\nhdi\ngdi\n\n\n\n\n0\nAfghanistan\n0.509\n0.663\n\n\n1\nAlbania\n0.792\n0.971\n\n\n2\nAlgeria\n0.746\n0.860\n\n\n3\nAndorra\n0.867\nNaN\n\n\n4\nAngola\n0.582\n0.903"
  },
  {
    "objectID": "labs/lab6-regression/data/data-preprocessing.html#wdi-preprocessing",
    "href": "labs/lab6-regression/data/data-preprocessing.html#wdi-preprocessing",
    "title": "HDR preprocessing",
    "section": "WDI preprocessing",
    "text": "WDI preprocessing\n\n# import world bank development indicators\nwdi = pd.read_csv('raw/wdi-data.csv', na_values = '..')\n\n# replace column names by variable codes\ncodenames = np.append(wdi.columns[0:4].values.tolist(), wdi.columns[4:].str.extract('.*\\[(.*)\\].*').values.tolist())\nwdi.columns = codenames\n\n# substitue short names for variable codes\nvarnames = pd.read_csv('raw/wdi-variablenames.csv')\ncode_dict = varnames.loc[:, ['Code', 'New name']].set_index('Code').transpose().to_dict('records')[0]\nwdi = wdi.rename(columns = code_dict).set_index(['Time', 'Country Name'])\n\n# preview\nwdi.head()\n\n\n\n\n\n\n\n\n\nTime Code\nCountry Code\neduc_bach_f\neduc_sec_f\neduc_psec_f\neduc_prim_f\neduc_tert_f\neduc_upsec_f\neduc_master_f\neduc_phd_f\n...\nmortality_infant_f\nmortality_infant_m\nmortality_child_f\nmortality_child_m\nmortality_suicide_f\nmortality_suicide_m\nfertility_total\nfertility_adolescent\ncontraceptive_any\ncontraceptive_modern\n\n\nTime\nCountry Name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2010\nAfghanistan\nYR2010\nAFG\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n60.0\n67.9\n83.7\n91.3\n3.9\n4.6\n5.977\n113.7150\n21.8\n19.9\n\n\nAlbania\nYR2010\nALB\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n10.3\n13.3\n11.9\n14.6\n6.1\n9.5\n1.660\n19.8208\nNaN\nNaN\n\n\nAlgeria\nYR2010\nDZA\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n22.1\n25.0\n25.9\n28.9\n2.2\n3.8\n2.860\n10.8084\nNaN\nNaN\n\n\nAmerican Samoa\nYR2010\nASM\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nAndorra\nYR2010\nAND\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n3.8\n4.6\n4.1\n5.1\nNaN\nNaN\n1.270\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 61 columns\n\n\n\n\n# non-gender-specific variables\nvariable_set1 = ['fertility_total', 'fertility_adolescent', 'gdp_percap', 'immunization_dpt', 'immunization_measles']\n\n# variables to remove\nvariable_drop = ['pop_65up', 'pop_15to64', 'pop_0to14', 'mortality_infant_f', 'mortality_infant_m', 'mortality_child_f', 'mortality_maternal', 'mortality_child_m', 'mortality_suicide_f', 'mortality_suicide_m', 'contraceptive_any', 'educ_expected_yrs']\n\n# separate gender-specific and non-gender specific variables\nwdi_gender = wdi.drop(columns = np.append(variable_set1, variable_drop))\nwdi_nongender = wdi.loc[:, variable_set1]\n\n# average immunization rates\nwdi_nongender['immunization'] = wdi_nongender.iloc[:, 3:5].mean(axis = 1)\n\n# drop individual immunization rates, slice 2018 data, rename country\nwdi_nongender = wdi_nongender.drop(\n    columns = ['immunization_dpt', 'immunization_measles']\n).loc[2018, :].reset_index().rename(\n    columns = {'Country Name': 'Country'}\n)"
  },
  {
    "objectID": "labs/lab6-regression/data/data-preprocessing.html#export-datasets",
    "href": "labs/lab6-regression/data/data-preprocessing.html#export-datasets",
    "title": "HDR preprocessing",
    "section": "Export datasets",
    "text": "Export datasets\n\n# merge nongender data with hdr data\nnongender_data = pd.merge(wdi_nongender, hdr_data, how = 'inner', on = 'Country').set_index('Country')\n\n# separate\nfertility_rates = nongender_data.iloc[:, 0:2].reset_index()\ncountry_indicators = nongender_data.iloc[:, 2:6].reset_index()\n\n# slice and reformat gender-specific variables\ngender_data = wdi_gender.loc[2018].reset_index().rename(\n    columns = {'Country Name': 'Country'}\n).drop(columns = ['Time Code'])\n\n\ngender_data.to_csv('gender-data.csv', index = False)\nfertility_rates.to_csv('fertility.csv', index = False)\ncountry_indicators.to_csv('country-indicators.csv', index = False)"
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html",
    "title": "Lab 0: Getting started",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab0-gettingstarted.ipynb\")\nThis lab is meant to help you familiarize yourself with using the LSIT server and Jupyter notebooks. Some light review of numpy arrays is also included."
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html#jupyter-notebooks",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html#jupyter-notebooks",
    "title": "Lab 0: Getting started",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nJupyter notebooks are organized into ‘cells’ that can contain either text or codes. For example, this is a text cell.\nTechnically, Jupyter is an application/interface that runs atop a kernel – a programming-language-specific independent environment in which code cells are executed. This basic organization allows for interactive computing with text integration.\nSelecting a cell and pressing Enter will enter edit mode and allow you to edit the cell. From edit mode, pressing Esc will revert to command mode and allow you to navigate the notebook’s cells.\nIn edit mode, most of the keyboard is dedicated to typing into the cell’s editor. Thus, in edit mode there are relatively few shortcuts. In command mode, the entire keyboard is available for shortcuts, so there are many more. Here are a few useful ones:\n\nCtrl + Return : Evaluate the current cell\nShift + Return: Evaluate the current cell and move to the next\nSaving the notebook: s\nBasic navigation: up one cell k, down one cell j\na : create a cell above\nb : create a cell below\ndd : delete a cell\nz : undo the last cell operation\nm : convert a cell to markdown\ny : convert a cell to code\n\nTake a moment to find out what the following commands do:\n\nCell editing: x, c, v, z\nKernel operations: i, 0 (press twice)\n\n\n# Practice the above commands on this cell\n\n\nRunning Cells and Displaying Output\nRun the following cell.\n\nprint(\"Hello, World!\")\n\nHello, World!\n\n\nIn Jupyter notebooks, all print statements are displayed below the cell. Furthermore, the output of only the last line is displayed following the cell upon execution.\n\n\"Will this line be displayed?\"\n\nprint(\"Hello\" + \",\", \"world!\")\n\n5 + 3\n\nHello, world!\n\n\n8\n\n\n\n\nViewing Documentation\nTo output the documentation for a function, use the help() function.\n\nhelp(print)\n\nHelp on built-in function print in module builtins:\n\nprint(...)\n    print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False)\n    \n    Prints the values to a stream, or to sys.stdout by default.\n    Optional keyword arguments:\n    file:  a file-like object (stream); defaults to the current sys.stdout.\n    sep:   string inserted between values, default a space.\n    end:   string appended after the last value, default a newline.\n    flush: whether to forcibly flush the stream.\n\n\n\nYou can also use Jupyter to view function documentation inside your notebook. The function must already be defined in the kernel for this to work.\nBelow, click your mouse anywhere on print() and use Shift + Tab to view the function’s documentation.\n\nprint('Welcome to this course!')\n\nWelcome to this course!\n\n\n\n\nImporting Libraries\nIn this course, we will be using common Python libraries to help us retrieve, manipulate, and perform operations on data. By convention, we import all libraries at the very top of the notebook. There are also a set of standard aliases that are used to shorten the library names. Below are some of the libraries that you may encounter throughout the course, along with their respective aliases.\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html#practice-questions-and-numpy-review",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html#practice-questions-and-numpy-review",
    "title": "Lab 0: Getting started",
    "section": "Practice questions and numpy review",
    "text": "Practice questions and numpy review\nMost assignments for this class will be given as notebooks organized into explanation and prompts followed by response cells; you will complete assignments by filling in all of the response cells.\nMany response cells are followed by a test cell that performs a few checks on your work. Please be aware that test cells don’t always confirm that your response is correct or incorrect. They are meant to give you some useful feedback, but it’s your responsibility to interpret the feedback – please be sure to read and think about test output if tests fail, and make your own assessment of whether you need to revise your response.\nBelow are a few practice questions for you to familiarize yourself with the process. These assume familiarity with basic python syntax and the numpy package.\n\nQuestion 1\nWrite a function summation that evaluates the following summation for \\(n \\geq 1\\):\n\\[\\sum_{i=1}^{n} \\left(i^3 + 5 i^3\\right)\\]\nHint: np.arange(5).sum() will generate an array comprising \\(1, 2, \\dots, 5\\) and then add up the elements of the array.\n\ndef summation(n):\n    \"\"\"Compute the summation i^3 + 5 * i^3 for 1 &lt;= i &lt;= n.\"\"\"\n    # BEGIN SOLUTION\n    out = (6*(np.arange(n + 1)**3)).sum()\n    return out\n    # END SOLUTION\n\n\ngrader.check(\"q1\")\n\nUse your function to compute the sum for…\n\n# n = 2\n...\n\n\n# n = 20\n...\n\n\n\nQuestion 2\nThe core of numpy is the array. Let’s use np.array to create an array. It takes a sequence, such as a list or range (remember that list elements are included between the square brackets [ and ], such as [1, 5, 3]).\nBelow, create an array containing the values 1, 2, 3, 4, and 5 (in that order) and assign it the name my_array.\n\nmy_array = np.array([1, 2, 3, 4, 5]) #SOLUTION\n\n\ngrader.check(\"q2\")\n\nNumpy arrays are integer-indexed by position, with the first element indexed as position 0. Elements can be retrieved by enclosing the desired positions in brackets [].\n\nmy_array[3]\n\n4\n\n\nTo retrieve consecutive positions, specify the starting index and the ending index separated by :, for instance, arr[from:to]. This syntax is non-inclusive of the left endpoint, meaning that the starting index is not included in the output.\n\nmy_array[2:4]\n\narray([3, 4])\n\n\nIn addition to values in the array, we can access attributes such as array’s shape and data type that can be retrieved by name using syntax of the form array.attr. Some useful attributes are:\n\n.shape, a tuple with the length of each array dimension\n.size, the length of the first array dimension\n.dtype, the data type of the entries (float, integer, etc.)\n\nA full list of attributes is here.\n\nmy_array.shape\n\n(5,)\n\n\n\nmy_array.size\n\n5\n\n\n\nmy_array.dtype\n\ndtype('int32')\n\n\nArrays, unlike Python lists, cannot store items of different data types.\n\n# A regular Python list can store items of different data types\n[1, '3']\n\n[1, '3']\n\n\n\n# Arrays will convert everything to the same data type\nnp.array([1, '3'])\n\narray(['1', '3'], dtype='&lt;U11')\n\n\n\n# Another example of array type conversion\nnp.array([5, 8.3])\n\narray([5. , 8.3])\n\n\nArrays are also useful in performing vectorized operations. Given two or more arrays of equal length, arithmetic will perform element-wise computations across the arrays.\nFor example, observe the following:\n\n# Python list addition will concatenate the two lists\n[1, 2, 3] + [4, 5, 6]\n\n[1, 2, 3, 4, 5, 6]\n\n\n\n# NumPy array addition will add them element-wise\nnp.array([1, 2, 3]) + np.array([4, 5, 6])\n\narray([5, 7, 9])\n\n\nArrays can be subsetted by index position, as shown above, or by a logical vector of the same length. For example:\n\nexample_arr = np.arange(4, 10)\nexample_arr\n\narray([4, 5, 6, 7, 8, 9])\n\n\nSuppose we want the last three elements. One option is to use index position:\n\nexample_arr[3:6]\n\narray([7, 8, 9])\n\n\nOr a logical vector:\n\nexample_arr[np.array([False, False, False, True, True, True])]\n\narray([7, 8, 9])\n\n\nThe latter approach allows one to subset based on a condition defined by the values of the vector. For example, we can use the condition \\(x \\geq 7\\) to obtain the logical vector used above.\n\nexample_arr &gt;= 7\n\narray([False, False, False,  True,  True,  True])\n\n\nAnd then we can subset just as before:\n\nexample_arr[example_arr &gt;= 7]\n\narray([7, 8, 9])\n\n\nYou’ll see this done frequently, and it’s sometimes referred to as filtering, because we’re selectively removing values.\n\n\nQuestion 3\nGiven the array random_arr, create an array containing all values \\(x\\) such that \\(2x^4 &gt; 1\\). Name the array valid_values.\n\n# for reproducibility - setting the seed will result in the same random draw each time\nnp.random.seed(42)\n\n# draw 60 uniformly random integers between 0 and 1\nrandom_arr = np.random.rand(60)\n\n# solution here\nvalid_values = random_arr[2*(random_arr**4) &gt; 1] # SOLUTION\n\n\ngrader.check(\"q3\")\n\n\n\nA note on np.arange and np.linspace\nUsually we use np.arange to return an array that steps from a to b with a fixed step size s. While this is fine in some cases, we sometimes prefer to use np.linspace(a, b, N), which divides the interval [a, b] into N equally spaced points.\nnp.arange(start, stop, step) produces an array with all the numbers starting at start, incremendted up by step, stopping before stop is reached. For example, the value of np.arange(1, 6, 2) is an array with elements 1, 3, and 5 – it starts at 1 and counts up by 2, then stops before 6. np.arange(4, 9, 1) is an array with elements 4, 5, 6, 7, and 8. (It doesn’t contain 9 because np.arange stops before the stop value is reached.)\nnp.linspace always includes both end points while np.arange will not include the second end point b. For this reason, especially when we are plotting ranges of values we tend to prefer np.linspace.\nNotice how the following two statements have different parameters but return the same result.\n\nnp.arange(-5, 6, 1.0)\n\narray([-5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.])\n\n\n\nnp.linspace(-5, 5, 11)\n\narray([-5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.])\n\n\nCheck your understanding. Will np.arange(1, 10) produce an array that contains 10? Add a cell below and check to confirm your answer."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html",
    "title": "Lab 2: Sampling designs and statistical bias",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab2-sampling.ipynb\")\nimport numpy as np\nimport pandas as pd\nimport altair as alt\n# disable row limit for plotting\nalt.data_transformers.disable_max_rows()\n# uncomment to ensure graphics display with pdf export\n# alt.renderers.enable('mimetype')\nIn this lab you’ll explore through simulation how nonrandom sampling can produce datasets with statistical properties that are distored relative to the population that the sample was drawn from. This kind of distortion is known as bias.\nIn common usage, the word ‘bias’ means disproportion or unfairness. In statistics, the concept has the same connotation – biased sampling favors certain observational units over others, and biased estimates are estimates that favor larger or smaller values than the truth. The goal of this lab is to refine your understanding about what statistical bias is and is not and develop your intuition about potential mechanisms by which bias is introduced and the effect that this can have on sample statistics.\nObjectives:"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html#sampling-designs",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html#sampling-designs",
    "title": "Lab 2: Sampling designs and statistical bias",
    "section": "Sampling designs",
    "text": "Sampling designs\nThe sampling design of a study refers to the way observational units are selected from the collection of all observational units. Any design can be expressed by the probability that each unit is included in the sample. In a random sample, all units are equally likely to be included.\nFor example, you might want to learn about U.S. residents (population), but only be able for ethical or practical reasons to study adults (sampling frame), and decide to do a mail survey of 2000 randomly selected addresses in each state (sampling design). Each collection of 2000 addresses may constitute a random sample of households, but even with a 100% response rate the survey results will not be a random sample of adult U.S. residents because individuals share addresses and the population sizes are different from state to state."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html#bias",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html#bias",
    "title": "Lab 2: Sampling designs and statistical bias",
    "section": "Bias",
    "text": "Bias\nFormally, bias describes the ‘typical’ deviation of a sample statistic the correspongind population value.\nFor example, if a particular sampling design tends to produce an average measurement around 1.5 units, but the true average in the population is 2 units, then the estimate has a bias of -0.5 units. The language ‘typical’ and ‘tends to’ is important here. Estimates are never perfect, so just because an estimate is off by -0.5 units for one sample doesn’t make it biased – it is only biased if it is consistently off.\nAlthough bias is technically a property of a sample statistic (like the sample average), it’s common to talk about a biased sample – this term refers to a dataset collected using a sampling design that produces biased statistics.\nThis is exactly what you’ll explore in this lab – the relationship between sampling design and bias."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html#simulated-data",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html#simulated-data",
    "title": "Lab 2: Sampling designs and statistical bias",
    "section": "Simulated data",
    "text": "Simulated data\nYou will be simulating data in this lab. Simulation is a great means of exploration because you can control the population properties, which are generally unknown in practice.\nWhen working with real data, you just have one dataset, and you don’t know any of the properties of the population or what might have happened if a different sample were collected. That makes it difficult to understand sampling variation and impossible to directly compare the sample properties to the population properties.\nWith simulated data, by contrast, you control how data are generated with exact precision – so by extension, you know everything there is to know about the population. In addition, repeated simulation of data makes it possible to explore the typical behavior of a particular sampling design, so you can learn ‘what usually happens’ for a particular sampling design by direct observation."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html#hypothetical-population",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html#hypothetical-population",
    "title": "Lab 2: Sampling designs and statistical bias",
    "section": "Hypothetical population",
    "text": "Hypothetical population\nTo provide a little context to this scenario, imagine that you’re measuring eucalyptus seeds to determine their typical diameter. The cell below simulates diameter measurements for a hypothetical population of 5000 seeds; imagine that this is the total number of seeds in a small grove at some point in time.\n\n# simulate seed diameters\nnp.random.seed(40221) # for reproducibility\npopulation = pd.DataFrame(\n    data = {'diameter': np.random.gamma(shape = 2, scale = 1/2, size = 5000), \n            'seed': np.arange(5000)}\n).set_index('seed')\n\n# check first few rows\npopulation.head(3)\n\n\n\n\n\n\n\n\ndiameter\n\n\nseed\n\n\n\n\n\n0\n0.831973\n\n\n1\n1.512187\n\n\n2\n0.977392\n\n\n\n\n\n\n\n\nQuestion 1\nCalculate the mean diameter for the hypothetical population and store the value as mean_diameter.\n\nmean_pop_diameter = population.diameter.mean() #SOLUTION\n\nmean_pop_diameter\n\n1.0189291497049837\n\n\n\ngrader.check(\"q1\")\n\n\n\nQuestion 2\nCalculate the standard deviation of diameters for the hypothetical population and store the value as std_dev_pop_diameter.\n\nstd_dev_pop_diameter = population.diameter.std() # SOLUTION\nstd_dev_pop_diameter\n\n0.7239297185874436\n\n\n\ngrader.check(\"q2\")\n\nThe cell below produces a histogram of the population values – the distribution of diameter measurements among the hypothetical population – with a vertical line indicating the population mean.\n\n# base layer\nbase_pop = alt.Chart(population).properties(width = 400, height = 300)\n\n# histogram of diameter measurements\nhist_pop = base_pop.mark_bar(opacity = 0.8).encode(\n    x = alt.X('diameter', \n              bin = alt.Bin(maxbins = 20), \n              title = 'Diameter (mm)', \n              scale = alt.Scale(domain = (0, 6))),\n    y = alt.Y('count()', title = 'Number of seeds in population')\n)\n\n# vertical line for population mean\nmean_pop = base_pop.mark_rule(color='blue').encode(\n    x = 'mean(diameter)'\n)\n\n# display\nhist_pop + mean_pop"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html#random-sampling",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html#random-sampling",
    "title": "Lab 2: Sampling designs and statistical bias",
    "section": "Random sampling",
    "text": "Random sampling\nImagine that your sampling design involves collecting bunches of plant material from several locations in the grove and sifting out the seeds with a fine sieve until you obtaining 250 seeds. We’ll suppose that using your collection method, any of the 5000 seeds is equally likely to be obtained, so that your 250 seeds comprise a random sample of the population.\nWe can simulate samples obtained using your hypothetical design by drawing values without replacement from the population.\n\n# draw a random sample of seeds\nnp.random.seed(40221) # for reproducibility\nsample = population.sample(n = 250, replace = False)\n\n\nQuestion 3\nCalculate the mean diameter of seeds in the simulated sample and store the value as mean_sample_diameter.\n\nmean_sample_diameter = sample.diameter.mean() # SOLUTION\nmean_sample_diameter\n\n0.9777218824084053\n\n\n\ngrader.check(\"q3\")\n\nYou should see above that the sample mean is close to the population mean. In fact, all sample statistics are close to the population; this can be seen by comparing the distribution of sample values with the distribution of population values.\n\n# base layer\nbase_samp = alt.Chart(sample).properties(width = 400, height = 300)\n\n# histogram of diameter measurements\nhist_samp = base_samp.mark_bar(opacity = 0.8).encode(\n    x = alt.X('diameter', \n              bin = alt.Bin(maxbins = 20),\n              scale = alt.Scale(domain = (0, 6)),\n              title = 'Diameter (mm)'),\n    y = alt.Y('count()', title = 'Number of seeds in sample')\n)\n\n# vertical line for population mean\nmean_samp = base_samp.mark_rule(color='blue').encode(\n    x = 'mean(diameter)'\n)\n\n# display\nhist_samp + mean_samp | hist_pop + mean_pop\n\n\n\n\n\n\nWhile there are some small differences, the overall shape is similar and the sample mean is almost exactly the same as the population mean. So with this sampling design, you obtained a dataset with few distortions of the population properties, and the sample mean is a good estimate of the population mean.\n\n\nAssessing bias through simulation\nYou may wonder: does that happen all the time, or was this just a lucky draw? This question can be answered by simulating a large number of samples and checking the average behavior to see whether the undistorted representation of the population is typical for this sampling design.\nThe cell below estimates the bias of the sample mean by:\n\ndrawing 1000 samples of size 300;\nstoring the sample mean from each sample;\ncomputing the average difference between the sample means and the population mean.\n\n\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\nnsim = 1000\n\n# storage for the sample means\nsamp_means = np.zeros(nsim)\n\n# repeatedly sample and store the sample mean\nfor i in range(0, nsim):\n    samp_means[i] = population.sample(n = 250, replace = False).diameter.mean()\n\nThe bias of the sample mean is its average distance from the population mean. We can estimate this using our simulation results as follows:\n\n# bias\nsamp_means.mean() - population.diameter.mean()\n\n-0.0012458197406362004\n\n\nSo the average error observed in 1000 simulations was about 0.001 mm! This suggests that the sample mean is unbiased: on average, there is no error. Therefore, at least with respect to estimating the population mean, random samples appear to be unbiased samples.\nHowever, unbiasedness does not mean that you won’t observe estimation error. There is a natural amount of variability from sample to sample, because in each sample a different collection of seeds is measured. We can estimate this as well using the simulation results by checking the standard deviation of the sample means across all 1000 samples:\n\nsamp_means.std()\n\n0.04285044949708931\n\n\nSo on average, the sample mean varies by about 0.04 mm from sample to sample.\nWe could also check how much the sample mean deviates from the population mean on average by computing root mean squared error:\n\nnp.sqrt(np.sum((samp_means - population.diameter.mean())**2)/1000)\n\n0.0428685559463899\n\n\nNote that this is very close to the variance of the sample mean across simulations, but not exactly the same; this latter calculation measures the spread around the population mean, and is a conventional measure of estimation accuracy.\nThe cell below plots a histogram representing the distribution of values of the sample mean across the 1000 samples you simulated (this is known as the sampling distribution of the sample mean). It shows a peak right at the population mean (blue vertical line) but some symmetric variation to either side – most values are between about 0.93 and 1.12.\n\n# plot the simulated sampling distribution\nsampling_dist = alt.Chart(pd.DataFrame({'sample mean': samp_means})).mark_bar().encode(\n    x = alt.X('sample mean', bin = alt.Bin(maxbins = 30), title = 'Value of sample mean'),\n    y = alt.Y('count()', title = 'Number of simulations')\n)\n\nsampling_dist + mean_pop"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html#biased-sampling",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html#biased-sampling",
    "title": "Lab 2: Sampling designs and statistical bias",
    "section": "Biased sampling",
    "text": "Biased sampling\nIn this scenario, you’ll use the same hypothetical population of eucalyptus seed diameter measurements and explore the impact of a biased sampling design.\nIn the first design, you were asked to imagine that you collected and sifted plant material to obtain seeds. Suppose you didn’t know that the typical seed is about 1mm in diameter and decided to use a sieve that is a little too coarse, tending only to sift out larger seeds and letting smaller seeds pass through. As a result, small seeds have a lower probability of being included in the sample and large seeds have a higher probability of being included in the sample.\nThis kind of sampling design can be described by assigning differential sampling weights \\(w_1, \\dots, w_N\\) to each observation. The cell below defines some hypothetical weights such that larger diameters are more likely to be sampled.\n\npopulation_mod1 = population.copy()\n\n\n# inclusion weight as a function of seed diameter\ndef weight_fn(x, r = 10, c = 1.5):\n    out = 1/(1 + np.e**(-r*(x - c)))\n    return out\n\n# create a grid of values to use in plotting the function\ngrid = np.linspace(0, 6, 100)\nweight_df = pd.DataFrame(\n    {'seed diameter': grid,\n     'weight': weight_fn(grid)}\n)\n\n# plot of inclusion probability against diameter\nweight_plot = alt.Chart(weight_df).mark_area(opacity = 0.3, line = True).encode(\n    x = 'seed diameter',\n    y = 'weight'\n).properties(height = 100)\n\n# show plot\nweight_plot\n\n\n\n\n\n\nThe actual probability that a seed is included in the sample – its inclusion probability – is proportional to the sampling weight. These inclusion probabilities \\(\\pi_i\\) can be calculated by normalizing the weights \\(w_i\\) over all seeds in the population \\(i = 1, \\dots, 5000\\):\n\\[\\pi_i = \\frac{w_i}{\\sum_i w_i}\\]\nIt may help you to picture how the weights will be used in sampling to line up this plot with the population distribution. In effect, we will sample more from the right tail of the population distribution, where the weight is nearest to 1.\n\nhist_pop & weight_plot\n\n\n\n\n\n\nThe following cell draws a sample with replacement from the hypothetical seed population with seeds weighted according to the inclusion probability given by the function above.\n\n# assign weight to each seed\npopulation_mod1['weight'] = weight_fn(population_mod1.diameter)\n\n# draw weighted sample\nnp.random.seed(40721)\nsample2 = population_mod1.sample(n = 250, replace = False, weights = 'weight').loc[:, ['diameter']]\n\n\nQuestion 4\nCalculate the mean diameter of seeds in the simulated sample and store the value as mean_sample2_diameter.\n\nmean_sample2_diameter = sample2.diameter.mean() #SOLUTION\n\nmean_sample2_diameter\n\n2.026720254934241\n\n\n\ngrader.check(\"q4\")\n\n\n\n\nQuestion 5\nShow side-by-side plots of the distribution of sample values and the distribution of population values, with vertical lines indicating the corresponding mean on each plot.\nHint: copy the cell that produced this plot in scenario 1 and replace sample with sample2. Utilizing different methods is also welcome.\n\n# BEGIN SOLUTION NO PROMPT\n\n# base layer\nbase_samp = alt.Chart(sample2).properties(width = 400, height = 300)\n\n# histogram of diameter measurements\nhist_samp = base_samp.mark_bar(opacity = 0.8).encode(\n    x = alt.X('diameter', \n              bin = alt.Bin(maxbins = 20),\n              scale = alt.Scale(domain = (0, 6)),\n              title = 'Diameter (mm)'),\n    y = alt.Y('count()', title = 'Number of seeds in sample')\n)\n\n# vertical line for population mean\nmean_samp = base_samp.mark_rule(color='blue').encode(\n    x = 'mean(diameter)'\n)\n\n# END SOLUTION\n\"\"\" # BEGIN PROMPT\n# base layer\nbase_samp = ...\n\n# histogram of diameter measurements\nhist_samp = ...\n\n# vertical line for population mean\nmean_samp = ...\n\n# combine layers\n\"\"\" # END PROMPT\n\n# display\nhist_samp + mean_samp | hist_pop + mean_pop\n\n\n\n\n\n\n\n\n\nAssessing bias through simulation\nHere you’ll mimic the simulation done in scenario 1 to assess the bias of the sample mean under this new sampling design.\n\npopulation_mod1.head()\n\n\n\n\n\n\n\n\ndiameter\nweight\n\n\nseed\n\n\n\n\n\n\n0\n0.831973\n0.001254\n\n\n1\n1.512187\n0.530430\n\n\n2\n0.977392\n0.005346\n\n\n3\n2.874944\n0.999999\n\n\n4\n0.506508\n0.000048\n\n\n\n\n\n\n\n\n\nQuestion 6\nInvestigate the bias of the sample mean by:\n\ndrawing 1000 samples with observations weighted by inclusion probability;\nstoring the collection of sample means from each sample as samp_means;\ncomputing the average difference between the sample means and the population mean (in that order!) and storing the result as avg_diff.\n\n(Hint: copy the cell that performs this simulation in scenario 1, and be sure to replace population with population_mod1 and adjust the sampling step to include weights = ... with the appropriate argument.)\n\n# BEGIN SOLUTION NO PROMPT\n\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\nnsim = 1000\n\n# storage for the sample means\nsamp_means = np.zeros(nsim)\n\n# repeatedly sample and store the sample mean\nfor i in range(0, nsim):\n    samp_means[i] = population_mod1.sample(\n        n = 250, \n        replace = False, \n        weights = 'weight'\n    ).diameter.mean()\n\n# bias\navg_diff = samp_means.mean() - population_mod1.diameter.mean()\n\n# END SOLUTION\n\"\"\" # BEGIN PROMPT\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\n\n# storage for the sample means\n\n# repeatedly sample and store the sample mean in the samp_means array\n\n# bias\n\"\"\" # END PROMPT\n\navg_diff\n\n1.0576986191465758\n\n\n\ngrader.check(\"q6\")\n\n\n\n\nQuestion 7\nDoes this sampling design seem to introduce bias? If so, does the sample mean tend to over-estimate or under-estimate the population mean?\nType your answer here, replacing this text.\nSOLUTION: Yes, this sampling design seems to introduce bias, where the sample mean tends to over-estimate the population mean."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html#hypothetical-population-1",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html#hypothetical-population-1",
    "title": "Lab 2: Sampling designs and statistical bias",
    "section": "Hypothetical population",
    "text": "Hypothetical population\nSuppose you’re interested in determining the average beak-to-tail length of red-tailed hawks to help differentiate them from other hawks by sight at a distance. Females and males differ slightly in length – females are generally larger than males. The cell below generates length measurements for a hypothetical population of 3000 females and 2000 males.\n\n# for reproducibility\nnp.random.seed(40721)\n\n# simulate hypothetical population\nfemale_hawks = pd.DataFrame(\n    data = {'length': np.random.normal(loc = 57.5, scale = 3, size = 3000),\n            'sex': np.repeat('female', 3000)}\n)\n\nmale_hawks = pd.DataFrame(\n    data = {'length': np.random.normal(loc = 50.5, scale = 3, size = 2000),\n            'sex': np.repeat('male', 2000)}\n)\n\npopulation_hawks = pd.concat([female_hawks, male_hawks], axis = 0)\n\n# preview\npopulation_hawks.groupby('sex').head(2)\n\n\n\n\n\n\n\n\nlength\nsex\n\n\n\n\n0\n53.975230\nfemale\n\n\n1\n60.516768\nfemale\n\n\n0\n53.076663\nmale\n\n\n1\n49.933166\nmale\n\n\n\n\n\n\n\nThe cell below produces a histogram of the lengths in the population overall (bottom panel) and when distinguished by sex (top panel).\n\nbase = alt.Chart(population_hawks).properties(height = 200)\n\nhist = base.mark_bar(opacity = 0.5, color = 'red').encode(\n    x = alt.X('length', \n              bin = alt.Bin(maxbins = 40), \n              scale = alt.Scale(domain = (40, 70)),\n              title = 'length (cm)'),\n    y = alt.Y('count()', \n              stack = None,\n              title = 'number of birds')\n)\n\nhist_bysex = hist.encode(color = 'sex').properties(height = 100)\n\nhist_bysex & hist\n\n\n\n\n\n\nThe population mean – average length of both female and male red-tailed hawks – is shown below.\n\n# population mean\npopulation_hawks.mean(numeric_only = True)\n\nlength    54.737717\ndtype: float64\n\n\nFirst try drawing a random sample from the population:\n\n# for reproducibility\nnp.random.seed(40821)\n\n# randomly sample\nsample_hawks = population_hawks.sample(n = 300, replace = False)\n\n\nQuestion 8\nDo you expect that the sample will contain equal numbers of male and female hawks? Think about this for a moment (you don’t have to provide a written answer), and then compute the proportions of individuals in the sample of each sex and store the result as a dataframe named proportion_hawks_sample. The dataframe should have one column named proportion and two rows indexed by sex.\nHint: group by sex, use .count(), and divide by the sample size. Be sure to rename the output column appropriately, as the default behavior produces a column called length.\n\nproportion_hawks_sample = sample_hawks.groupby('sex').count().rename(columns = {'length': 'proportion'})/300 #SOLUTION\n\nproportion_hawks_sample\n\n\n\n\n\n\n\n\nproportion\n\n\nsex\n\n\n\n\n\nfemale\n0.596667\n\n\nmale\n0.403333\n\n\n\n\n\n\n\n\ngrader.check(\"q8\")\n\nThe sample mean is shown below, and is fairly close to the population mean. This should be expected, since you already saw in scenario 1 that random sampling is an unbiased sampling design with respect to the mean.\n\nsample_hawks.mean(numeric_only = True)\n\nlength    54.952103\ndtype: float64"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html#biased-sampling-1",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html#biased-sampling-1",
    "title": "Lab 2: Sampling designs and statistical bias",
    "section": "Biased sampling",
    "text": "Biased sampling\nLet’s now consider a biased sampling design. Usually, length measurements are collected from dead specimens collected opportunistically. Imagine that male mortality is higher, so there are better chances of finding dead males than dead females. Suppose in particular that specimens are five times as likely to be male; to represent this situation, we’ll assign sampling weights of 5/6 to all male hawks and weights of 1/6 to all female hawks.\n\ndef weight_fn(sex, p = 5/6):\n    if sex == 'male':\n        out = p\n    else:\n        out = 1 - p\n    return out\n\nweight_df = pd.DataFrame(\n    {'length': [50.5, 57.5],\n     'weight': [5/6, 1/6],\n     'sex': ['male', 'female']})\n\nwt = alt.Chart(weight_df).mark_bar(opacity = 0.5).encode(\n    x = alt.X('length', scale = alt.Scale(domain = (40, 70))),\n    y = alt.Y('weight', scale = alt.Scale(domain = (0, 1))),\n    color = 'sex'\n).properties(height = 70)\n\nhist_bysex & wt\n\n\n\n\n\n\n\nQuestion 9\nDraw a weighted sample sample_hawks_biased from the population population_hawks using the weights defined by weight_fn, and compute and store the value of the sample mean as sample_hawks_biased_mean.\n\n# BEGIN SOLUTION NO PROMPT\n\n# for reproducibility\nnp.random.seed(40821)\n\n# assign weights\npopulation_hawks['weight'] = population_hawks.sex.aggregate(func = weight_fn)\n\n# randomly sample\nsample_hawks_biased = population_hawks.sample(n = 300, replace = False, weights = 'weight').loc[:, ['length', 'sex']]\n\n# compute mean\nsample_hawks_biased_mean = sample_hawks_biased.length.mean()\n\n# END SOLUTION\n\"\"\" # BEGIN PROMPT\n# for reproducibility\nnp.random.seed(40821)\n\n# assign weights\n\n# randomly sample\n\n# compute mean\nsample_hawks_biased_mean = ...\n\"\"\" # END PROMPT\n\nsample_hawks_biased_mean\n\n51.88710627046723\n\n\n\ngrader.check(\"q9\")\n\n\n\nQuestion 10\nInvestigate the bias of the sample mean by:\n\ndrawing 1000 samples with observations weighted by weight_fn;\nstoring the sample mean from each sample as samp_means_hawks;\ncomputing the average difference between the sample means and the population mean and storing the resulting value as avg_diff_hawks.\n\n\n# BEGIN SOLUTION NO PROMPT\n\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\nnsim = 1000\n\n# storage for the sample means\nsamp_means_hawks = np.zeros(nsim)\n\n# repeatedly sample and store the sample mean\nfor i in range(0, nsim):\n    samp_means_hawks[i] = population_hawks.sample(\n        n = 300, \n        replace = False, \n        weights = 'weight'\n    ).length.mean(numeric_only = True)\n\n# bias\navg_diff_hawks = samp_means_hawks.mean() - population_hawks.length.mean()\n\n# END SOLUTION\n\"\"\" # BEGIN PROMPT\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\n\n# storage for the sample means\nsamp_means_hawks = ...\n\n# repeatedly sample and store the sample mean in the samp_means array\n\n# bias\navg_diff_hawks = ...\n\"\"\" # END PROMPT\n\navg_diff_hawks\n\n-2.5720649894415715\n\n\n\ngrader.check(\"q10\")\n\n\n\n\nQuestion 11\nReflect a moment on your simulation result in question 3c. If instead female mortality is higher and specimens for measurement are collected opportunistically, as described in the previous sampling design, do you expect that the average length in the sample will be an underestimate or an overestimate of the population mean? Explain why in 1-2 sentences, and carry out a simulation to check your intuition.\nType your answer here, replacing this text.\nSOLUTION: If female mortality was higher, then it would be an overestimate.\n\n# BEGIN SOLUTION NO PROMPT\n\nnp.random.seed(40221) # for reproducibility\n\n# invert weights\npopulation_hawks['weight_inv'] = 1 - population_hawks.weight\n\n# number of samples to simulate\nnsim = 1000\n\n# storage for the sample means\nsamp_means_hawks = np.zeros(nsim)\n\n# repeatedly sample and store the sample mean\nfor i in range(0, nsim):\n    samp_means_hawks[i] = population_hawks.sample(\n        n = 300, \n        replace = False, \n        weights = 'weight_inv'\n    ).length.mean(numeric_only = True)\n\n# bias\nestimated_bias = samp_means_hawks.mean() - population_hawks.length.mean()\n\n# END SOLUTION\n\"\"\" # BEGIN PROMPT\nnp.random.seed(40221) # for reproducibility\n\n# invert weights\n\n# storage for the sample means\n\n# repeatedly sample and store the sample mean\n\n# compute bias\nestimated_bias = ...\n\"\"\" # END PROMPT\n\nestimated_bias\n\n1.9796123471243803"
  },
  {
    "objectID": "hw/hw1-brfss/hw1-brfss-soln.html",
    "href": "hw/hw1-brfss/hw1-brfss-soln.html",
    "title": "Background",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"hw1-brfss.ipynb\")\n\n\nimport numpy as np\nimport pandas as pd\nimport altair as alt\n# disable row limit for plotting\nalt.data_transformers.disable_max_rows()\n# uncomment to ensure graphics display with pdf export\n# alt.renderers.enable('mimetype')\n\nThe Behavioral Risk Factor Surveillance System (BRFSS) is a long-term effort administered by the CDC to collect data on behaviors affecting physical and mental health, past and present health conditions, and access to healthcare among U.S. residents. The BRFSS comprises telephone surveys of U.S. residents conducted annually since 1984; in the last decade, over half a million interviews have been conducted each year. This is the largest such data collection effort in the world, and many countries have developed similar programs. The objective of the program is to support monitoring and analysis of factors influencing public health in the United States.\nEach year, a standard survey questionnaire is developed that includes a core component comprising questions about: demographic and household information; health-related perceptions, conditions, and behaviors; substance use; and diet. Trained interviewers in each state call randomly selected telephone (landline and cell) numbers and administer the questionnaire; the phone numbers are chosen so as to obtain a representative sample of all households with telephone numbers. Take a moment to read about the 2019 survey here.\nIn this assignment you’ll import and subsample the BRFSS 2019 data and perform a simple descriptive analysis exploring associations between adverse childhood experiences, health perceptions, tobacco use, and depressive disorders. This is an opportunity to practice:\n\nreview of data documentation\ndata assessment and critical thinking about data collection\ndataframe transformations in pandas\ncommunicating and interpreting grouped summaries\n\n\nData import and assessment\nThe cell below imports select columns from the 2019 dataset as a pandas DataFrame. The file is big, so this may take a few moments. Run the cell and then have a quick look at the first few rows and columns.\n\n# store variable names of interest\nselected_vars = ['_SEX', '_AGEG5YR', \n                 'GENHLTH', 'ACEPRISN', \n                 'ACEDRUGS', 'ACEDRINK', \n                 'ACEDEPRS', 'ADDEPEV3', \n                 '_SMOKER3', '_LLCPWT']\n\n# import full 2019 BRFSS dataset\nbrfss = pd.read_csv('data/brfss2019.zip', compression = 'zip', usecols = selected_vars)\n\n# invert sampling weights\nbrfss['_LLCPWT'] = 1/brfss._LLCPWT\n\n# print first few rows\nbrfss.head()\n\n\n\n\n\n\n\n\nGENHLTH\nADDEPEV3\nACEDEPRS\nACEDRINK\nACEDRUGS\nACEPRISN\n_LLCPWT\n_SEX\n_AGEG5YR\n_SMOKER3\n\n\n\n\n0\n3.0\n2.0\n2.0\n2.0\n2.0\n2.0\n0.007391\n2.0\n13.0\n3.0\n\n\n1\n4.0\n2.0\n2.0\n1.0\n2.0\n2.0\n0.000687\n2.0\n11.0\n4.0\n\n\n2\n3.0\n2.0\n2.0\n2.0\n2.0\n2.0\n0.004639\n2.0\n10.0\n4.0\n\n\n3\n4.0\n2.0\nNaN\nNaN\nNaN\nNaN\n0.003827\n2.0\n13.0\n9.0\n\n\n4\n2.0\n2.0\n2.0\n2.0\n2.0\n2.0\n0.001868\n2.0\n13.0\n3.0\n\n\n\n\n\n\n\n\nQuestion 1: Data dimensions\nCheck the dimensions of the dataset. Store the dimensions as nrows and ncolumns.\n\nnrows, ncolumns = brfss.shape # SOLUTION\n\nprint(nrows, ncolumns)\n\n418268 10\n\n\n\ngrader.check(\"q1\")\n\n\n\n\nQuestion 2: Row and column information\nNow that you’ve imported the data, you should verify that the dimensions conform to the format you expect based on data documentation and ensure you understand what each row and each column represents.\nCheck the number of records (interviews conducted) reported and variables measured for 2019 by reviewing the surveillance summaries by year, and then answer the following questions in a few sentences:\n\nDoes the number of rows match the number of reported records?\nHow many columns were imported, and how many columns are reported in the full dataset?\nWhat does each row in the brfss dataframe represent?\nWhat does each column in the brfss dataframe represent\n\nType your answer here, replacing this text.\nSOLUTION:\nYes, there are exactly as many rows as reported records. The documentation reports 342 variables measured; we only imported 10 of those variables. Each row corresponds to a respondent, and each column corresponds to a respondent attribute or answer to one of the survey questions.\n\n\n\n\nQuestion 3: Sampling design and data collection\nSkim the overview documentation for the 2019 BRFSS data. Focus specifically the ‘Background’ and ‘Data Collection’ sections, read selectively for relevant details, and answer the following questions in a few sentences:\n\nWho conducts the interviews and how long does a typical interview last?\nWho does an interviewer speak to in each household?\nWhat criteria must a person meet to be interviewed?\nWho can’t appear in the survey? Give two examples.\nWhat is the study population (i.e., all individuals who could possibly be sampled)?\nDoes the data contain any identifying information?\n\nType your answer here, replacing this text.\nSOLUTION: State healthcare personell or trained contractors conduct interveiews, and they generally last 17-27 minutes. After speaking with whoever answers the phone, the interviewer determines a randomly selected adult in the household to survey. The respondent must be over 18, live in a private residence or college housing, and have a working phone. Anyone not meeting these criteria cannot participate, such as: anyone under 18; anyone living in residential care facilities or prisons; anyone without a permanent home. The study population is all adult U.S. residents with working phones and living in private or college housing. The data are de-identified and none of the variables allow for easy reconstruction of a respondent’s identity.\n\n\n\n\nQuestion 4: Variable descriptions\nYou’ll work with the small subset variables imported above: sex, age, general health self-assessment, smoking status, depressive disorder, and adverse childhood experiences (ACEs). The names of these variables as they appear in the raw dataset are defined in the cell in which you imported the data as selected_vars. It is often useful, and therefore good practice, to include a brief description of each variable at the outset of any reported analyses, both for your own clarity and for that of any potential readers. Open the 2019 BRFSS codebook in your browser and use text searching to locate each of the variable names of interest. Read the codebook entries and fill in the second column in the table below with a one-sentence description of each variable identified in selected_vars. Rephrase the descriptions in your own words – do not copy the codebook descriptions verbatim.\n\n\n\nVariable name\nDescription\n\n\n\n\nGENHLTH\n\n\n\n_SEX\n\n\n\n_AGEG5YR\n\n\n\nACEPRISN\n\n\n\nACEDRUGS\n\n\n\nACEDRINK\n\n\n\nACEDEPRS\n\n\n\nADDEPEV3\n\n\n\n_SMOKER3\n\n\n\n\nSOLUTION\n\n\n\n\n\n\n\nVariable name\nDescription\n\n\n\n\nGENHLTH\nSelf-rated general health\n\n\n_SEX\nRespondent’s sex\n\n\n_AGEG5YR\nAge bracket in 5-year intervals\n\n\nACEPRISN\nLived with anyone who served prison time or was in prison?\n\n\nACEDRUGS\nLived with anyone abusing substances?\n\n\nACEDRINK\nLived with a problem drinker or alcoholic?\n\n\nACEDEPRS\nLived with anyone depressed, mentally ill, or suicidal?\n\n\nADDEPEV3\nEver diagnosed with a depressive disorder?\n\n\n_SMOKER3\nSmoking status\n\n\n\n\n\n\n\nSubsampling\nTo simplify life a little, we’ll draw a large random sample of the rows and work with that in place of the full dataset. This is known as subsampling.\nThe cell below draws a random subsample of 10k records. Because the subsample is randomly drawn, we should not expect it to vary in any systematic way from the overall dataset, and distinct subsamples should have similar properties – therefore, results downstream should be similar to an analysis of the full dataset, and should also be possible to replicate using distinct subsamples.\n\n# for reproducibility\nnp.random.seed(32221) \n\n# randomly sample 10k records\nsamp = brfss.sample(n = 10000, \n                    replace = False, \n                    weights = '_LLCPWT')\n\nAsides:\n\nNotice that the random number generator seed is set before carrying out this task – this ensures that every time the cell is run, the same subsample is drawn. As a result, the computations in this notebook are reproducible: when I run the notebook on my computer, I get the same results as you get when you run the notebook on your computer.\nNotice also that sampling weights provided with the dataset are used to draw a weighted sample. Some respondents are more likely to be selected than others from the general population of U.S. adults with phone numbers, so the BRFSS calculates derived weights that are inversely proportional to estimates of the probability that the respondent is included in the survey. This is a somewhat sophisticated calculation, however if you’re interested, you can read about how these weights are calculated and why in the overview documentation you used to answer the questions above. We use the sampling weights in drawing the subsample so that we get a representative sample of U.S. adults with phone numbers.\nNotice the missing values. How many entries are missing in each column? The cell below computes the proportion of missing values for each of the selected variables. We’ll return to this issue later on.\n\n\n# proportions of missingness \nsamp.isna().mean()\n\nGENHLTH     0.0000\nADDEPEV3    0.0000\nACEDEPRS    0.8086\nACEDRINK    0.8088\nACEDRUGS    0.8088\nACEPRISN    0.8088\n_LLCPWT     0.0000\n_SEX        0.0000\n_AGEG5YR    0.0000\n_SMOKER3    0.0000\ndtype: float64\n\n\n\n\nTidying\nIn the following series of questions you’ll tidy up the subsample by performing these steps:\n\nselecting columns of interest;\nreplacing coded values of question responses with responses;\ndefining new variables based on existing ones;\nrenaming columns.\n\nThe goal of this is to produce a clean version of the dataset that is well-organized, intuitive to navigate, and ready for analysis.\nThe variable entries are coded numerically to represent certain responses. These should be replaced by more informative entries. We can use the codebook to determine which number means what, and replace the values accordingly.\nThe cell below replaces the numeric values for _AGEG5YR by their meanings, illustrating how to use .replace() with a dictionary to convert the numeric coding to interpretable values. The basic strategy is:\n\nStore the variable coding for VAR as a dictionary var_codes.\nUse .replace({'VAR': var_codes}) to modify values.\n\nIf you need additional examples, check the pandas documentation for .replace().\n\n# dictionary representing variable coding\nage_codes = {\n    1: '18-24', 2: '25-29', 3: '30-34',\n    4: '35-39', 5: '40-44', 6: '45-49',\n    7: '50-54', 8: '55-59', 9: '60-64',\n    10: '65-69', 11: '70-74', 12: '75-79',\n    13: '80+', 14: 'Unsure/refused/missing'\n}\n\n# recode age categories\nsamp_mod1 = samp.replace({'_AGEG5YR': age_codes})\n\n# check result\nsamp_mod1.head()\n\n\n\n\n\n\n\n\nGENHLTH\nADDEPEV3\nACEDEPRS\nACEDRINK\nACEDRUGS\nACEPRISN\n_LLCPWT\n_SEX\n_AGEG5YR\n_SMOKER3\n\n\n\n\n237125\n5.0\n2.0\nNaN\nNaN\nNaN\nNaN\n0.057004\n2.0\n25-29\n3.0\n\n\n329116\n5.0\n2.0\nNaN\nNaN\nNaN\nNaN\n0.108336\n2.0\n80+\n3.0\n\n\n178937\n3.0\n2.0\nNaN\nNaN\nNaN\nNaN\n0.000998\n1.0\n18-24\n4.0\n\n\n410081\n4.0\n1.0\nNaN\nNaN\nNaN\nNaN\n0.021973\n2.0\n45-49\n2.0\n\n\n184555\n2.0\n2.0\n2.0\n2.0\n2.0\n2.0\n0.027175\n2.0\n80+\n3.0\n\n\n\n\n\n\n\n\nQuestion 5: Recoding variables\nFollowing the example immediately above and referring to the 2019 BRFSS codebook, replace the numeric codings with response categories for each of the following variables:\n\n_SEX\nGENHLTH\n_SMOKER3\n\nNotice that above, the first modification (slicing) was stored as samp_mod1, and was a function of samp. You’ll follow this pattern, creating samp_mod2, samp_mod3, and so on so that each step (modification) of your data manipulations is stored separately, for easy troubleshooting.\n\nRecode _SEX: define a new dataframe samp_mod2 that is the same as samp_mod1 but with the _SEX variable recoded as M and F.\nRecode GENHLTH: define a new dataframe samp_mod3 that is the same as samp_mod2 but with the GENHLTH variable recoded as Excellent, Very good, Good, Fair, Poor, Unsure, and Refused.\nRecode _SMOKER3: define a new dataframe samp_mod4 that is the same as samp_mod3 but with _SMOKER3 recoded as Daily, Some days, Former, Never, and Unsure/refused/missing.\nPrint the first few rows of samp_mod4.\n\n\n# define dictionary for sex\nsex_codes = {1: 'M', 2: 'F'} # SOLUTION\n\n# recode sex\nsamp_mod2 = samp_mod1.replace({'_SEX': sex_codes}) # SOLUTION\n\n# define dictionary for health\nhealth_codes = { 1: 'Excellent', 2: 'Very good', 3: 'Good', 4: 'Fair', 5: 'Poor', 7: 'Unsure', 9: 'Refused'} # SOLUTION\n\n# recode health\nsamp_mod3 = samp_mod2.replace({'GENHLTH': health_codes}) # SOLUTION\n\n# define dictionary for smoking\nsmoke_codes = { 1: 'Daily', 2: 'Some days', 3: 'Former', 4: 'Never', 9: 'Unsure/refused/missing'} # SOLUTION\n\n# recode smoking\nsamp_mod4 = samp_mod3.replace({'_SMOKER3': smoke_codes}) # SOLUTION\n\n# print a few rows\nsamp_mod4.head() # SOLUTION\n\n\n\n\n\n\n\n\nGENHLTH\nADDEPEV3\nACEDEPRS\nACEDRINK\nACEDRUGS\nACEPRISN\n_LLCPWT\n_SEX\n_AGEG5YR\n_SMOKER3\n\n\n\n\n237125\nPoor\n2.0\nNaN\nNaN\nNaN\nNaN\n0.057004\nF\n25-29\nFormer\n\n\n329116\nPoor\n2.0\nNaN\nNaN\nNaN\nNaN\n0.108336\nF\n80+\nFormer\n\n\n178937\nGood\n2.0\nNaN\nNaN\nNaN\nNaN\n0.000998\nM\n18-24\nNever\n\n\n410081\nFair\n1.0\nNaN\nNaN\nNaN\nNaN\n0.021973\nF\n45-49\nSome days\n\n\n184555\nVery good\n2.0\n2.0\n2.0\n2.0\n2.0\n0.027175\nF\n80+\nFormer\n\n\n\n\n\n\n\n\ngrader.check(\"q5\")\n\n\n\nQuestion 6: Value replacement\nNow all the variables except the adverse childhood experience and depressive disorder question responses are represented interpretably. In the codebook that the answer key is identical for these remaining variables.\nThe numeric codings can be replaced all at once by applying .replace() to the dataframe with an argument of the form\n\ndf.replace({'var1': varcodes1, 'var2': varcodes1, ..., 'varp': varcodesp})\n\nDefine a new dataframe samp_mod5 that is the same as samp_mod4 but with the remaining variables recoded according to the answer key Yes, No, Unsure, Refused. Print the first few rows of the result using .head().\n\n# define dictionary\nanswer_codes = {1: 'Yes', 2: 'No', 7: 'Unsure', 9: 'Refused'} #SOLUTION\n\n# recode\nsamp_mod5 =  samp_mod4.replace({'ACEPRISN': answer_codes, 'ACEDRUGS': answer_codes, 'ACEDRINK': answer_codes, 'ACEDEPRS': answer_codes, 'ADDEPEV3': answer_codes}) #SOLUTION\n\n# check using head()\nsamp_mod5.head() #SOLUTION\n\n\n\n\n\n\n\n\nGENHLTH\nADDEPEV3\nACEDEPRS\nACEDRINK\nACEDRUGS\nACEPRISN\n_LLCPWT\n_SEX\n_AGEG5YR\n_SMOKER3\n\n\n\n\n237125\nPoor\nNo\nNaN\nNaN\nNaN\nNaN\n0.057004\nF\n25-29\nFormer\n\n\n329116\nPoor\nNo\nNaN\nNaN\nNaN\nNaN\n0.108336\nF\n80+\nFormer\n\n\n178937\nGood\nNo\nNaN\nNaN\nNaN\nNaN\n0.000998\nM\n18-24\nNever\n\n\n410081\nFair\nYes\nNaN\nNaN\nNaN\nNaN\n0.021973\nF\n45-49\nSome days\n\n\n184555\nVery good\nNo\nNo\nNo\nNo\nNo\n0.027175\nF\n80+\nFormer\n\n\n\n\n\n\n\n\ngrader.check(\"q6\")\n\nFinally, all the variables in the dataset are categorical. Notice that the current data types do not reflect this.\n\nsamp_mod5.dtypes\n\nGENHLTH      object\nADDEPEV3     object\nACEDEPRS     object\nACEDRINK     object\nACEDRUGS     object\nACEPRISN     object\n_LLCPWT     float64\n_SEX         object\n_AGEG5YR     object\n_SMOKER3     object\ndtype: object\n\n\nLet’s coerce the variables to category data types using .astype().\n\n# coerce to categorical\nsamp_mod6 = samp_mod5.astype('category')\n\n# check new data types\nsamp_mod6.dtypes\n\nGENHLTH     category\nADDEPEV3    category\nACEDEPRS    category\nACEDRINK    category\nACEDRUGS    category\nACEPRISN    category\n_LLCPWT     category\n_SEX        category\n_AGEG5YR    category\n_SMOKER3    category\ndtype: object\n\n\n\n\nQuestion 7: Define ACE indicator variable\nDownstream analysis of ACEs will be facilitated by having an indicator variable that is a 1 if the respondent answered ‘Yes’ to any ACE question, and a 0 otherwise – that way, you can easily count the number of respondents reporting ACEs by summing up the indicator or compute the proportion by taking an average.\nTo this end, define a new logical variable:\n\nadverse_conditions: did the respondent answer yes to any of the adverse childhood condition questions?\n\nYou can accomplish this task in several steps:\n\nObtain a logical array indicating the positions of the ACE variables (hint: use .columns to obtain the column index and operate on the result with .str.startswith(...).). Store this as ace_positions.\nUse the logical array ace_positions to select the ACE columns via .loc[]. Store this as ace_data.\nObtain a dataframe that indicates whether each entry is a ‘Yes’ (hint: use the boolean operator ==, which is a vectorized operation). Store this as ace_yes.\nCompute the row sums using .sum(). Store this as ace_numyes.\nDefine the new variable as ace_numyes &gt; 0.\n\nStore the result as samp_mod7, and print the first few rows using .head().\n\n# BEGIN SOLUTION NO PROMPT\n# copy samp_mod6\nsamp_mod7 = samp_mod6.copy()\n\n# ace column positions\nace_positions = samp_mod7.columns.str.startswith('ACE')\n\n# ace data\nace_data = samp_mod7.loc[:, ace_positions]\n\n# ace yes indicators\nace_yes = (ace_data == 'Yes')\n\n# number of yesses\nace_numyes = ace_yes.sum(axis = 1)\n\n# assign new variable\nsamp_mod7['adverse_conditions'] = (ace_numyes &gt; 0)\n\n# check result\nsamp_mod7.head()\n# END SOLUTION\n\n\"\"\" # BEGIN PROMPT\n# copy samp_mod6\nsamp_mod7 = samp_mod6.copy()\n\n# ace column positions\nace_positions = ...\n\n# ace data\nace_data = ...\n\n# ace yes indicators\nace_yes = ...\n\n# number of yesses\nace_numyes = ...\n\n# assign new variable\nsamp_mod7['adverse_conditions'] = ...\n\n# check result using .head()\n...\n\"\"\"; # END PROMPT\n\n\ngrader.check(\"q7\")\n\n\n\nQuestion 8: Define missingness indicator variable\nAs you saw earlier, there are some missing values for the ACE questions. These arise whenever a respondent is not asked these questions. In fact, answers are missing for nearly 80% of the respondents in our subsample. We should keep track of this information. Define a missing indicator:\n\nadverse_missing: is a response missing for at least one of the ACE questions?\n\n\n# BEGIN SOLUTION NO PROMPT\n# copy modification 7\nsamp_mod8 = samp_mod7.copy()\n\n# define missing indicator using loc\nsamp_mod8.loc[:, 'adverse_missing'] = samp_mod8.loc[:, samp_mod8.columns.str.startswith('ACE').tolist()].isna().sum(axis = 1) &gt; 0\n\n# check\nsamp_mod8.head()\n\n# END SOLUTION\n\"\"\" # BEGIN PROMPT\n# copy modification 7\nsamp_mod8 = samp_mod7.copy()\n\n# define missing indicator using loc\n...\n\n# check using head()\n\"\"\"; # END PROMPT\n\n\ngrader.check(\"q8\")\n\n\n\nQuestion 9: Filter respondents who did not answer ACE questions\nSince values are missing for the ACE question if a respondent was not asked, we can remove these observations and do any analysis conditional on respondents having been asked the ACE questions. Use your indicator variable adverse_missing to filter out respondents who were not asked the ACE questions.\nNote that this dramatically limits the scope of inference for subsequent analyses to only those locations where the ACE module was included in the survey.\n\nsamp_mod9 = samp_mod8[~samp_mod8.adverse_missing] #SOLUTION\n\n\ngrader.check(\"q9\")\n\n\n\nQuestion 10: Define depression indicator variable\nIt will prove similarly helpful to define an indicator for reported depression:\n\ndepression: did the respondent report having been diagnosed with a depressive disorder?\n\nFollow the same strategy as above for the ACE variables, and store the result as samp_mod10. See if you can perform the calculation of the new variable in a single line of code. Print the first few rows using .head().\n\n# BEGIN SOLUTION NO PROMPT\n# copy samp_mod9\nsamp_mod10 = samp_mod9.copy()\n\n# define missing indicator using loc\nsamp_mod10['depression'] = ((samp_mod10.loc[:, 'ADDEPEV3'] == 'Yes') &gt; 0)\n\n# check\nsamp_mod10.head()\n\n# END SOLUTION\n\"\"\" # BEGIN PROMPT\n# copy samp_mod9\nsamp_mod10 = samp_mod9.copy()\n\n# define new variable using loc\n...\n\n# check using .head()\n...\n\"\"\"; # END PROMPT\n\n\ngrader.check(\"q10\")\n\n\n\nQuestion 11: Final dataset\nFor the final dataset, drop the respondent answers to individual questions, the missingness indicator, and select just the derived indicator variables along with general health, sex, age, and smoking status. Check the pandas documentation for .rename() and follow the examples to rename the latter variables:\n\ngeneral_health\nsex\nage\nsmoking\n\nSee if you can perform both operations (slicing and renaming) in a single chain. Store the result as data.\n\nsamp_mod10.columns\n\nIndex(['GENHLTH', 'ADDEPEV3', 'ACEDEPRS', 'ACEDRINK', 'ACEDRUGS', 'ACEPRISN',\n       '_LLCPWT', '_SEX', '_AGEG5YR', '_SMOKER3', 'adverse_conditions',\n       'adverse_missing', 'depression'],\n      dtype='object')\n\n\n\n# BEGIN SOLUTION NO PROMPT\n# slice and rename\ndata = samp_mod10.iloc[:, [0, 7, 8, 9, 10, 12]].rename( #dropping some variables is the same as only selecting the remaining variables\n    columns = {'GENHLTH': 'general_health',\n               '_SEX': 'sex',\n               '_AGEG5YR': 'age',\n               '_SMOKER3': 'smoking'}\n)\n\n# preview\ndata.head()\n\n# END SOLUTION\n\"\"\" # BEGIN PROMPT\n# slice and rename\ndata = ...\n\n# check using .head()\n\n\"\"\"; # END PROMPT\n\n\ngrader.check(\"q11\")\n\n\n\n\nDescriptive analysis\nNow that you have a clean dataset, you’ll use grouping and aggregation to compute several summary statistics that will help you explore whether there is an apparent association between experiencing adverse childhood conditions and self-reported health, smoking status, and depressive disorders in areas where the ACE module was administered.\nThe basic strategy will be to calculate the proportions of respondents who answered yes to one of the adverse experience questions when respondents are grouped by the other variables.\n\nQuestion 12: Proportion of respondents reporting ACEs\nCalculate the overall proportion of respondents in the subsample that reported experiencing at least one adverse condition (given that they answered the ACE questions). Use .mean(); store the result as mean_ace and print.\n\n# proportion of respondents reporting at least one adverse condition\nmean_ace = data.adverse_conditions.mean() #SOLUTION\n\n# print\nmean_ace\n\n0.3070083682008368\n\n\n\ngrader.check(\"q12\")\n\nDoes the proportion of respondents who reported experiencing adverse childhood conditions vary by general health?\nThe cell below computes the porportion separately by general health self-rating. Notice that the depression variable is dropped so that the result doesn’t also report the proportion of respondents reporting having been diagnosed with a depressive disorder. Notice also that the proportion of missing values for respondents indicating each general health rating is shown.\n\n# proportions grouped by general health\ndata.drop(\n    columns = 'depression'\n).groupby(\n    'general_health'\n).mean(numeric_only = True)\n\n\n\n\n\n\n\n\nadverse_conditions\n\n\ngeneral_health\n\n\n\n\n\nExcellent\n0.300000\n\n\nFair\n0.355491\n\n\nGood\n0.299174\n\n\nPoor\n0.441667\n\n\nRefused\n0.000000\n\n\nUnsure\n0.000000\n\n\nVery good\n0.264957\n\n\n\n\n\n\n\nNotice that the row index lists the general health rating out of order. This can be fixed using a .loc[] call and the dictionary that was defined for the variable coding.\n\n# same as above, rearranging index\nace_health = data.drop(\n    columns = 'depression'\n).groupby(\n    'general_health'\n).mean(\n    numeric_only = True\n).loc[list(health_codes.values()), :]\n\n# print\nace_health\n\n\n\n\n\n\n\n\nadverse_conditions\n\n\ngeneral_health\n\n\n\n\n\nExcellent\n0.300000\n\n\nVery good\n0.264957\n\n\nGood\n0.299174\n\n\nFair\n0.355491\n\n\nPoor\n0.441667\n\n\nUnsure\n0.000000\n\n\nRefused\n0.000000\n\n\n\n\n\n\n\n\n\nQuestion 13: Association between smoking status and ACEs\nDoes the proportion of respondents who reported experiencing adverse childhood conditions vary by smoking status?\nFollowing the example above for computing the proportion of respondents reporting ACEs by general health rating, calculate the proportion of respondents reporting ACEs by smoking status (be sure to arrange the rows in appropriate order of smoking status) and store as ace_smoking.\n\n# proportions grouped by smoking status\nace_smoking = data.drop(\n    columns = 'depression'\n).groupby(\n    'smoking'\n).mean(\n    numeric_only = True\n).loc[list(smoke_codes.values()), :] #SOLUTION\n\n# print\nace_smoking\n\n\n\n\n\n\n\n\nadverse_conditions\n\n\nsmoking\n\n\n\n\n\nDaily\n0.453125\n\n\nSome days\n0.527778\n\n\nFormer\n0.334459\n\n\nNever\n0.251434\n\n\nUnsure/refused/missing\n0.100000\n\n\n\n\n\n\n\n\ngrader.check(\"q13\")\n\n\n\nQuestion 14: Association between depression and ACEs\nDoes the proportion of respondents who reported experiencing adverse childhood conditions vary by smoking status?\nCalculate the proportion of respondents reporting ACEs by whether respondents had been diagnosed with a depressive disorder and store as ace_depr.\n\n# proportions grouped by having experienced depression\nace_depr = data.groupby(\n    'depression'\n).mean(\n    numeric_only = True\n) #SOLUTION\n\n# print\nace_depr\n\n\n\n\n\n\n\n\nadverse_conditions\n\n\ndepression\n\n\n\n\n\nFalse\n0.250975\n\n\nTrue\n0.537433\n\n\n\n\n\n\n\n\ngrader.check(\"q14\")\n\n\n\nQuestion 15: Exploring subgroupings\nDoes the apparent association between general health and ACEs persist after accounting for sex?\nRepeat the calculation of the proportion of respondents reporting ACEs by general health rating, but also group by sex. Store the result as ace_health_sex.\n\n# group by general health and sex\nace_health_sex = data.drop(\n    columns = 'depression'\n).groupby(\n    ['general_health', 'sex']\n).mean(numeric_only = True) #SOLUTION\n\n\ngrader.check(\"q15\")\n\nThe cell below rearranges the table a little for better readability.\n\n# pivot table for better display\nace_health_sex.reset_index().pivot(columns = 'sex', index = 'general_health', values = 'adverse_conditions').loc[list(health_codes.values()), :]\n\n\n\n\n\n\n\nsex\nF\nM\n\n\ngeneral_health\n\n\n\n\n\n\nExcellent\n0.328671\n0.261682\n\n\nVery good\n0.282123\n0.237885\n\n\nGood\n0.308108\n0.285106\n\n\nFair\n0.367150\n0.338129\n\n\nPoor\n0.549296\n0.285714\n\n\nUnsure\nNaN\n0.000000\n\n\nRefused\n0.000000\nNaN\n\n\n\n\n\n\n\nEven after rearrangement, the table in the last question is a little tricky to read (few people like visually scanning tables). This information would be better displayed in a plot. The example below generates a bar chart showing the summaries you calculated in Q2(d), with the proportion on the y axis, the health rating on the x axis, and separate bars for the two sexes.\n\n# coerce indices to columns for plotting\nplot_df = ace_health_sex.reset_index()\n\n# specify order of general health categories\ngenhealth_order = list(health_codes.values())\nplot_df.general_health.cat.set_categories(genhealth_order, inplace=True)\nplot_df.sort_values([\"general_health\"], inplace=True)\n\n# plot\nalt.Chart(plot_df).mark_bar().encode(\n    x = alt.X('general_health', \n              sort = ['general_health'],\n              title = 'Self-rated general health'),\n    y = alt.Y('adverse_conditions',\n              title = 'Prop. of respondents reporting ACEs'),\n    color = 'sex',\n    column = 'sex'\n).properties(\n    width = 200, \n    height = 200\n)\n\nC:\\Users\\trdea\\AppData\\Local\\Temp\\ipykernel_10156\\2150558614.py:6: FutureWarning: The `inplace` parameter in pandas.Categorical.set_categories is deprecated and will be removed in a future version. Removing unused categories will always return a new Categorical object.\n  plot_df.general_health.cat.set_categories(genhealth_order, inplace=True)\n\n\n\n\n\n\n\n\n\n\nQuestion 16: Visualization\nUse the example above to plot the proportion of respondents reporting ACEs against smoking status for men and women.\nHint: you only need to modify the example by substituting smoking status for general health.\n\n# BEGIN SOLUTION NO PROMPT\n# proportions grouped by smoking status\nace_smoking_sex = data.drop(\n    columns = 'depression'\n).groupby(\n    ['smoking', 'sex']\n).mean(numeric_only = True).loc[list(smoke_codes.values()), :]\n\n# coerce indices to columns for plotting\nplot_df = ace_smoking_sex.reset_index()\n\n# specify order of general health categories\nsmoke_order = pd.CategoricalDtype(list(smoke_codes.values()), ordered = True)\nplot_df['smoking'] = plot_df.smoking.astype(smoke_order)\n\n# plot\nalt.Chart(plot_df).mark_bar().encode(\n    x = alt.X('smoking', \n              sort = list(health_codes.values()),\n              title = 'Smoking status'),\n    y = alt.Y('adverse_conditions',\n              title = 'Prop. of respondents reporting ACEs'),\n    color = 'sex',\n    column = 'sex'\n).properties(\n    width = 200, \n    height = 200\n)\n# END SOLUTION\n\n\n\n\n\n\n\n# BEGIN PROMPT\n# dataframe of proportions grouped by smoking status\nace_smoking_sex = ...\n\n# coerce indices to columns for plotting\n...\n\n# specify order of general health categories\n...\n\n# plot\n...\n# END PROMPT\n\nEllipsis\n\n\n\n\n\n\nCommunicating results\nHere you’ll be asked to reflect briefly on your findings.\n\n\nQuestion 17: Summary\nIs there an observed association between reporting ACEs and general health, smoking status, and depression among survey respondents who answered the ACE questions?\nWrite a two to three sentence answer to the above question summarizing your findings. State an answer to the question in your first sentence, and then in your second/third sentences describe exactly what you observed in the foregoing descriptive analysis of the BRFSS data. Be precise, but also concise. There is no need to describe any of the data manipulations, survey design, or the like.\nType your answer here, replacing this text.\nSOLUTION\nYes, there are observed associations between reported adverse childhood experiences and general health, smoking status, and depression. The proportion of respondents reporting ACEs generally increases with smoking frequency for both men and women; there are higher observed rates of ACE reports among respondents in poorer health for both men and women; and there are higher observed rates of ACE reports among respondents with a diagnosed depressive disorder.\n\n\n\n\nQuestion 18: Scope of inference\nRecall from the overview documentation all the care that the BRFSS dedicates to collecting a representative sample of the U.S. adult population with phone numbers. Do you think that your findings provide evidence of an association among the general public (not just the individuals survey)? Why or why not? Answer in two sentences.\nType your answer here, replacing this text.\nSOLUTION\nThe sample is a probability sample of the study population, so results are in principle generalizable; however, many ACE responses were missing because certain states did not ask those questions. As a result, the observed proportions are likely underestimates of the rates among the general public (U.S. adults with phone numbers in private or college housing) and may misrepresent the overall pattern of association. More narrowly, the findings do provide evidence of associations between adverse childhood experiences and health, depression, and smoking among a subset of states.\n\n\n\n\nQuestion 19: Bias\nWhat is a potential source of bias in the survey results, and how might this affect the proportions you’ve calculated?\nAnswer in one or two sentences.\nType your answer here, replacing this text.\nSOLUTION\nAdverse childhood experience is a sensitive matter; respondents may not be comfortable responding truthfully to some of these questions. This would likely produce negative bias – the sample proportions may be underestimates if this is common.\n\n\n\nComment\nNotice that the language ‘association’ is non-causual: we don’t say that ACEs cause (or don’t cause) poorer health outcomes. This is intentional, because the BRFSS data are what are known as ‘observational’ data, i.e. not originating from a controlled experiment. There could be unobserved factors that explain the association.\nTo take a simple example, dog owners live longer, but the reason is simply that dog owners walk more – so it’s the exercise, not the dogs, that cause an increase in longevity. An observational study that doesn’t measure exercise would show a positive association between dog ownership and lifespan, but it’s a non-causal relationship.\n(As an interesting/amusing aside, there is a well known study that established an association between birdkeeping and lung cancer; obviously this is non-causal, yet the study authors recommended that individuals at high risk for cancer avoid ‘avian exposure’, as they were unsure of the mechanism.)\nSo there could easily be unobserved factors that account for the observed association in the BRFSS data. We guard against over-interpreting the results by using causally-neutral language.\n\n\n\nSubmission\n\nSave the notebook.\nRestart the kernel and run all cells. (CAUTION: if your notebook is not saved, you will lose your work.)\nCarefully look through your notebook and verify that all computations execute correctly and all graphics are displayed clearly. You should see no errors; if there are any errors, make sure to correct them before you submit the notebook.\nDownload the notebook as an .ipynb file. This is your backup copy.\nExport the notebook as PDF and upload to Gradescope.\n\n\nTo double-check your work, the cell below will rerun all of the autograder tests.\n\ngrader.check_all()"
  },
  {
    "objectID": "hw/hw3-diatom/hw3-diatom.html",
    "href": "hw/hw3-diatom/hw3-diatom.html",
    "title": "Background: diatoms and paleoclimatology",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"hw3-diatom.ipynb\")\n\n\nimport numpy as np\nimport pandas as pd\nimport altair as alt\nfrom sklearn.decomposition import PCA\nimport statsmodels.api as sm\n# disable row limit for plotting\nalt.data_transformers.disable_max_rows()\n# uncomment to ensure graphics display with pdf export\n# alt.renderers.enable('mimetype')\n\nDiatoms are a type of phytoplankton – they are photosynthetic algae that function as primary producers in aquatic ecosystems. Diatoms are at the bottom of the food web: they are consumed by filter feeders, like clams, mussels, and many fish, which are in turn consumed by larger organisms like scavengers and predators and, well, us. As a result, changes in the composition of diatom species in marine ecosystems have ripple effects that can dramatically alter overall community structure in any environment of which marine life forms a part.\nDiatoms have glass bodies. As a group of organisms, they display a great diversity of body shapes, and many are quite elaborate. The image below, taken from a Scientific American article, shows a small sample of their shapes and structures.\n\n\n\nBecause they are made of glass, diatoms preserve extraordinarily well over time. When they die, their bodies sink and form part of the sediment. Due to their abundance, there is a sort of steady rain of diatoms forming part of the sedimentation process, which produces sediment layers that are dense with diatoms.\nSedimentation is a long-term process spanning great stretches of time, and the deeper one looks in sediment, the older the material. Since diatoms are present in high density throughout sedimentation layers, and they preserve so well, it is possible to study their presence over longer time spans – potentially hundreds of thousands of years.\nA branch of paleoclimatology is dedicated to studying changes in biological productivity on geologic time scales, and much research in this area has involved studying the relative abundances of diatoms. In this assignment, you’ll do just that on a small scale and work with data from sediment cores taken in the gulf of California at the location indicated on the map:\n\n\n\nThe data is publicly available: &gt; Barron, J.A., et al. 2005. High Resolution Guaymas Basin Geochemical, Diatom, and Silicoflagellate Data. IGBP PAGES/World Data Center for Paleoclimatology Data Contribution Series # 2005-022. NOAA/NGDC Paleoclimatology Program, Boulder CO, USA.\nIn this assignment, you’ll use the exploratory techniques we’ve been discussing in class to analyze the relative abundances of diatom taxa over a time span of 15,000 years. This will involve practicing the following:\n\ndata import and preprocessing\ngraphical techniques for visualizing distributions\nmultivariate analysis with PCA\n\n\nDiatom data\nThe data are diatom counts sampled from evenly-spaced depths in a sediment core from the gulf of California. In sediment cores, depth correlates with time before the present – deeper layers are older – and depths are typically chosen to obtain a desired temporal resolution. The counts were recorded by sampling material from sediment cores at each depth, and examining the sampled material for phytoplankton cells. For each sample, phytoplankton were identified at the taxon level and counts of diatom taxa were recorded along with the total number of phytoplankton cells identified. Thus:\n\nThe observational units are sediment samples.\n\nThe variables are depth (age), diatom abundance counts, and the total number of identified phytoplankton. Age is inferred from radiocarbon.\nOne observation is made at each depth from 0cm (surface) to 13.71 cm.\n\nThe table below provides variable descriptions and units for each column in the dataframe.\n\n\n\n\n\n\n\n\nVariable\nDescription\nUnits\n\n\n\n\nDepth\nDepth interval location of sampled material in sediment core\nCentimeters (cm)\n\n\nAge\nRadiocarbon age\nThousands of years before present (KyrBP)\n\n\nA_curv\nAbundance of Actinocyclus curvatulus\nCount (n)\n\n\nA_octon\nAbundance of Actinocyclus octonarius\nCount (n)\n\n\nActinSpp\nAbundance of Actinoptychus species\nCount (n)\n\n\nA_nodul\nAbundance of Azpeitia nodulifer\nCount (n)\n\n\nCocsinSpp\nAbundance of Coscinodiscus species\nCount (n)\n\n\nCyclotSpp\nAbundance of Cyclotella species\nCount (n)\n\n\nRop_tess\nAbundance of Roperia tesselata\nCount (n)\n\n\nStephanSpp\nAbundance of Stephanopyxis species\nCount (n)\n\n\nNum.counted\nNumber of diatoms counted in sample\nCount (n)\n\n\n\nThe cell below imports the data.\n\n# import diatom data\ndiatoms_raw = pd.read_csv('data/barron-diatoms.csv')\ndiatoms_raw.head(5)\n\nThe data are already in tidy format, because each row is an observation (a set of measurements on one sample of sediment) and each column is a variable (one of age, depth, or counts). However, examine rows 3 and 4, and note:\n\nNaNs are present\nThe number of individuals counted in each sample varies by a lot from sample to sample.\n\nLet’s address those before conducting initial explorations.\nThe NaNs are an artefact of the data recording – if no diatoms in a particular taxa are observed, a - is entered in the table (you can verify this by checking the .csv file). In these cases the value isn’t missing, but rather zero. These entries are parsed by pandas as NaNs, but they correspond to a value of 0 (no diatoms observed).\n\nQuestion 1: Filling NaNs\nUse .fill_na() to replace all NaNs by zeros, and store the result as diatoms_mod1. Store rows 4 and 5 (index, not integer location) of the resulting dataframe as diatoms_mod1_examplerows and display these rows.\n\ndiatoms_mod1 = ...\n\n# print rows 4 and 5\ndiatoms_mod1_examplerows = ...\nprint(diatoms_mod1_examplerows)\n\n\ngrader.check(\"q1\")\n\nSince the total number of phytoplankton counted in each sample varies, the raw counts are not directly comparable – e.g., a count of 18 is actually a different abundance in a sample with 200 individuals counted than in a sample with 300 individuals counted.\nFor exploratory analysis, you’ll want the values to be comparable across rows. This can be achieved by a simple transformation so that the values are relative abundances: proportions of phytoplankton observed from each taxon.\n\n\nQuestion 2: Counts to proportions\nConvert the counts to proportions by dividing by the relevant entry in the Num.counted column. There are a few ways to do this, but here’s one approach:\n\nSet Depth and Age to row indices using .set_index(...) and store the result as diatoms_mod2.\nStore the Num.counted column from diatoms_mod2 as sampsize.\nUse .div(...) to divide entrywise every column in diatoms_mod2 by sampsize and store the result as diatoms_mod3.\nDrop the Num.counted column from diatoms_mod3 and reset the index; store the result as diatoms.\n\nCarry out these steps and print the first four rows of diatoms.\n(Hint: careful with the axis = ... argument in .div(...); you may want to look at the documentation and check one or two values manually to verify the calculation works as intended.)\n\n# set depth, age to indices\ndiatoms_mod2 = ...\n\n# store sample sizes\nsampsize = ...\n\n# divide\ndiatoms_mod3 = ...\n\n# drop num.counted and reset index\ndiatoms = ...\n\n# print\n...\n\n\ngrader.check(\"q2\")\n\nTake a moment to think about what the data represent. They are relative abundances over time; essentially, snapshots of the community composition of diatoms over time, and thus information about how ecological community composition changes.\nBefore diving in, it will be helpful to resolve two matters:\n\nHow far back in time do the data go?\nWhat is the time resolution of the data?\n\n\n\nQuestion 3: Time span\nWhat is the geological time span covered by the data? Compute the minimum and maximum age using .aggregate(...) and store the result as min_max_age. (You will need to use .aggregate() to pass the automated tests.)\nNote: This may be a new function for you, but it’s simple: it takes as an argument a list of functions that will be applied to the dataframe (columnwise by default). So for example, to get the mean and variance of each column in df, one would use df.aggregate(['mean', 'var']). See the documentation for further examples.\nRemember: age is reported as thousands of years before present, so Age = 2 means 2000 years ago.\n\nmin_max_age = ...\nmin_max_age\n\n\ngrader.check(\"q3\")\n\n\n\n\nQuestion 4: Time resolution\n(i) How are the observations spaced in time?\nFollow these steps:\n\nExtract the Age column from diatoms, sort the values in ascending order, compute the differences between consecutive rows, and store the result as diffs.\n\nHint: use .sort_values() and .diff().\nNotice: that the first difference is NaN, because there is no previous value to compare the first row with. Drop this entry when you store diffs.\n\nMake a simple count histogram (no need to manually bin or convert to density scale) with bins of width 0.02 (20 years). Store this as fig1 and display the figure.\n\nLabel the x axis ‘Time step between consecutive samples’\nPut the y axis on a square root scale so that bins with one observation are discernible\n\n\n(ii) What is the typical time step in years?\nWrite your answer based on the count histogram.\nType your answer here, replacing this text.\n\n# store differences\ndiffs = ...\n\n# construct histogram\nfig1 = alt.Chart(diffs).mark_bar().encode(\n    x = ...\n    y = ...\n)\n\n# display\nfig1\n\n\ngrader.check(\"q4\")\n\n\n\n\n\nUnivariate explorations\nTo begin, you’ll examine the variation in relative abundance over time for the eight individual taxa, one variable at a time.\nHere are some initial questions in this spirit that will help you to hone in and develop more focuesed exploratory questions: * Which taxa are most and least abundant on average over time? * Which taxa vary the most over time?\nThese can be answered by computing simple summary statistics for each column in the diatom data.\n\nQuestion 5: Summary statistics\nUse .aggregate(...) to find the mean and standard deviation of relative abundances for each taxon. Follow these steps:\n\nDrop the depth and age variables before performing the aggregation.\nUse .transpose() to ensure that the table is rendered in long form (8 rows by 2 columns rather than 2 columns by 8 rows).\nStore the resulting dataframe as diatom_summary display.\n\n\ndiatom_summary = ...\n\n# print the dataframe\ndiatom_summary\n\n\ngrader.check(\"q5\")\n\nIt will be easier to determine which taxa are most/least abundant and most variable by displaying this information visually.\n\n\n\nQuestion 6: Visualizing summary statistics\n(i) Create a plot of the average relative abundances and their variation over time.\n\nReset the index of diatom_summary so that the taxon names are stored as a column and not an index. Store the result as plot_df.\nCreate an Altair chart based on plot_df with no marks – just alt.Chart(...).encode(...) – and pass the columnn of taxon names to the Y encoding channel with the title ‘Taxon’ and sorted in descending order of mean relative abundance. Store the result as base.\n\nHint: alt.Y(..., sort = {'field': 'column', 'order': 'descending'}) will sort the Y channel by ‘column’ in descending order.\n\nModify base to create a point plot of the average relative abundances for each taxon; store the result as means.\n\nAverage relative abundance (the mean you calculated in Q1 (a)) should appear on the x axis, and taxon on the y axis.\nSince the Y encoding was already specified in base, you do not need to add a Y encoding at this stage.\nGive the x axis the title ‘Average relative abundance’.\n\nModify base to create a plot with bars spanning two standard deviations in either direction from the mean. Store the result as bars.\n\nFirst use base.transform_calculate(...) to compute lwr and upr for the positions of the bar endpoints:\n\n\\(\\texttt{lwr} = \\texttt{mean} - 2\\times\\texttt{std}\\)\n\\(\\texttt{upr} = \\texttt{mean} + 2\\times\\texttt{std}\\).\n\nThen append .mark_errorbar().encode(...) to the chain:\n\npass lwr:Q to the X encoding channel with the title ‘Average relative abundance’ (to match the point plot)\npass upr:Q to the X2 encoding channel (no specific title needed).\n\n\nLayer the plots: means + bars. Store the result as fig2 and display the figure.\n\nIt may help to have a look at this example.\n(ii) Based on the figure, answer the following questions.\n\nWhich taxon is most abundant on average over time?\nWhich taxon is most rare on average over time?\nWhich taxon varies most in relative abundance over time?\n\nType your answer here, replacing this text.\n\n# reset index\nplot_df = ...\n\n# create base chart\nbase = alt.Chart(plot_df).encode(\n    y = ...\n)\n\n# create point plot\nmeans = base.mark_point().encode(\n    x = ...\n)\n\n# create bar plot\nbars = base.transform_calculate(\n    lwr = ...\n    upr = ...\n).mark_errorbar().encode(\n    x = ...\n    x2 = ...\n)\n\n# layer\nfig2 = ...\n\n# display\nfig2\n\n\nNow that you have a sense of the typical abundances for each taxon (measured by means) and the variations in abundance (measured by standard deviations), you’ll dig in a bit further and examine the variation in abundance of the most variable taxon.\n\n\n\nQuestion 7: Distribution of Azpeitia nodulifer abundance over time\n(i) Construct a density scale histogram of the relative abundances of Azpeitia nodulifer and overlay a kernel density estimate.\nUse the diatoms dataframe and a bin width of 0.03. Be sure to choose an appropriate kernel and smoothing bandwidth. Store the result as fig3.\n(ii) Answer the following questions in a few sentences.\n\nWhich values are common?\nWhich values are rare?\nHow spread out are the values?\nAre values spread evenly or irregularly?\n\nType your answer here, replacing this text.\n\n# density scale histogram\nhist = alt.Chart(diatoms).transform_bin(\n    as_ = ...\n    field = ...\n    bin = ...\n).transform_aggregate(\n    Count = ...\n    groupby = ...\n).transform_calculate(\n    Density = ...\n    binshift = ...\n).mark_bar(size = 25, opacity = 0.8).encode(\n    x = ...\n    y = ...\n)\n\n# compute density estimate\n...\n\n# plot kde\nsmooth = ...\n\n# layer\nfig3 = ...\n\n# display\nfig3\n\n\n\nComment: There are a disproportionately large number of zeroes, because in many samples no Azpeitia nodulifer diatoms were observed. This is a common phenomenon in ecological data, and even has a name: it results in a ‘zero inflated’ distribution of values. The statistician to identify and name the phenomenon was Diane Lambert in 1992. Zero inflation can present a variety of challenges. You may have noticed, for example, that there was no bandwidth parameter for the KDE that both captured the shape of the histogram near zero and away from zero, regardless of the choice of kernel. The key difficulty with zero inflated data is that no common probability distribution fits the data well. This requires the use of mixtures as probability models, which are challenging to incorporate into common statistical models.\n\nThere was a transition between geological epochs during the time span covered by the diatom data. The oldest data points in the diatom data correspond to the end of the Pleistocene epoch (ice age), at which time there was a pronounced warming (Late Glacial Interstadial, 14.7 - 12.9 KyrBP) followed by a return to glacial conditions (Younger Dryas, 12.9 - 11.7 KyrBP).\nThis fluctuation can be seen from temperature reconstructions. Below is a plot of sea surface temperature reconstructions off the coast of Northern California. Data come from the following source:\n\nBarron et al., 2003. Northern Coastal California High Resolution Holocene/Late Pleistocene Oceanographic Data. IGBP PAGES/World Data Center for Paleoclimatology. Data Contribution Series # 2003-014. NOAA/NGDC Paleoclimatology Program, Boulder CO, USA.\n\nThe shaded region indicates the time window with unusually large flucutations in sea surface temperature; this window roughly corresponds to the dates of the climate event.\n\n# import sea surface temp reconstruction\nseatemps = pd.read_csv('data/barron-sst.csv')\n\n# line plot of time series\nline = alt.Chart(seatemps).mark_line().encode(\n    x = alt.X('Age', title = 'Thousands of years before present'),\n    y = 'SST'\n)\n\n# highlight region with large variations\nhighlight = alt.Chart(\n    pd.DataFrame(\n        {'SST': np.linspace(0, 14, 100), \n         'upr': np.repeat(11, 100), \n         'lwr': np.repeat(15, 100)}\n    )\n).mark_area(opacity = 0.2, color = 'orange').encode(\n    y = 'SST',\n    x = alt.X('upr', title = 'Thousands of years before present'),\n    x2 = 'lwr'\n)\n\n# add smooth trend\nsmooth = line.transform_loess(\n    on = 'Age',\n    loess = 'SST',\n    bandwidth = 0.2\n).mark_line(color = 'black')\n\n# layer\nline + highlight + smooth\n\n\n\n\nQuestion 8: Conditional distributions of relative abundance\nDoes the distribution of relative abundance of Azpeitia nodulifer differ when variation in sea temperatures was higher (before 11KyrBP)?\n(i) Plot kernel density estimates to show the distribution of relative abundances before and after 11KyrBP.\nUse the Altair implementation of Gaussian KDE: 1. Use .transform_caluculate(...) to calculate an indicator variable, pleistocene, that indicates whether Age exceeds 11. 2. Use .transform_density(...) to compute KDEs separately for observations of relative abundance before and after 11KyrBP. + Hint: group by pleistocene 3. Plot the KDEs distinguished by color; give the color legend the title ‘Before 11KyrBP’ and store the plot as kdes. 4. Add a shaded area beneath the KDE curves. Adjust the opacity of the area to your liking.\nStore the result as fig4 and display the figure.\n(ii) Does the distribution seem to change between epochs? If so, how?\nAnswer based on the figure in a few sentences.\nType your answer here, replacing this text.\n\n...\n\n\n\n\n\nVisualizing community composition with PCA\nSo far you’ve seen that the abundances of one taxon – Azpeitia nodulifer – change markedly before and after a shift in climate conditions. In this part you’ll use PCA to compare variation in community composition among all eight taxa during the late Pleistocene and Holocene epochs.\n\nQuestion 9: Pairwise correlations in relative abundances\n(i) Compute the pairwise correlations between relative abundances and make a heatmap of the correlation matrix.\nBe sure to remove or set to indices the Depth and Age variables before computing the correlation matrix. Save the matrix as corr_mx.\n\nMelt corr_mx to obtain a dataframe with three columns:\n\nrow, which contains the values of the index of corr_mx (taxon names);\ncolumn, which contains the names of the columns of corr_mx (also taxon names); and\nCorrelation, which contains the values of corr_mx.\nStore the result as corr_mx_long.\n\nCreate an Altair chart based on corr_mx_long and construct the heatmap by following the examples indicated above.\n\nAdjust the color scheme to blueorange over the extent (-1, 1) to obtain a diverging color gradient where a correlation of zero is blank (white).\nAdjust the color legend to indicate the color values corresponding to correlations of 1, 0.5, 0, -0.5, and -1.\nSort the rows and columns in ascending order of correlation.\n\n\n(ii) How does A. nodulifer seem to vary with the other taxa, if at all?\nAnswer in a few sentences based on the heatmap.\nType your answer here, replacing this text.\n\ncorr_mx = ...\n\n# melt corr_mx\n...\n\n# construct heatmap\n...\n\n# display\nfig5\n\n\ngrader.check(\"q9\")\n\n\n\nQuestion 10: Computing and selecting principal components\nHere you’ll perform all of the calculations involved in PCA and check the variance ratios to select an appropriate number of principal components. The parts of this question correspond to the individual steps in this process.\n(i) Center and scale the data columns.\nFor PCA it is usually recommended to center and scale the data; set Depth and Age as indices and center and scale the relative abundances. Store the normalized result as pcdata.\n(ii) Compute the principal components.\nCompute all 8 principal components. For this part you do not need to show any specific output.\n(iii) Examine the variance ratios.\nCreate a dataframe called pcvars with the variance information by following these steps:\n\nStore the proportion of variance explained (called .explained_variance_ratio_ in the PCA output) as a dataframe named pcvars with just one column named Proportion of variance explained.\nAdd a column named Component to pcvars with the integers 1 through 8 as values (indicating the component number).\nAdd a column named Cumulative variance explained to pcvars that is the cumulative sum of Proportion of variance explained.\n\nHint: slice the Proportion of variance explained column and use .cumsum(axis = ...).\n\n\nFor this part you do not need to show any specific output.\n(iv) Plot the variance explained by each PC.\nUse pcvars to construct a dual-axis plot showing the proportion of variance explained (left y axis) and cumulative variance explained (right y axis) as a function of component number (x axis), with points indicating the variance ratios and lines connecting the points. Follow these steps:\n\nConstruct a base chart that encodes only Component on the X channel. Store this as base.\nMake a base layer for the proportion of variance explained that modifies base by encoding Proportion of variance explained on the Y channel. Store the result as prop_var_base.\n\nGive the Y axis title a distinct color of your choosing via alt.Y(..., axis = alt.Axis(titleColor = ...)).\n\nMake a base layer for the cumulative variance explained that modifies base by endocing Cumulative variance explained on the Y channel. Store the result as cum_var_base.\n\nGive the Y axis title another distinct color of your choosing via alt.Y(..., axis = alt.Axis(titleColor = ...)).\n\nCreate a plot layer for the proportion of variance explained by combining points (prop_var_base.mark_point()) with lines (prop_var_base.mark_line()). Store the result as cum_var.\n\nApply the color you chose for the axis title to the points and lines.\n\nRepeat the previous step for the cumulative variance explained.\n\nApply the color you chose for the axis title to the points and lines.\n\nLayer the plots together using alt.layer(l1, l2).resolve_scale(y = 'independent').\n\nStore the result as fig6 and display the figure.\n(v) How many PCs should be used?\nPropose an answer based on the variance explained plots and indicate how much total variation your proposed number of components capture jointly.\nType your answer here, replacing this text.\n\n## (i) center and scale data\n\n# helper variable pcdata_raw; set Depth and Age as indices\npcdata_raw = ...\n\n# center and scale the relative abundances\npcdata = ...\n\n\n## (ii) compute pcs\n\npca = ...\n...\n\n\n## (iii) retrieve variance info\n\n# store proportion of variance explained as a dataframe\npcvars = ...\n\n# add component number as a new column\npcvars['Component'] = ...\n\n# add cumulative variance explained as a new column\npcvars['Cumulative variance explained'] = ...\n\n\n## (iv) plot variance explained\n\n# encode component axis only as base layer\nbase = ...\n\n# make a base layer for the proportion of variance explained\n...\n\n# make a base layer for the cumulative variance explained\n...\n\n# add points and lines to each base layer\n...\n\n# layer the layers\nfig6 = ...\n\n# display\nfig6\n\n\ngrader.check(\"q10\")\n\nNow that you’ve performed the calculations for PCA, you can move on to the fun/difficult part: figuring out what they say about the data.\nThe first step in this process is to examine the loadings. Each principal component is a linear combination of the relative abundances by taxon, and the loadings tell you how that combination is formed; the loadings are the linear combination coefficients, and thus correspond to the weight of each taxon in the corresponding principal component. Some useful points to keep in mind:\n\na high loading value (negative or positive) indicates that a variable strongly influences the principal component;\na negative loading value indicates that\n\nincreases in the value of a variable decrease the value of the principal component\nand decreases in the value of a variable increase the value of the principal component;\n\na positive loading value indicates that\n\nincreases in the value of a variable increase the value of the principal component\nand decreases in the value of a variable decrease the value of the principal component;\n\nsimilar loadings between two or more variables indicate that the principal component reflects their average;\ndivergent loadings between two sets of variables indicates that the principal component reflects their difference.\n\n\n\nQuestion 11: Interpreting component loadings\n(i) Extract the loadings from pca.\nStore the loadings for the first two principal components (called .components_ in the PCA output) in a dataframe named loading_df. Name the columns PC1 and PC2, and append a column Taxon with the corresponding variable names, and print the resulting dataframe.\n(ii) Construct loading plots\nConstruct a line-and-point plot connecting the loadings of the first two principal components. Display the value of the loading on the y axis and the taxa names on the x axis, and show points indicating the loading values. Distinguish the PC’s by color, and add lines connecting the loading values for each principal component. Store the result as fig7 and display the figure – you may need to resize for better readability.\nHint: you will need to first melt loading_df to long form with three columns – the taxon name, the principal component (1 or 2), and the value of the loading.\n(iii) Interpret the first principal component.\nIn a few sentences, answer the following questions. 1. Which taxa are up-weighted and which are down-weighted in this component? 2. How would you describe the principal component in context (e.g., average abundance among a group, differences in abundances, etc.)? 3. How would you interpret a larger value of the PC versus a smaller value of the PC in terms of diatom communnity composition?\n(iv) Interpret the second principal component.\nAnswer the same questions for component 2.\nType your answer here, replacing this text.\n\n## (i) retrieve loadings\n# store the loadings as a data frame with appropriate names\nloading_df = ...\n\n# add a column with the taxon names\nloading_df['Taxon'] = ...\n\n\n## (ii) construct loading plots\n# melt from wide to long\nloading_plot_df = loading_df.melt(\n    id_vars = ...\n    var_name = ...\n    value_name = ...\n)\n\n# create base layer with encoding\nbase = alt.Chart(loading_plot_df).encode(\n    x = ...\n    y = ...\n    color = ...\n)\n\n# store horizontal line at zero\nrule = ...\n\n# layer points + lines + rule to construct loading plot\nfig7 = ...\n\n# show\n...\n\n\ngrader.check(\"q11\")\n\nRecall that there was a shift in climate around 11,000 years ago, and A. nodulifer abundances seemed to differ before and after the shift.\nYou can now use PCA to investigate whether not just individual abundances but community composition may have shifted around that time. To that end, let’s think of the principal components as ‘community composition indices’:\n\nconsider PC1 a nodulifer/non-nodulifer community composition index; and\nconsider PC2 a complex community composition index.\n\nA pattern of variation or covariation in the principal components can be thought of as reflecting a particular ecological community composition dynamic – a way that community composition varies throughout time. Here you’ll look for distinct patterns of variation/covariation before and after 11,000 years ago via an exploratory plot of the principal components.\n\n\nQuestion 12: Visualizing community composition shift\n(i) Project the centered and scaled data onto the first two component directions.\nThis sounds a little more complicated than it is – all that means is compute the values of the principal components for each data point. Create a dataframe called projected_data containing just the first two principal components as two columns named PC1 and PC2, and two additional columns with the Age and Depth variables.\n(ii) Construct a scatterplot of PC1 and PC2 by epoch.\nConstruct a scatterplot of the principal components with observations colored according to whether they occurred in the Pleistocene or Holocene epoch. Store the result as fig8 and display the figure.\n(iii) Comment on the plot: does there appear to be any change in community structure?\nAnswer in a few sentences.\nType your answer here, replacing this text.\n\n## (i) project pcdata onto first two components; store as data frame\n\n# retrieve principal component scores for pc1 and pc2\nprojected_data = ...\n\n# adjust index\nprojected_data.index = ...\nprojected_data = ...\n\n\n## (ii) construct scatterplot\n...\n\n# display\n...\n\n\ngrader.check(\"q12\")\n\n\n\n\n(Optional) Question 13: Multi-panel visualization\nSometimes it’s helpful to see marginal distributions together with a scatterplot. Follow the steps below to create a multi-panel figure with marginal density estimates appended to the projected scatter from the previous question.\n\nCreate an Altair chart based on projected_data and use .transform_calculate(...) to define a variable holocene that indicates whether Age is older than 11,000 years. Store the result as base.\nModify base to add points with the following encodings.\n\nPass PC1 to the X encoding channel and title the axis ‘A. Nodulifer/non-A. nodulifer composition’.\nPass PC2 to the Y encoding channel and title the axis ‘Complex community composition’.\nPass the variable you created in step 1. to the color encoding channel and title it ‘Holocene’. Store the result as scatter.\n\nConstruct plots of kernel density estimates for each principal component conditional on age being older than 11,000 years:\n\nmodify base to create a top_panel plot with the KDE curves for PC1, with color corresponding to the age indicator from the .transform_calculate(...) step in making the base layer;\nmodify base again to create a side_panel plot with the KDE curves for PC2, rotated 90 degrees relative to the usual orientation (flip the typical axes), and with color corresponding to the age indicator from the .transform_calculate(...) step in making the base layer.\n\nThen, resize these panels appropriately (top should be thin, side should be narrow), and use Altair’s faceting operators & (vertical concatenation) and | (horizontal concatenation) to combine them with your scatterplot.\n\nStore the result as fig9 and display the figure.\n\n# make base layer\n...\n\n# data scatter\n...\n\n# construct side panel (kdes for pc2)\n...\n\n# facet\nfig9 = ...\n\n# display\nfig9\n\n\n\n\n\nCommunicating results\nTake a moment to review and reflect on the results of your analysis in the previous parts. Think about how you would describe succinctly what you’ve learned from the diatom data.\n\n\nQuestion 14: Summary\nWrite a brief paragraph (3-5 sentences) that addresses the following questions by referring to your results above.\n\nHow would you characterize the typical ecological community composition of diatom taxa before and after 11,000 years ago?\n\nHint: focus on the side and top panels and the typical values of each index in the two time periods.\n\nDoes the variation in ecological community composition over time seem to differ before and after 11,000 years ago?\n\nHint: focus on the shape of data scatter.\n\n\nType your answer here, replacing this text.\n\n\n\n\nQuestion 15: Further work\nWhat more might you like to know, given what you’ve learned? Pose a question that your exploratory analysis raises for you.\n\nAnswer\nType your answer here.\n\n\n\n\n\nSubmission\n\nSave the notebook.\nRestart the kernel and run all cells. (CAUTION: if your notebook is not saved, you will lose your work.)\nCarefully look through your notebook and verify that all computations execute correctly and all graphics are displayed clearly. You should see no errors; if there are any errors, make sure to correct them before you submit the notebook.\nDownload the notebook as an .ipynb file. This is your backup copy.\nExport the notebook as PDF and upload to Gradescope.\n\n\nTo double-check your work, the cell below will rerun all of the autograder tests.\n\ngrader.check_all()"
  },
  {
    "objectID": "hw/hw2-seda/hw2-seda-soln.html",
    "href": "hw/hw2-seda/hw2-seda-soln.html",
    "title": "Background",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"hw2-seda.ipynb\")\nimport numpy as np\nimport pandas as pd\nimport altair as alt\n# disable row limit for plotting\nalt.data_transformers.disable_max_rows()\n# uncomment to ensure graphics display with pdf export\n# alt.renderers.enable('mimetype')\n\nDataTransformerRegistry.enable('default')\nGender achievement gaps in education have been well-documented over the years – studies consistently find boys outperforming girls on math tests and girls outperforming boys on reading and language tests. A particularly controversial article was published in Science in 1980 arguing that this pattern was due to an ‘innate’ difference in ability (focusing on mathematics rather than on reading and language). Such views persisted in part because studying systematic patterns in achievement nationwide was a challenge due to differential testing standards across school districts and the general lack of availability of large-scale data.\nIt is only recently that data-driven research has begun to reveal socioeconomic drivers of achievement gaps. The Standford Educational Data Archive (SEDA), a publicly available database on academic achievement and educational opportunity in U.S. schools, has supported this effort. The database is part of a broader initiave aiming to improve educational opportunity by enabling researchers and policymakers to identify systemic drivers of disparity.\nThe database standardizes average test scores for schools 10,000 U.S. school districts relative to national standards to allow comparability between school districts and across grade levels and years. The test score data come from the U.S. Department of Education. In addition, multiple data sources (American Community Survey and Common Core of Data) are integrated to provide district-level socioeconomic and demographic information.\nA study of the SEDA data published in 2018 identified the following persistent patterns across grade levels 3 - 8 and school ears from 2008 through 2015: * a consistent reading and language achievement gap favoring girls; * no national math achievement gap on average; and * local math achievement gaps that depend on the socioeconomic conditions of school districts. You can read about the main findings of the study in this brief NY Times article.\nBelow, we’ll work with selected portions of the database. The full datasets can be downloaded here."
  },
  {
    "objectID": "hw/hw2-seda/hw2-seda-soln.html#assignment-objectives",
    "href": "hw/hw2-seda/hw2-seda-soln.html#assignment-objectives",
    "title": "Background",
    "section": "Assignment objectives",
    "text": "Assignment objectives\nIn this assignment, you’ll explore achievement gaps in California school districts in 2018, reproducing the findings described in the article above on a more local scale and with the most recent SEDA data. You’ll practice the following:\n\nreview of data documentation\nassessment of sampling design and scope of inference\ndata tidying operations\n\nslicing and filtering\nmerging multiple data frames\npivoting tables\nrenaming and reordering variables\n\nconstructing exploratory graphics and visualizing trends\ndata aggregations\nnarrative summary of exploratory analysis"
  },
  {
    "objectID": "hw/hw2-seda/hw2-seda-soln.html#test-score-data",
    "href": "hw/hw2-seda/hw2-seda-soln.html#test-score-data",
    "title": "Background",
    "section": "Test score data",
    "text": "Test score data\nThe first few rows of the test data are shown below. The columns are:\n\n\n\n\n\n\n\nColumn name\nMeaning\n\n\n\n\nsedalea\nDistrict ID\n\n\ngrade\nGrade level\n\n\nstateabb\nState abbreviation\n\n\nsedaleaname\nDistrict name\n\n\nsubject\nTest subject\n\n\ncs_mn_...\nEstimated mean test score\n\n\ncs_mnse_...\nStandard error for estimated mean test score\n\n\ntotgyb_...\nNumber of individual tests used to estimate the mean score\n\n\n\n\n# import seda data\nca_main = pd.read_csv('data/ca-main.csv')\nca_cov = pd.read_csv('data/ca-cov.csv')\n\n# preview test score data\nca_main.head(3)\n\n\n\n\n\n\n\n\nsedalea\ngrade\nstateabb\nsedaleaname\nsubject\ncs_mn_all\ncs_mnse_all\ntotgyb_all\ncs_mn_asn\ncs_mnse_asn\n...\ntotgyb_whg\ncs_mn_wht\ncs_mnse_wht\ntotgyb_wht\ncs_mn_wmg\ncs_mnse_wmg\ntotgyb_wmg\ncs_mn_wng\ncs_mnse_wng\ntotgyb_wng\n\n\n\n\n0\n600001\n4\nCA\nACTON-AGUA DULCE UNIFIED ...\nmth\n-0.367007\n0.108543\n86.0\nNaN\nNaN\n...\n79.0\n-0.208654\n0.165783\n35.0\n-0.089003\n0.518066\n38.0\nNaN\nNaN\nNaN\n\n\n1\n600001\n4\nCA\nACTON-AGUA DULCE UNIFIED ...\nrla\n0.005685\n0.117471\n85.0\nNaN\nNaN\n...\n78.0\n0.259587\n0.189614\n35.0\n0.526942\n0.602989\n38.0\nNaN\nNaN\nNaN\n\n\n2\n600001\n6\nCA\nACTON-AGUA DULCE UNIFIED ...\nrla\n-0.000040\n0.092172\n114.0\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n3 rows × 59 columns\n\n\n\nThe test score means for each district are named cs_mn_... with an abbreviation indicating subgroup (such as mean score for all cs_mean_all, for boys cs_mean_mal, for white students cs_mn_wht, and so on). Notice that these are generally small-ish: decimal numbers between -0.5 and 0.5.\nThese means are estimated from a number of individual student tests and standardized relative to national averages. They represent the number of standard deviations by which a district mean differs from the national average. So, for instance, the value cs_mn_all = 0.1 indicates that the district average is estimated to be 0.1 standard deviations greater than the national average on the corresponding test and at the corresponding grade level.\n\n\nQuestion 1: Interpreting test score values\nInterpret the average math test score for all 4th grade students in Acton-Agua Dulce Unified School District (the first row of the dataset shown above).\nType your answer here, replacing this text.\nSOLUTION In Acton-Agua Dulce Unified School District, the mean math test score for fourth graders in 2018 is estimated to be 0.37 standard deviations below the national average."
  },
  {
    "objectID": "hw/hw2-seda/hw2-seda-soln.html#covariate-data",
    "href": "hw/hw2-seda/hw2-seda-soln.html#covariate-data",
    "title": "Background",
    "section": "Covariate data",
    "text": "Covariate data\nThe first few rows of the covariate data are shown below. The column information is as follows:\n\n\n\nColumn name\nMeaning\n\n\n\n\nsedalea\nDistrict ID\n\n\ngrade\nGrade level\n\n\nsedaleanm\nDistrict name\n\n\nurban\nIndicator: is the district in an urban locale?\n\n\nsuburb\nIndicator: is the district in a suburban locale?\n\n\ntown\nIndicator: is the district in a town locale?\n\n\nrural\nIndicator: is the district in a rural locale?\n\n\nlocale\nDescription of district locale\n\n\nRemaining variables\nDemographic and socioeconomic measures\n\n\n\n\nca_cov.head(3)\n\n\n\n\n\n\n\n\nsedalea\ngrade\nsedaleanm\nurban\nsuburb\ntown\nrural\nlocale\nperind\nperasn\n...\nsnapall\nsnapblk\nsnaphsp\nsnapwht\nsingle_momall\nsingle_momblk\nsingle_momhsp\nsingle_momwht\nseswhtblk\nseswhthsp\n\n\n\n\n0\n600001\n4.0\nACTON-AGUA DULCE UNIFIED ...\n0.0\n0.0\n0.0\n1.0\nRural, Distant\n0.003893\n0.045901\n...\n0.035165\n0.20293\n0.0819\n0.032362\n0.084385\n0.349636\n0.198482\n0.061653\n1.839339\n0.692566\n\n\n1\n600001\n5.0\nACTON-AGUA DULCE UNIFIED ...\n0.0\n0.0\n0.0\n1.0\nRural, Distant\n0.003788\n0.046652\n...\n0.035165\n0.20293\n0.0819\n0.032362\n0.084385\n0.349636\n0.198482\n0.061653\n1.839339\n0.692566\n\n\n2\n600001\n6.0\nACTON-AGUA DULCE UNIFIED ...\n0.0\n0.0\n0.0\n1.0\nRural, Distant\n0.003218\n0.043657\n...\n0.035165\n0.20293\n0.0819\n0.032362\n0.084385\n0.349636\n0.198482\n0.061653\n1.839339\n0.692566\n\n\n\n\n3 rows × 60 columns\n\n\n\nYou will only be working with a handful of the demographic and socioeconomic measures, so you can put off getting acquainted with those until selecting a subset of variables.\n\n\nQuestion 2: Data semantics\nIn the non-public data, observational units are students – test scores are measured for each student. However, in the SEDA data you’ve imported, scores are aggregated to the district level by grade. Let’s regard estimated test score means for each grade as distinct variables, so that an observation consists in a set of estimated means for different grade levels and groups. In this view, what are the observational units in the test score dataset? Are they the same or different for the covariate dataset?\nType your answer here, replacing this text.\nSOLUTION The observational units are school districts. They are the same for each dataset.\n\n\n\nQuestion 3: Sample sizes\nHow many observational units are in each dataset? Count the number of units in the test dataset and the number of units in the covariate dataset separately. Store the values as ca_cov_units and ca_main_units, respectively.\n(Hint: use .nunique().)\n\nca_cov_units = ca_cov.sedalea.nunique() # SOLUTION\nca_main_units =ca_main.sedalea.nunique() # SOLUTION\n\nprint('units in covariate data: ', ca_cov_units)\nprint('units in test score data: ', ca_main_units)\n\nunits in covariate data:  913\nunits in test score data:  872\n\n\n\ngrader.check(\"q3\")\n\n\n\n\nQuestion 4: Sample characteristics and scope of inference\nAnswer the questions below about the sampling design in a short paragraph. You do not need to dig through any data documentation in order to resolve these questions.\n\n\nWhat is the relevant population for the datasets you’ve imported?\n\n\nAbout what proportion (to within 0.1) of the population is captured in the sample? (Hint: have a look at this website.)\n\n\nConsidering that the sampling frame is not identified clearly, what kind of dataset do you suspect this is (e.g., administrative, data from a ‘typical sample’, census, etc.)?\n\n\n\nIn light of your description of the sample characteristics, what is the scope of inference for this dataset?\n\n\nType your answer here, replacing this text.\nSOLUTION\nThe population of interest is all school districts in California in 2018. There are ~1000 school districts in total in 2022-2023, and although districts change a little from year to year, the total number doesn’t change too much in adjacent years. So this sample from 2018 likely covers about 90% of the population. The dataset is best described as administrative data: it is not a census, because the entire population is not included; and it is not a ‘typical’ sample or a random sample because there is no random selection mechanism from a well-defined frame. This data does not support inference and should be used for summary purposes only – conclusions should not be generalized beyond the sample. However, this actually isn’t much of a limitation here because the sample covers so much of the population."
  },
  {
    "objectID": "hw/hw2-seda/hw2-seda-soln.html#gender-gaps-and-socioeconomic-factors",
    "href": "hw/hw2-seda/hw2-seda-soln.html#gender-gaps-and-socioeconomic-factors",
    "title": "Background",
    "section": "Gender gaps and socioeconomic factors",
    "text": "Gender gaps and socioeconomic factors\nThe cell below generates a panel of scatterplots showing the relationship between estimated gender gap and socioeconomic factors for all grade levels by test subject. The plot suggests that the reading gap favors girls consistently across the socioeconomic spectrum – in a typical district girls seem to outperform boys by 0.25 standard deviations of the national average. By contrast, the math gap appears to depend on socioeconomic factors – boys only seem to outperform girls under better socioeconomic conditions.\n\n# plot gap against socioeconomic variables by subject for all grades\nfig1 = alt.Chart(plot_df).mark_circle(opacity = 0.1).encode(\n    y = 'Gap',\n    x = alt.X('Measure', scale = alt.Scale(zero = False), title = ''),\n    color = 'Gap type'\n).properties(\n    width = 100,\n    height = 100\n).facet(\n    column = alt.Column('Socioeconomic variable')\n).resolve_scale(x = 'independent')\n\nfig1\n\n\n\n\n\n\n\n\nQuestion 13: Relationships by grade level\nDoes the pattern shown in the plot above persist within each grade level? Modify the plot above to show these relationships by grade level: generate a panel of scatterplots of gap against socioeconomic measures by subject, where each column of the panel corresponds to one socioeconomic variable and each row corresponds to one grade level; the result should by a 5x5 panel. Resize the width and height of each facet so that the panel is of reasonable size. Keep a fixed axis scale for the variable of interest, but allow the axis scales for socioeconomic variables to vary independently. Store the plot as fig2; display the figure and provide an answer to the question of interest in the text cell.\n(Hint: you may find it useful to have a look at the altair documentation on compound charts, and lab 3, for examples to follow.)\nType your answer here, replacing this text.\n\n# plotting codes here\n# BEGIN SOLUTION\nfig2 = alt.Chart(plot_df).mark_circle(opacity = 0.2).encode(\n    y = 'Gap',\n    x = alt.X('Measure', scale = alt.Scale(zero = False), title = ''),\n    color = 'Gap type'\n).properties(\n    width = 75,\n    height = 75\n).facet(\n    column = alt.Column('Socioeconomic variable'),\n    row = alt.Row('Grade')\n).resolve_scale(x = 'independent')\n\n# END SOLUTION\n\n# display\nfig2 # SOLUTION\n\n\n\n\n\n\nSOLUTION Yes, the patterns observed across all grade levels are similar to the patterns within grade level.\n\n\n\n\nQuestion 14: Association with grade level\nDo gaps shift across grade levels? It’s not so easy to tell from the last figure. Construct a 2x5 panel of scatterplots showing estimated achievement gap against each of the 5 socioeconomic variables, with one row per test subject. Display grade level using a color gradient. Store the plot as fig3; display the figure and answer the question of interest in a short sentence or two in the text cell provided.\nType your answer here, replacing this text.\n\n# plotting codes here\n# BEGIN SOLUTION\nfig3 = alt.Chart(plot_df).mark_circle(opacity = 0.2).encode(\n    y = 'Gap',\n    x = alt.X('Measure', scale = alt.Scale(zero = False), title = ''),\n    color = 'Grade'\n).properties(\n    width = 75,\n    height = 75\n).facet(\n    column = alt.Column('Socioeconomic variable'),\n    row = alt.Row('Gap type')\n).resolve_scale(x = 'independent')\n\n# END SOLUTION\n\n# display\nfig3 # SOLUTION\n\n\n\n\n\n\nSOLUTION Yes – the scatter shifts from dark to light as the estimated gap decreases for both subjects and all socioeconomic variables, indicating that as grade level increases, the gap increasingly favors girls in both math and reading and language.\n\nWhile the magnitude of the achievement gaps seems to depend very slightly on grade level (figure 3), the form of relationship between achievement gap and socioeconomic factors does not differ from grade to grade (figure 2).\nGiven that the relationships between achievement gaps and socioeconomic factors don’t change drastically across grade levels, it is reasonable to look at the average relationship between estimated achievement gap and median income after aggregating across grade.\n\n\nQuestion 15: Aggregation across grade levels\nCompute the mean estimated achievement gap in each subject across grade levels by district using District ID and retain the district-level socioeconomic variables. Store the resulting data frame as seda_data_agg.\nNote: best practice here would be to aggregate just the test scores by district and then re-merge the result with the district-level socioeconomic variables. However, since the district-level socioeconomic variables do not differ by grade within a district, averaging them across grade levels by district together with the test scores will simply return their unique values; so the aggregation can be applied across all columns for a fast-and-loose way to obtain the desired result.\n\n# aggregate across grades\n# BEGIN SOLUTION\nseda_data_agg = seda_data.groupby(\n    ['District ID']\n    ).mean(\n    numeric_only = True\n    ).reset_index(\n    ).drop(columns = 'Grade')\n# END SOLUTION\n\n# print first few rows\nseda_data_agg.head() # SOLUTION\n\n\n\n\n\n\n\n\nDistrict ID\nlog(Median income)\nPoverty rate\nUnemployment rate\nSNAP rate\nSocioeconomic index\nMath gap\nReading gap\n\n\n\n\n0\n600001\n11.392048\n0.091894\n0.048886\n0.035165\n1.237209\n-0.562855\n-0.785321\n\n\n1\n600006\n11.607236\n0.041418\n0.048269\n0.028006\n1.912972\n0.061163\n-0.242572\n\n\n2\n600011\n10.704570\n0.159981\n0.066333\n0.102054\n-0.478127\n-0.015417\n-0.191400\n\n\n3\n600012\n10.589787\n0.179102\n0.059158\n0.074903\n-0.096379\nNaN\nNaN\n\n\n4\n600013\n11.399662\n0.060338\n0.045533\n0.035016\n1.398133\n0.054454\n-0.312638\n\n\n\n\n\n\n\n\ngrader.check(\"q15\")\n\n\n\nQuestion 16: Melt aggregated data for plotting\nSimilar to working with the disaggregated data, it will be helpful for plotting to melt the two gap variables into a single column. Follow the example above at the beginning of this section to melt only the test score gap columns (not the district-level variables – we will not create scatterplot panels as before). Name the new columns Subject and Average estimated gap; store the resulting data frame as agg_plot_df and print the first four rows.\n\n# format for plotting\n# BEGIN SOLUTION\nagg_plot_df = seda_data_agg.melt(\n    id_vars = seda_data_agg.columns[0:6],\n    value_vars = ['Math gap', 'Reading gap'],\n    var_name = 'Subject',\n    value_name = 'Average estimated gap'\n)\n# END SOLUTION\n\n# print four rows\nagg_plot_df.head(4) # SOLUTION\n\n\n\n\n\n\n\n\nDistrict ID\nlog(Median income)\nPoverty rate\nUnemployment rate\nSNAP rate\nSocioeconomic index\nSubject\nAverage estimated gap\n\n\n\n\n0\n600001\n11.392048\n0.091894\n0.048886\n0.035165\n1.237209\nMath gap\n-0.562855\n\n\n1\n600006\n11.607236\n0.041418\n0.048269\n0.028006\n1.912972\nMath gap\n0.061163\n\n\n2\n600011\n10.704570\n0.159981\n0.066333\n0.102054\n-0.478127\nMath gap\n-0.015417\n\n\n3\n600012\n10.589787\n0.179102\n0.059158\n0.074903\n-0.096379\nMath gap\nNaN\n\n\n\n\n\n\n\n\ngrader.check(\"q16\")\n\n\n\n\nQuestion 17: District average gaps\nConstruct a scatterplot of the average estimated gap against log(Median income) by subject for each district and add trend lines (see lab 4). Store the plot as fig4. Describe and interpret the plot in a few sentences.\nType your answer here, replacing this text.\n\n# scatterplot\n# BEGIN SOLUTION\nbase = alt.Chart(agg_plot_df).mark_point(opacity = 0.5).encode(\n    y = 'Average estimated gap',\n    x = alt.X('log(Median income)', scale = alt.Scale(zero = False)),\n    color = 'Subject'\n)\n# END SOLUTION\n\n# trend line\ntrend = base.transform_regression('log(Median income)', 'Average estimated gap', groupby = ['Subject']).mark_line() # SOLUTION\n\n# combine layers\nfig4 = base + trend # SOLUTION\n\n# display\nfig4 # SOLUTION\n\n\n\n\n\n\nSOLUTION Figure 4 shows average estimated achievement gaps – the difference between boys’ scores and girls’ scores – on math and reading tests across grade levels for 872 school districts in California against the median district income. Linear fits help visualize trends. The reading achievement gap favors girls and appears uncorrelated with median income; the math achievement gap increasingly favors boys in more affluent districts.\n\nNow let’s try to capture this pattern in tabular form. The cell below adds an Income bracket variable by cutting the median income into 8 contiguous intervals using pd.cut(), and tabulates the average socioeconomic measures and estimated gaps across districts by income bracket. Notice that with respect to the gaps, this displays the pattern that is shown visually in the figures above.\n\nseda_data_agg['Income bracket'] = pd.cut(np.e**seda_data_agg['log(Median income)'], 8)\nseda_data_agg.groupby('Income bracket').mean().drop(columns = ['District ID', 'log(Median income)'])\n\n\n\n\n\n\n\n\nPoverty rate\nUnemployment rate\nSNAP rate\nSocioeconomic index\nMath gap\nReading gap\n\n\nIncome bracket\n\n\n\n\n\n\n\n\n\n\n(21980.176, 46455.372]\n0.194870\n0.072689\n0.155061\n-0.651999\n-0.070284\n-0.309743\n\n\n(46455.372, 70736.321]\n0.134078\n0.063788\n0.095303\n0.291085\n-0.034061\n-0.315545\n\n\n(70736.321, 95017.269]\n0.088713\n0.052785\n0.048242\n1.110433\n0.004239\n-0.302114\n\n\n(95017.269, 119298.218]\n0.064131\n0.046848\n0.030548\n1.640159\n0.050006\n-0.287117\n\n\n(119298.218, 143579.167]\n0.050315\n0.044343\n0.011023\n2.167272\n0.090138\n-0.289529\n\n\n(143579.167, 167860.115]\n0.043896\n0.042379\n0.008451\n2.382258\n0.084683\n-0.335975\n\n\n(167860.115, 192141.064]\n0.040552\n0.040120\n0.010159\n2.652906\n0.175793\n-0.232306\n\n\n(192141.064, 216422.013]\n0.047097\n0.054055\n0.002555\n2.588499\n0.267301\n-0.299798\n\n\n\n\n\n\n\n\n\nQuestion 18: Proportion of districts with a math gap\nWhat proportion of districts in each income bracket have an average estimated math achievement gap favoring boys? Answer this question by performing the following steps:\n\nAppend an indicator variable Math gap favoring boys to seda_data_agg that records whether the average estimated math gap favors boys by more than 0.1 standard deviations relative to the national average.\nCompute the proportion of districts in each income bracket for which the indicator is true: group by bracket and take the mean. Store this as income_bracket_boys_favored\n\n\n# define indicator\nseda_data_agg['Math gap favoring boys'] = seda_data_agg['Math gap'] &gt; 0.1 # SOLUTION\n\n# proportion of districts with gap favoring boys, by income bracket\n# BEGIN SOLUTION\nincome_bracket_boys_favored = seda_data_agg.groupby(\n    ['Income bracket']\n    ).mean(\n    ).reset_index(\n    ).loc[:, ['Income bracket', 'Math gap favoring boys']] \n# END SOLUTION\n\n# print result\nincome_bracket_boys_favored # SOLUTION\n\n\n\n\n\n\n\n\nIncome bracket\nMath gap favoring boys\n\n\n\n\n0\n(21980.176, 46455.372]\n0.036585\n\n\n1\n(46455.372, 70736.321]\n0.061224\n\n\n2\n(70736.321, 95017.269]\n0.084337\n\n\n3\n(95017.269, 119298.218]\n0.232143\n\n\n4\n(119298.218, 143579.167]\n0.388889\n\n\n5\n(143579.167, 167860.115]\n0.444444\n\n\n6\n(167860.115, 192141.064]\n0.500000\n\n\n7\n(192141.064, 216422.013]\n1.000000\n\n\n\n\n\n\n\n\ngrader.check(\"q18\")\n\n\n\nQuestion 19: Statewide averages\nTo wrap up the exploration, calculate a few statewide averages to get a sense of how some of the patterns above compare with the state as a whole.\n\n\nCompute the statewide average estimated achievement gaps. Store the result as state_avg.\n\n\nCompute the proportion of districts in the state with a math gap favoring boys. Store this result as math_boys_proportion\n\n\nCompute the proportion of districts in the state with a math gap favoring girls. You will need to define a new indicator within seda_data_agg to perform this calculation.\n\n\n\n# statewide average\nstate_avg = seda_data_agg.loc[:, ['Reading gap', 'Math gap']].mean() # SOLUTION\n\n# proportion of districts in the state with a math gap favoring boys\nmath_boys_proportion = seda_data_agg['Math gap favoring boys'].mean() # SOLUTION\n\n# proportion of districts in the state with a math gap favoring girls\nseda_data_agg['Math gap favoring girls'] = seda_data_agg['Math gap'] &lt; -0.1 # SOLUTION\nmath_girls_proportion = seda_data_agg['Math gap favoring girls'].mean() # SOLUTION\n\n\ngrader.check(\"q19\")"
  },
  {
    "objectID": "hw/hw4-dds/hw4-dds.html",
    "href": "hw/hw4-dds/hw4-dds.html",
    "title": "Background: California Department of Developmental Services",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"hw4-dds.ipynb\")\n\n\nimport numpy as np\nimport pandas as pd\nimport altair as alt\nimport statsmodels.api as sm\n# disable row limit for plotting\nalt.data_transformers.disable_max_rows()\n# uncomment to ensure graphics display with pdf export\n# alt.renderers.enable('mimetype')\n\nFrom Taylor, S. A., & Mickel, A. E. (2014). Simpson’s Paradox: A Data Set and Discrimination Case Study Exercise. Journal of Statistics Education, 22(1):\n\nMost states in the USA provide services and support to individuals with developmental disabilities (e.g., intellectual disability, cerebral palsy, autism, etc.) and their families. The agency through which the State of California serves the developmentally-disabled population is the California Department of Developmental Services (DDS) … One of the responsibilities of DDS is to allocate funds that support over 250,000 developmentally-disabled residents. A number of years ago, an allegation of discrimination was made and supported by a univariate analysis that examined average annual expenditures on consumers by ethnicity. The analysis revealed that the average annual expenditures on Hispanic consumers was approximately one-third of the average expenditures on White non-Hispanic consumers. This finding was the catalyst for further investigation; subsequently, state legislators and department managers sought consulting services from a statistician.\n\nIn this assignment, you’ll analyze the deidentified DDS data published with this article to answer the question: is there evidence of ethnic or gender discrimination in allocation of DDS funds? This will involve practicing the following:\n\nexploratory data visualization\nregression analysis\nmodel visualization\n\nAside: The JSE article focuses on what’s known as Simpson’s paradox, an arithmetic phenomenon in which aggregate trends across multiple groups show the opposite of within-group trends. We won’t emphasize this topic, though the data does provide a nice illustration.\n\nDDS data\nThe data for this assignment are already tidy, so in this section you’ll just familiarize yourself with basic characteristics. The first few rows of the data are shown below:\n\ndds = pd.read_csv('data/california-dds.csv')\ndds.head()\n\nTake a moment to open and read the data documentation (data &gt; california-dds-documentation.md).\n\n\nQuestion 1: Data description\nWrite a short paragraph answering the following questions based on the data documentation.\n\n\nWhy were the data collected? What is the purpose of this dataset?\n\n\nWhat are the observational units?\n\n\nWhat is the population of interest?\n\n\nHow was the sample obtained (e.g. random sampling, adminsitrative data, convenience sampling, etc.)?\n\n\nCan inferences about the population be drawn from the sample?\n\n\nIn addition, make a table summarizing the variables measured. Use the format below.\n\n\n\nName\nVariable description\nType\nUnits of measurement\n\n\n\n\nID\nUnique consumer identifier\nNumeric\nNone\n\n\n\nType your answer here, replacing this text.\n\n\n\n\nExploratory analysis\nHere you’ll use graphical and descriptive techniques to explore the allegation of discriminatory allocation of benefits.\n\nQuestion 2: Alleged discrimination\nConstruct a table of median expenditures by ethnicity that also shows the sample size for each ethnic group in the data.\n\nSlice the ethnicity and expenditure variables from dds, group by ethnicity, and calculate the median expenditure. Store the resulting dataframe as median_expend_by_eth.\nCompute the sample sizes for each ethnicity using .value_counts(): obtain a pandas series indexed by ethnicity with a single column named n. You’ll need to use .rename(...) to avoid having the column named Ethnicity. Store this pandas series as ethnicity_n.\nUse pd.concat(...) to append the sample sizes in ethnicity_n to the median expenditures in median_expend_by_eth. Store the result as tbl_1.\n\nPrint tbl_1. Does expenditure seem to differ by ethnicity? Does sample size?\nType your answer here, replacing this text.\n\n# compute median expenditures\nmedian_expend_by_eth = ...\n\n# compute sample sizes\nethnicity_n = ...\n\n# concatenate\ntbl_1 = ...\n\n# print\ntbl_1\n\n\ngrader.check(\"q2\")\n\n\n\n\nQuestion 3: Plot median expenditures\nConstruct a point-and-line plot of median expenditure (y) against ethnicity (x), with: * ethnicities sorted by descending median expenditure; * the median expenditure axis shown on the log scale; * the y-axis labeled ‘Median expenditure’; and * no x-axis label (since the ethnicity group names are used to label the axis ticks, the label ‘Ethnicity’ is redundant).\nStore the result as fig_1 and display the plot.\nHints: * you’ll need to use tbl_1.reset_index() to obtain the ethnicity group as a variable; * recall that .mark_line(point = True) will add points to a line plot; * sorting can be done using alt.X(..., sort = alt.EncodingSortField(field = ..., order = ...))\n\n...\n\n\n\n\n\nQuestion 4: Age and expenditure\nHow does expenditure differ by age? Construct a scatterplot of expenditure against age. Store the plot as fig_2. In one or two sentences, comment on the plot – what is the main pattern it reveals?\nType your answer here, replacing this text.\n\n# construct scatterplot\n...\n\n# display\n...\n\n\nPrecisely because recipients have different needs at different ages that translate to jumps in expenditure, age has been discretized into age cohorts defined based on need level. Going forward, we’ll work with these age cohorts – by treating age as discrete, we won’t need to attempt to model the discontinuities in the relationship between age and expenditure.\nThe cohort labels are stored as Age Cohort in the dataset. There are six cohorts; the cell below coerces the labels to an ordered category, puts them in the proper order, and prints the category levels.\n\n# convert data types\ndds_cat = dds.astype({'Age Cohort': 'category', 'Ethnicity': 'category', 'Gender': 'category'}).copy()\n\ndds_cat['Age Cohort'] = dds_cat['Age Cohort'].cat.as_ordered().cat.reorder_categories(\n    dds_cat['Age Cohort'].cat.categories[[0, 5, 1, 2, 3, 4]]\n)\n\n# age cohorts\ndds_cat['Age Cohort'].cat.categories\n\nHere is an explanation of how the cohort age boundaries were chosen:\n\nThe 0-5 cohort (preschool age) has the fewest needs and requires the least amount of funding. For the 6-12 cohort (elementary school age) and 13-17 (high school age), a number of needed services are provided by schools. The 18-21 cohort is typically in a transition phase as the consumers begin moving out from their parents’ homes into community centers or living on their own. The majority of those in the 22-50 cohort no longer live with their parents but may still receive some support from their family. Those in the 51+ cohort have the most needs and require the most amount of funding because they are living on their own or in community centers and often have no living parents.\n\nNote that the ordering can be retrieved using .cat.codes, which coerces an ordered categorical variable to its integer encoding (0 for lowest level, 1 for next lowest, and so on). It will be helpful to store the ordering for plotting purposes.\n\n# retrieve ordering\ndds_cat['cohort_order'] = dds_cat['Age Cohort'].cat.codes.head()\ndds_cat.head()\n\n\n\n\nQuestion 5: age structure of the sample\nHere you’ll explore the age composition of each ethnic group in the sample.\n\n\nGroup the data by ethnic group and tabulate the sample sizes for each group. Use dds_cat so that the order of age cohorts is preserved. Store the result as samp_sizes.\n\n\nVisualize the age structure of each ethnic group in the sample. Construct a point-and-line plot of the sample size (y) against age cohort (x) by ethnicity (color or linetype). Make sure to preserve the ordering of age cohorts on the x axis (hint: create a variable like cohort_order above). Store the plot as fig_3 and display.\n\n\nComment on the figure. Are there differences in age composition by ethnic group among the individuals sampled?\nType your answer here, replacing this text.\n\n# compute sample sizes for each age/ethnic group\nsamp_sizes = dds_cat.groupby(\n    ...\n).Id.count().reset_index().rename(\n    columns = ...\n)\n\n# construct plot\n...\n\n# display\n...\n\n\nAge structure among ethnic groups might be related to the observed differences in median expenditure, because we know that:\n\n\namong the individuals in the sample, age distributions differed by ethnic group\n\n\nage is related to benefit expenditure\n\n\nTo see this, think through an example.\n\n\n\nQuestion 6: potential confounding\nLook at the age distribution for Multi Race and consider the age-expenditure relationship. Can you explain why the median expenditure for this group might be lower than the others? Answer in 1-2 sentences.\nType your answer here, replacing this text.\n\n\n\n\nQuestion 7: correcting for age\nHopefully, the last few prompts convinced you that the apparent discrimination could simply be an artefact of differing age structure. You can investigate this by plotting median expenditure against ethnicity, as in figure 1, but now also correcting for age cohort.\nConstruct an Altair point-and-line chart based on dds_cat with:\n\nethnicity on the x axis\nno x axis label\nmedian expenditure on the y axis\nthe y axis displayed on the log scale\nage cohort mapped to color and sorted in order of age\nlines connecting points that display the median expenditure for each ethnicity and cohort, with one line per age cohort\n\nStore the result as fig_4 and display the graphic.\n\n# construct plot\n...\n\n# display\n...\n\n\n\n\n\nRegression analysis\nNow that you’ve thoroughly explored the data, you’ll use a linear model in this part to estimate the differences in median expenditure that you observed graphically in part 1.\nMore specifically, you’ll model the log of expenditures (response variable) as a function of gender, age cohort, and ethnicity:\n\\[\n\\log\\left(\\text{expend}_i\\right)\n    = \\beta_0 + \\underbrace{\\beta_1\\left(\\text{6-12}\\right)_i + \\cdots + \\beta_5\\left(\\text{51+}\\right)_i}_\\text{age cohort} + \\underbrace{\\beta_6\\text{female}_i}_\\text{sex} + \\underbrace{\\beta_7\\text{hispanic}_i + \\cdots + \\beta_{13}\\text{other}_i}_\\text{ethnicity} + \\epsilon_i\n\\]\nIn this model, all of the explanatory variables are categorical and encoded using indicators; in this case, the linear model coefficients capture means for each group.\nBecause this model is a little different than the examples you’ve seen so far in two respects – the response variable is log-transformed and all explanatory variables are categorical – some comments are provided below on these features. You can review or skip the comments, depending on your level of interest in understanding the model better mathematically.\nCommments about parameter interpretation\nIn particular, each coefficient represents a difference in means from the ‘baseline’ group. All indicators are zero for a white male recipient between ages 0 and 5, so this is the baseline group and:\n\\[\\mathbb{E}\\left(\\log(\\text{expend})\\;|\\; \\text{male, white, 0-5}\\right) = \\beta_1\\]\nThen, the expected log expenditure for a hispanic male recipient between ages 0 and 5 is:\n\\[\\mathbb{E}\\left(\\log(\\text{expend})\\;|\\; \\text{male, hispanic, 0-5}\\right) = \\beta_0 + \\beta_7\\]\nSo \\(\\beta_7\\) is the difference in mean log expenditure between hispanic and white recipients after accounting for gender and age. The other parameters have similar interpretations.\nWhile the calculation shown above may seem a little foreign, you should know that the parameters represent marginal differences in means between genders (holding age and ethnicity fixed), between ages (holding gender and ethnicity fixed), and between ethnicities (holding age and gender fixed).\nComments about the log transformation\nThe response in this model is the log of expenditures (this gives a better model for a variety of reasons). The statistical assumption then becomes that:\n\\[\\log(\\text{expend})_i \\sim N\\left(\\mathbf{x}_i'\\beta, \\sigma^2\\right)\\]\nIf the log of a random variable \\(Y\\) is normal, then \\(Y\\) is known as a lognormal random variable; it can be shown mathematically that the exponentiated mean of \\(\\log Y\\) is the median of \\(Y\\). As a consequence, according to our model:\n\\[\\text{median}(\\text{expend}_i) = \\exp\\left\\{\\mathbf{x}_i'\\beta\\right\\}\\]\nYou’ll work on the log scale throughout to avoid complicating matters, but know that this model for the log of expenditures is equivalently a model of the median expenditures.\nThe cell below reorders the category levels to match the model written above. To ensure the parameters appear in the proper order, this reordering is done for you.\n\n# remove ID and quantitative age\nreg_data = dds_cat.copy().drop(columns = ['Id', 'Age'])\n\n# reorder ethnicity\nreg_data['Ethnicity'] = reg_data.Ethnicity.cat.as_ordered().cat.reorder_categories(\n    reg_data.Ethnicity.cat.categories[[7, 3, 2, 1, 5, 0, 4, 6]]\n)\n\n# reorder gender\nreg_data['Gender'] = reg_data.Gender.cat.as_ordered().cat.reorder_categories(['Male', 'Female'])\n\n\nQuestion 8: Data preprocessing\nObtain the explanatory variable matrix and response vector needed to fit the linear model.\n\nUse pd.get_dummies(..., drop_first = True) to create the indicator variable encodings for gender, ethnicity, and age. Note that this function can process multiple categorical variables at once. Store the data frame of indicators for all three variables as indicators.\nAdd an intercept to obtain the explanatory variable matrix. Store this as a data frame called x.\nStore the response variable as a pandas series named y.\n\n\nindicators = ...\nx = ...\ny = ...\n\n\ngrader.check(\"q8\")\n\n\n\nQuestion 9: model fitting\nFit the model:\n\\[\n\\log\\left(\\text{expend}_i\\right)\n    = \\beta_0 + \\beta_1\\left(\\text{6-12}\\right)_i + \\cdots + \\beta_5\\left(\\text{51+}\\right)_i\n        + \\beta_6\\text{female}_i\n        + \\beta_7\\text{hispanic}_i + \\cdots + \\beta_{13}\\text{other}_i\n        + \\epsilon_i\n\\]\nStore the parameter estimates and standard errors as a data frame named coef_tbl. Index the data frame by variable name, and don’t forget to include the error variance estimate. Display the result.\n\n# fit model\nmlr = ...\nrslt = ...\n\n# retreive estimates and std errors\ncoef_tbl = pd.DataFrame(\n    ...\n)\ncoef_tbl.loc['error variance', 'estimate'] = ...\n\n# display\ncoef_tbl\n\n\ngrader.check(\"q9\")\n\nNow look at both the estimates and standard errors for each level of each categorical variable; if some estimates are large for at least one level and the standard errors aren’t too big, then estimated mean log expenditures differ according to the value of that variable when the other variables are held constant.\nFor example: the estimate for Gender_Female is 0.04; that means that, if age and ethnicity are held fixed, the estimated difference in mean log expenditure between female and male recipients is 0.04. If \\(\\log(a) - \\log(b) = 0.04\\), then \\(\\frac{a}{b} = e^{0.04} \\approx 1.041\\); so the estimated expenditures (not on the log scale) differ by a factor of about 1, i.e., are about the same. Further, the standard error is 0.02, so the estimate is within 2SE of 0; the difference could well be zero. So the model suggests there is no difference in expenditure by gender.\n\n\n\nQuestion 10: interpretation\nDo the parameter estimates suggest differences in expenditure by age or ethnicity?\nFirst consider the estimates and standard errors for each level of age, and state whether any differences in mean log expenditure between levels appear significant; if so, cite one example. Then do the same for the levels of ethnicity. Answer in 2-4 sentences.\n(Hint: it may be helpful scratch work to exponentiate the coefficient estimates and consider whether they differ by much from 1.)\nType your answer here, replacing this text.\n\n# exponentiate parameter estimates\n...\n\n\nNow as a final step in the analysis, you’ll visualize your results. The idea is simple: plot the estimated mean log expenditures for each group. Essentially you’ll make a version of your figure 4 from part 1 in which the points are estimated rather than observed. So the model visualization graphic will look similar to your exploratory figure.\nThe cell below constructs a prediction grid for you. This grid comprises all unique combinations of the age, sex, and ethnicity categories.\n\n# obtain unique values of each categorical variable\ngenders = reg_data.Gender.cat.categories.values\nethnicities = reg_data.Ethnicity.cat.categories.values\nages = reg_data['Age Cohort'].cat.categories.values\n\n# generate mesh\ngx, ex, ax = np.meshgrid(genders, ethnicities, ages)\ngrid = np.array([gx.ravel(), ex.ravel(), ax.ravel()]).T\n\n# coerce to dataframe\ngrid_df = pd.DataFrame(grid, columns = ['Gender', 'Ethnicity', 'Age Cohort'])\ngrid_df.head()\n\n\n\nQuestion 11: compute predictions\nCalculate predictions with confidence intervals for the predicted mean for each grid point; append these to grid_df and store the result as pred_df. Ensure that the column containing the predictions is named mean.\nNote that you will need to generate indicators in order to compute the predictions; this can be done in the same way that the data were preprocessed. Note also that you will need to arrange the indicator columns in exactly the same order that they appear in the explanatory variable matrix in order to generate valid predictions.\n\n# generate indicators based on the prediction grid\ngrid_indicators = ...\n\n# add an intercept and arrange columns to match x\nx_grid = ...\n\n# compute predictions\npreds = ...\n\n# append values of categorical variables at grid points to predictions\npred_df = ...\n\n# preview\npred_df.head()\n\n\ngrader.check(\"q11\")\n\n\n\n\nQuestion 12: model visualization\nPlot estimated mean log expenditure (y) against ethnicity (x) by age cohort (color) and gender (facet). Construct a line plot with points at each estimated value, and include confidence bands. Use a sequential color scale for age and ensure that the age cohorts are in appropriate order (you may want to construct another cohort_order variable as before for this purpose).\n\n# add cohort ordering\npred_df['cohort_order'] = ...\n\n# construct point-and-line plot\nlines = alt.Chart(pred_df).mark_line(point = True).encode(\n    x = ...\n    y = ...\n    color = ...\n)\n\n# construct confidence bands\nbands = alt.Chart(pred_df).mark_area(opacity = 0.4).encode(\n    x = ...\n    y = ...\n    y2 = ...\n    color = ...\n)\n\n# layer then facet\nfig_5 = ...\n\n# display\nfig_5\n\n\n\n\n\nQuestion 13: uncertainty\nWhich estimates have greater uncertainty and why? Identify the ethnic groups for which the uncertainty band is relatively wide in the plot. Why might uncertainty be higher for these groups? Answer in 2 sentences.\n(Hint: it may help to refer to figure 3.)\nType your answer here, replacing this text.\n\n\n\n\nCommunicating results\nReview your exploratory and regression analyses above, and then answer the following questions.\n\n\nQuestion 14: summary\nWrite a one-paragraph summary of your analysis. Focus on answering the question, ‘do the data provide evidence of ethnic or gender discrimination in allocation of DDS funds?’\nYour summary should include the following:\n\na description of the data indicating observations, variables, and sampling mechanism;\na description of any important exploratory findings;\na description of the method you used to analyze the data (don’t worry about capturing every detail);\na description of the findings of the analysis;\nan answer to the question.\n\nType your answer here, replacing this text.\n\n\n\n\nSubmission\n\nSave the notebook.\nRestart the kernel and run all cells. (CAUTION: if your notebook is not saved, you will lose your work.)\nCarefully look through your notebook and verify that all computations execute correctly and all graphics are displayed clearly. You should see no errors; if there are any errors, make sure to correct them before you submit the notebook.\nDownload the notebook as an .ipynb file. This is your backup copy.\nExport the notebook as PDF and upload to Gradescope.\n\n\nTo double-check your work, the cell below will rerun all of the autograder tests.\n\ngrader.check_all()"
  },
  {
    "objectID": "hw/hw4-dds/data/california-dds-documentation.html",
    "href": "hw/hw4-dds/data/california-dds-documentation.html",
    "title": "Data documentation for california-dds.csv",
    "section": "",
    "text": "NAME: Expenditure data for developmentally-disabled California residents TYPE: Sample SIZE: 1000 observations, 6 variables ARTICLE TITLE: Simpson’s paradox: A data set and discrimination case study exercise\n\n\nThe State of California Department of Developmental Services (DDS) is responsible for allocating funds that support over 250,000 developmentally-disabled residents (referred to as “consumers”). The data set represents a sample of 1,000 of these consumers. Biographical characteristics and expenditure data (i.e., the dollar amount the State spends on each consumer in supporting these individuals and their families) are included in the data set for each consumer.\n\n\n\nThe data set originates from DDS’s “Client Master File.” In order to remain in compliance with California State Legislation, the data have been altered to protect the rights and privacy of specific individual consumers.\n\n\n\nThe data reside in a csv file and are tab-delimited. A header line contains the name of the variables. There are no missing values.\nId: 5-digit, unique identification code for each consumer (similar to a social security number and used for identification purposes)\nAge Cohort: Binned age variable represented as six age cohorts (0-5, 6-12, 13-17, 18-21, 22-50, and 51+) Age: Unbinned age variable Gender: Male or Female Expenditures: Dollar amount of annual expenditures spent on each consumer Ethnicity: Eight ethnic groups (American Indian, Asian, Black, Hispanic, Multi-race, Native Hawaiian, Other, and White non-Hispanic)\n\n\n\nThese data are from a dataset used in an alleged case of discrimination privileging White non-Hispanics over Hispanics in the allocation of funds. Based on the initial analysis, it would appear that discrimination existed; however, a more in-depth analysis revealed that discrimination did not exist and that Simpson’s-paradox phenomenon had occurred.\n\n\n\nThis data set can be used to teach a range of statistical concepts including Simpson’s paradox. The importance of considering all variables in an analysis by conducting a bivariate (instead of just a univariate analysis) is highlighted in this data set.\n\n\n\nName: Stanley Taylor and Amy Mickel Affiliation: California State University, Sacramento Address: College of Business Administration, CSUS, Sacramento, CA 95819-6088, USA Email: sataylor@csus.edu, mickela@csus.edu"
  },
  {
    "objectID": "hw/hw4-dds/data/california-dds-documentation.html#descriptive-abstract",
    "href": "hw/hw4-dds/data/california-dds-documentation.html#descriptive-abstract",
    "title": "Data documentation for california-dds.csv",
    "section": "",
    "text": "The State of California Department of Developmental Services (DDS) is responsible for allocating funds that support over 250,000 developmentally-disabled residents (referred to as “consumers”). The data set represents a sample of 1,000 of these consumers. Biographical characteristics and expenditure data (i.e., the dollar amount the State spends on each consumer in supporting these individuals and their families) are included in the data set for each consumer."
  },
  {
    "objectID": "hw/hw4-dds/data/california-dds-documentation.html#source",
    "href": "hw/hw4-dds/data/california-dds-documentation.html#source",
    "title": "Data documentation for california-dds.csv",
    "section": "",
    "text": "The data set originates from DDS’s “Client Master File.” In order to remain in compliance with California State Legislation, the data have been altered to protect the rights and privacy of specific individual consumers."
  },
  {
    "objectID": "hw/hw4-dds/data/california-dds-documentation.html#variable-descriptions",
    "href": "hw/hw4-dds/data/california-dds-documentation.html#variable-descriptions",
    "title": "Data documentation for california-dds.csv",
    "section": "",
    "text": "The data reside in a csv file and are tab-delimited. A header line contains the name of the variables. There are no missing values.\nId: 5-digit, unique identification code for each consumer (similar to a social security number and used for identification purposes)\nAge Cohort: Binned age variable represented as six age cohorts (0-5, 6-12, 13-17, 18-21, 22-50, and 51+) Age: Unbinned age variable Gender: Male or Female Expenditures: Dollar amount of annual expenditures spent on each consumer Ethnicity: Eight ethnic groups (American Indian, Asian, Black, Hispanic, Multi-race, Native Hawaiian, Other, and White non-Hispanic)"
  },
  {
    "objectID": "hw/hw4-dds/data/california-dds-documentation.html#story-behind-the-data",
    "href": "hw/hw4-dds/data/california-dds-documentation.html#story-behind-the-data",
    "title": "Data documentation for california-dds.csv",
    "section": "",
    "text": "These data are from a dataset used in an alleged case of discrimination privileging White non-Hispanics over Hispanics in the allocation of funds. Based on the initial analysis, it would appear that discrimination existed; however, a more in-depth analysis revealed that discrimination did not exist and that Simpson’s-paradox phenomenon had occurred."
  },
  {
    "objectID": "hw/hw4-dds/data/california-dds-documentation.html#pedagogical-notes",
    "href": "hw/hw4-dds/data/california-dds-documentation.html#pedagogical-notes",
    "title": "Data documentation for california-dds.csv",
    "section": "",
    "text": "This data set can be used to teach a range of statistical concepts including Simpson’s paradox. The importance of considering all variables in an analysis by conducting a bivariate (instead of just a univariate analysis) is highlighted in this data set."
  },
  {
    "objectID": "hw/hw4-dds/data/california-dds-documentation.html#submitted-by",
    "href": "hw/hw4-dds/data/california-dds-documentation.html#submitted-by",
    "title": "Data documentation for california-dds.csv",
    "section": "",
    "text": "Name: Stanley Taylor and Amy Mickel Affiliation: California State University, Sacramento Address: College of Business Administration, CSUS, Sacramento, CA 95819-6088, USA Email: sataylor@csus.edu, mickela@csus.edu"
  },
  {
    "objectID": "projects/mp1/mp1-airquality-soln.html",
    "href": "projects/mp1/mp1-airquality-soln.html",
    "title": "Data preparation for student distribution",
    "section": "",
    "text": "# packages\nimport numpy as np\nimport pandas as pd\n\n# read in raw data file\nair_raw = pd.read_csv('air-raw.csv')\n\n# split off city info\ncbsa_info = air_raw.iloc[:, 0:2].dropna().set_index('CBSA')\ncbsa_info.to_csv('cbsa-info.csv')\n\n# remove city, state from air quality data\nair_quality = air_raw.drop(columns = 'Core Based Statistical Area').set_index('CBSA')\nair_quality.to_csv('air-quality.csv')"
  },
  {
    "objectID": "projects/mp1/mp1-airquality-soln.html#part-i-dataset",
    "href": "projects/mp1/mp1-airquality-soln.html#part-i-dataset",
    "title": "Data preparation for student distribution",
    "section": "Part I: Dataset",
    "text": "Part I: Dataset\nMerge the city information with the air quality data and tidy the dataset (see notes below). Write a brief description of the data.\nIn your description, answer the following questions:\n\nWhat is a CBSA (the geographic unit of measurement)?\nHow many CBSA’s are included in the data?\nIn how many states and territories do the CBSA’s reside?\nIn which years were data values recorded?\nHow many observations are recorded?\nHow many variables are measured?\nWhich variables are non-missing most of the time (i.e., in at least 50% of instances)?\nWhat is PM 2.5 and why is it important?\n\nPlease write your description in narrative fashion; please do not list answers to the questions above one by one. A few brief paragraphs should suffice; please limit your data description to three paragraphs or less.\n\nAir quality data\nWrite your description here."
  },
  {
    "objectID": "projects/mp1/mp1-airquality-soln.html#part-ii-descriptive-analysis",
    "href": "projects/mp1/mp1-airquality-soln.html#part-ii-descriptive-analysis",
    "title": "Data preparation for student distribution",
    "section": "Part II: Descriptive analysis",
    "text": "Part II: Descriptive analysis\nFocus on the PM2.5 measurements that are non-missing most of the time. Answer each of the following questions in a brief paragraph or two. Your paragraph(s) should indicate both your answer and a description of how you obtained it; please do not include codes with your answers.\n\nHas PM 2.5 air pollution improved in the average U.S. city since 2000?\nWrite your answer here.\n\n\nOver time, has PM 2.5 pollution become more variable, less variable, or about the same from city to city?\nWrite your answer here.\n\n\nWhich state has seen the greatest improvement over time?\nWrite your answer here.\n\n\nChoose a location with some meaning to you (e.g. hometown, family lives there, took a vacation there, etc.). Was that location in compliance with EPA primary standards as of the most recent measurement?\nWrite your answer here."
  },
  {
    "objectID": "projects/mp2/mp2-ncca.html",
    "href": "projects/mp2/mp2-ncca.html",
    "title": "Mini project 2: primary productivity in coastal waters",
    "section": "",
    "text": "In this project you’re again given a dataset and some questions. The data for this project come from the EPA’s National Aquatic Resource Surveys, and in particular the National Coastal Condition Assessment (NCCA); broadly, you’ll do an exploratory analysis of primary productivity in coastal waters.\nBy way of background, chlorophyll A is often used as a proxy for primary productivity in marine ecosystems; primary producers are important because they are at the base of the food web. Nitrogen and phosphorus are key nutrients that stimulate primary production.\nIn the data folder you’ll find water chemistry data, site information, and metadata files. It might be helpful to keep the metadata files open when tidying up the data for analysis. It might also be helpful to keep in mind that these datasets contain a considerable amount of information, not all of which is relevant to answering the questions of interest. Notice that the questions pertain somewhat narrowly to just a few variables. It’s recommended that you determine which variables might be useful and drop the rest.\nAs in the first mini project, there are accurate answers to each question that are mutually consistent with the data, but there aren’t uniquely correct answers. You will likely notice that you have even more latitude in this project than in the first, as the questions are slightly broader. Since we’ve been emphasizing visual and exploratory techniques in class, you are encouraged (but not required) to support your answers with graphics.\nThe broader goal of these mini projects is to cultivate your problem-solving ability in an unstructured setting. Your work will be evaluated based on the following: - approach used to answer questions; - clarity of presentation; - code style and documentation.\nPlease write up your results separately from your codes; codes should be included at the end of the notebook."
  },
  {
    "objectID": "projects/mp2/mp2-ncca.html#part-1-data-description",
    "href": "projects/mp2/mp2-ncca.html#part-1-data-description",
    "title": "Mini project 2: primary productivity in coastal waters",
    "section": "Part 1: data description",
    "text": "Part 1: data description\nMerge the site information with the chemistry data and tidy it up. Determine which columns to keep based on what you use in answering the questions in part 2; then, print the first few rows here (but do not include your codes used in tidying the data) and write a brief description (1-2 paragraphs) of the dataset conveying what you take to be the key attributes. You do not need to describe preprocessing steps. Direct your description to a reader unfamiliar with the data; ensure that in your data preview the columns are named intelligibly.\nSuggestion: export your cleaned data as a separate .csv file and read that directly in below, as in: pd.read_csv('YOUR DATA FILE').head().\n\n# show a few rows of clean data\n\nWrite your description here."
  },
  {
    "objectID": "projects/mp2/mp2-ncca.html#part-2-exploratory-analysis",
    "href": "projects/mp2/mp2-ncca.html#part-2-exploratory-analysis",
    "title": "Mini project 2: primary productivity in coastal waters",
    "section": "Part 2: exploratory analysis",
    "text": "Part 2: exploratory analysis\nAnswer each question below and provide a graphic or other quantitative evidence supporting your answer. A description and interpretation of the graphic/evidence should be offered.\n\n\nWhat is the apparent relationship between nutrient availability and productivity? Comment: it’s fine to examine each nutrient – nitrogen and phosphorus – separately, but do consider whether they might be related to each other.\n\n\nAre there any notable differences in available nutrients among U.S. coastal regions?\n\n\nBased on the 2010 data, does productivity seem to vary geographically in some way? If so, explain how; If not, explain what options you considered and why you ruled them out.\n\n\nHow does primary productivity in California coastal waters change seasonally in 2010, if at all? Does your result make intuitive sense?\n\n\nPose and answer one additional question.\n\n\nWrite up your answers here."
  },
  {
    "objectID": "projects/cp/project-guidelines.html",
    "href": "projects/cp/project-guidelines.html",
    "title": "Course project guidelines",
    "section": "",
    "text": "Your assignment for the course project is to formulate and answer a question of your choosing based on one of the following datasets:\nA good question is one that you want to answer. It should be a question with contextual meaning, not a purely technical matter. It should be clear enough to answer, but not so specific or narrow that your analysis is a single line of code. It should require you to do some nontrivial exploratory analysis, descriptive analysis, and possibly some statistical modeling. You aren’t required to use any specific methods, but it should take a bit of work to answer the question. There may be multiple answers or approaches to contrast based on different ways of interpreting the question or different ways of analyzing the data. If your question is answerable in under 15 minutes, or your answer only takes a few sentences to explain, the question probably isn’t nuanced enough."
  },
  {
    "objectID": "projects/cp/project-guidelines.html#deliverable",
    "href": "projects/cp/project-guidelines.html#deliverable",
    "title": "Course project guidelines",
    "section": "Deliverable",
    "text": "Deliverable\nPrepare and submit a jupyter notebook that summarizes your work. Your notebook should contain the following sections/contents:\n\nData description: write up a short summary of the dataset you chose to work with following the conventions introduced in previous assignments. Cover the sampling if applicable and data semantics, but focus on providing high-level context and not technical details; don’t report preprocessing steps or describe tabular layouts, etc.\nQuestion of interest: motivate and formulate your question; explain what a satisfactory answer might look like.\nData analysis: provide a walkthrough with commentary of the steps you took to investigate and answer the question. This section can and should include code cells and text cells, but you should try to focus on presenting the analysis clearly by organizing cells according to the high-level steps in your analysis so that it is easy to skim. For example, if you fit a regression model, include formulating the explanatory variable matrix and response, fitting the model, extracting coefficients, and perhaps even visualization all in one cell; don’t separate these into 5-6 substeps.\nSummary of findings: answer your question by interpreting the results of your analysis, referring back as appropriate. This can be a short paragraph or a bulleted list."
  },
  {
    "objectID": "projects/cp/project-guidelines.html#evaluation",
    "href": "projects/cp/project-guidelines.html#evaluation",
    "title": "Course project guidelines",
    "section": "Evaluation",
    "text": "Evaluation\nYour work will be evaluated on the following criteria:\n\nThoughtfulness: does your question reflect some thoughtful consideration of the dataset and its nuances, or is it more superficial?\nThoroughness: is your analysis an end-to-end exploration, or are there a lot of loose ends or unexplained choices?\nMistakes or oversights: is your work free from obvious errors or omissions, or are there mistakes and things you’ve overlooked?\nClarity of write-up: is your report well-organized with commented codes and clear writing, or does it require substantial effort to follow?"
  },
  {
    "objectID": "slides/week3-sampling.html#announcements",
    "href": "slides/week3-sampling.html#announcements",
    "title": "Sampling and missingness",
    "section": "Announcements",
    "text": "Announcements\n\nFirst mini project released: air quality in U.S. cities"
  },
  {
    "objectID": "slides/week3-sampling.html#this-week",
    "href": "slides/week3-sampling.html#this-week",
    "title": "Sampling and missingness",
    "section": "This week",
    "text": "This week\nObjective: Enable you to critically assess data quality based on how it was collected.\n\nSampling and statistical bias\n\nSampling terminology\nCommon sampling scenarios\nSampling mechanisms\nStatistical bias\n\nThe missing data problem\n\nTypes of missingness: MCAR, MAR, and MNAR\nPitfalls and simple fixes\n\nCase study: voter fraud\n\nSteven Miller’s analysis of Voter Integrity Fund surveys\nSources of bias\nEthical considerations"
  },
  {
    "objectID": "slides/week3-sampling.html#sampling-terminology",
    "href": "slides/week3-sampling.html#sampling-terminology",
    "title": "Sampling and missingness",
    "section": "Sampling terminology",
    "text": "Sampling terminology\nHere we’ll introduce standard statistical terminology to describe data collection.\n\nAll data are collected somehow. A sampling design is a way of selecting observational units for measurement. It can be construed as a particular relationship between:\n\na population (all entities of interest);\na sampling frame (all entities that are possible to measure); and\na sample (a specific collection of entities)."
  },
  {
    "objectID": "slides/week3-sampling.html#population",
    "href": "slides/week3-sampling.html#population",
    "title": "Sampling and missingness",
    "section": "Population",
    "text": "Population\nLast week, we introduced the terminology observational unit to mean the entity measured for a study – datasets consist of observations made on observational units.\n\nIn less technical terms, all data are data on some kind of thing, such as countries, species, locations, and the like.\n\n\n\n\nA statistical population is the collection of all units of interest. For example:\n\nall countries (GDP data)\nall mammal species (Allison 1976)\nall babies born in the US (babynames data)\nall locations in a region (SB weather data)\nall adult U.S. residents (BRFSS data)"
  },
  {
    "objectID": "slides/week3-sampling.html#sampling-frame",
    "href": "slides/week3-sampling.html#sampling-frame",
    "title": "Sampling and missingness",
    "section": "Sampling frame",
    "text": "Sampling frame\nThere are usually some units in a population that can’t be measured due to practical constraints – for instance, many adult U.S. residents don’t have phones or addresses.\n\n\n\nFor this reason, it is useful to introduce the concept of a sampling frame, which refers to the collection of all units in a population that can be observed for a study. For example:\n\nall countries reporting economic output between 1961 and 2019\nall babies with birth certificates from U.S. hospitals born between 1990 and 2018\nall adult U.S. residents with phone numbers in 2019"
  },
  {
    "objectID": "slides/week3-sampling.html#sample",
    "href": "slides/week3-sampling.html#sample",
    "title": "Sampling and missingness",
    "section": "Sample",
    "text": "Sample\nFinally, it’s rarely feasible to measure every observable unit due to limited data collection resources – for instance, states don’t have the time or money to call every phone number every year.\n\n\n\nA sample is a subcollection of units in the sampling frame actually selected for study. For instance:\n\n234 countries;\n62 mammal species;\n13,684,689 babies born in CA;\n1 weather station location at SB airport;\n418,268 adult U.S. residents."
  },
  {
    "objectID": "slides/week3-sampling.html#sampling-scenarios",
    "href": "slides/week3-sampling.html#sampling-scenarios",
    "title": "Sampling and missingness",
    "section": "Sampling scenarios",
    "text": "Sampling scenarios\nWe can now imagine a few common sampling scenarios by varying the relationship between population, frame, and sample.\n\nDenote an observational unit by \\(U_i\\), and let:\n\\[\\begin{alignat*}{2}\n\\mathcal{U} &= \\{U_i\\}_{i \\in I} &&\\quad(\\text{universe}) \\\\\nP &= \\{U_1, \\dots, U_N\\} \\subseteq \\mathcal{U} &&\\quad(\\text{population}) \\\\\n    F &= \\{U_j: j \\in J \\subset I\\} \\subseteq P &&\\quad(\\text{frame})\\\\\n    S &\\subseteq F &&\\quad(\\text{sample})\n\\end{alignat*}\\]"
  },
  {
    "objectID": "slides/week3-sampling.html#census",
    "href": "slides/week3-sampling.html#census",
    "title": "Sampling and missingness",
    "section": "Census",
    "text": "Census\nThe simplest scenario is a population census, where the entire population is observed.\n\n\n\nFor a census: \\(S = F = P\\)\nAll properties of the population are definitevely known in a census. So there is no need to model census data."
  },
  {
    "objectID": "slides/week3-sampling.html#simple-random-sample",
    "href": "slides/week3-sampling.html#simple-random-sample",
    "title": "Sampling and missingness",
    "section": "Simple random sample",
    "text": "Simple random sample\nThe statistical gold standard for inference, modeling, and prediction is the simple random sample in which units are selected at random from the population.\n\n\n\nFor a simple random sample: \\(S \\subset F = P\\)\nSample properties are reflective of population properties in simple random samples. Population inference is straightforward."
  },
  {
    "objectID": "slides/week3-sampling.html#typical-sample",
    "href": "slides/week3-sampling.html#typical-sample",
    "title": "Sampling and missingness",
    "section": "‘Typical’ sample",
    "text": "‘Typical’ sample\nMore common in practice is a random sample from a sampling frame that overlaps but does not cover the population.\n\n\n\nFor a ‘typical’ sample: \\(S \\subset F \\quad\\text{and}\\quad F \\cap P \\neq \\emptyset\\)\nSample properties are reflective of the frame but not necessarily the study population. Population inference gets more complicated and may not be possible."
  },
  {
    "objectID": "slides/week3-sampling.html#administrative-data",
    "href": "slides/week3-sampling.html#administrative-data",
    "title": "Sampling and missingness",
    "section": "‘Administrative’ data",
    "text": "‘Administrative’ data\nAlso common is administrative data in which all units are selected from a convenient frame that partly covers the population.\n\n\n\nFor administrative data: \\(S = F \\quad\\text{and}\\quad F\\cap P \\neq \\emptyset\\)\nAdministrative data are not really proper samples; they cannot be replicated and they do not represent any broader group. No inference is possible."
  },
  {
    "objectID": "slides/week3-sampling.html#scope-of-inference",
    "href": "slides/week3-sampling.html#scope-of-inference",
    "title": "Sampling and missingness",
    "section": "Scope of inference",
    "text": "Scope of inference\nThe relationships among the population, frame, and sample determine the scope of inference: the extent to which conclusions based on the sample are generalizable.\n\nA good sampling design can ensure that the statistical properties of the sample are expected to match those of the population. If so, it is sound to generalize:\n\nthe sample is said to be representative of the population\nthe scope of inference is broad\n\n\n\nA poor sampling design will produce samples that distort the statistical properties of the population. If so, it is not sound to generalize:\n\nsample statistics are subjet to bias\nthe scope of inference is narrow"
  },
  {
    "objectID": "slides/week3-sampling.html#characterizing-sampling-designs",
    "href": "slides/week3-sampling.html#characterizing-sampling-designs",
    "title": "Sampling and missingness",
    "section": "Characterizing sampling designs",
    "text": "Characterizing sampling designs\nThe sampling scenarios above can be differentiated along two key attributes:\n\nThe overlap between the sampling frame and the population.\n\nframe \\(=\\) population\nframe \\(\\subset\\) population\nframe \\(\\cap\\) population \\(\\neq \\emptyset\\)\n\nThe mechanism of obtaining a sample from the sampling frame.\n\nrandom sampling\nconvenience sampling\n\n\n\nIf you can articulate these two points, you have fully characterized the sampling design."
  },
  {
    "objectID": "slides/week3-sampling.html#sampling-mechanisms",
    "href": "slides/week3-sampling.html#sampling-mechanisms",
    "title": "Sampling and missingness",
    "section": "Sampling mechanisms",
    "text": "Sampling mechanisms\nIn order to describe sampling mechanisms precisely, we need a little terminology.\n\nEach unit has some inclusion probability – the probability of being included in the sample.\n\n\nLet’s suppose that the frame \\(F\\) comprises \\(N\\) units, and denote the inclusion probabilities by:\n\\[\np_i = P(\\text{unit } i \\text{ is included in the sample})\n\\quad i = 1, \\dots, N\n\\]\nThe inclusion probability of each unit depends on the physical procedure of collecting data."
  },
  {
    "objectID": "slides/week3-sampling.html#sampling-mechanisms-1",
    "href": "slides/week3-sampling.html#sampling-mechanisms-1",
    "title": "Sampling and missingness",
    "section": "Sampling mechanisms",
    "text": "Sampling mechanisms\nSampling mechanisms are methods of drawing samples and are categorized into four types based on inclusion probabilities.\n\nin a census every unit is included\n\n\\(p_i = 1\\) for every unit \\(i = 1, \\dots, N\\)\n\nin a random sample every unit is equally likely to be included\n\n\\(p_i = p_j\\) for every pair of units \\(i, j\\)\n\nin a probability sample units have different inclusion probabilities\n\n\\(p_i \\neq p_j\\) for at least one \\(i \\neq j\\)\n\nin a nonrandom sample there is no random mechanism\n\n\\(p_i = 1\\) for \\(i \\in S\\)"
  },
  {
    "objectID": "slides/week3-sampling.html#revisiting-example-datasets-gdp",
    "href": "slides/week3-sampling.html#revisiting-example-datasets-gdp",
    "title": "Sampling and missingness",
    "section": "Revisiting example datasets: GDP",
    "text": "Revisiting example datasets: GDP\nAnnual observations of GDP growth for 234 countries from 1961 - 2018.\n\nPopulation: all countries in existence between 1961-2019.\nFrame: all countries reporting economic output for at least one year between 1961 and 2019.\nSample: equal to frame.\n\n\nSo:\n\nOverlap: frame partly overlaps population.\nMechanism: sample is every country in the sampling frame.\n\n\n\nThis is administrative data with no scope of inference."
  },
  {
    "objectID": "slides/week3-sampling.html#revisiting-example-datasets-brfss-data",
    "href": "slides/week3-sampling.html#revisiting-example-datasets-brfss-data",
    "title": "Sampling and missingness",
    "section": "Revisiting example datasets: BRFSS data",
    "text": "Revisiting example datasets: BRFSS data\nPhone surveys of 418K U.S. residents in 2019.\n\nPopulation: all U.S. residents.\nFrame: all adult U.S. residents with phone numbers.\nSample: 418K adult U.S. residents with phone numbers.\n\n\nSo:\n\nOverlap: frame is a subset of the population.\nMechanism: probability sample.\n\nRandomly selected phone numbers were dialed in each state, so individuals in less populous states or with multiple numbers are more likely to be included\n\n\n\n\nThis is a typical sample with narrow inference to adult residents with phone numbers."
  },
  {
    "objectID": "slides/week3-sampling.html#statistical-bias",
    "href": "slides/week3-sampling.html#statistical-bias",
    "title": "Sampling and missingness",
    "section": "Statistical bias",
    "text": "Statistical bias\nStatistical bias is the average difference between a sample property and a population property across all possible samples under a particular sampling design.\n\nIn less technical terms: the expected error of estimates.\n\n\nTwo possible sources of statistical bias:\n\nAn estimator systematically over- or under-estimates its target population property\n\ne.g., \\(\\frac{1}{n}\\sum_i (x_i - \\bar{x})^2\\) is biased for (underestimates) the population variance\n\nSampling design systematically over- or under-represents certain observational units\n\ne.g., studies conducted on college campuses are biased towards (overrepresent) young adults\n\n\n\n\nThese are distinct from other kinds of bias that we are not discussing:\n\nMeasurement bias: attributes or outcomes are measured unevenly across populations\nExperimenter bias: study design and/or outcomes favor an investigator’s preconceptions"
  },
  {
    "objectID": "slides/week3-sampling.html#sampling-bias",
    "href": "slides/week3-sampling.html#sampling-bias",
    "title": "Sampling and missingness",
    "section": "Sampling bias",
    "text": "Sampling bias\nIn Lab 2 you’ll explore sampling bias arising from sampling mechanisms. Here’s a preview:\n\n\n\n\n\nDistributions of body length by sex (top) and in aggregate (bottom) for a hypothetical population of 5K hawks.\n\n\n\nConsider:\n\nAre males or females generally longer?\nHow will the sample mean shift if disproportionately more males are sampled?\nIf disproportionately more females are sampled?"
  },
  {
    "objectID": "slides/week3-sampling.html#bias-corrections",
    "href": "slides/week3-sampling.html#bias-corrections",
    "title": "Sampling and missingness",
    "section": "Bias corrections",
    "text": "Bias corrections\nIf inclusion probabilities are known or estimable it is possible to apply bias corrections to estimates using inverse probability weighting.\n\nIf\n\n\\(p_i\\) is the probability that individual \\(i\\) is included in the sample \\(S\\)\n\\(Y_i\\) are observations of a variable of interest\n\n\n\nThen a bias-corrected estimate of the population mean is given by the weighted average:\n\\[\n\\sum_{i\\in S} \\left(\\frac{p_i^{-1}}{\\sum_i p_i^{-1}}\\right) Y_i\n\\]"
  },
  {
    "objectID": "slides/week3-sampling.html#bias-correction-example",
    "href": "slides/week3-sampling.html#bias-correction-example",
    "title": "Sampling and missingness",
    "section": "Bias correction example",
    "text": "Bias correction example\nSuppose we obtain a biased sample in which female hawks were 6 times as likely to be selected as males. This yields an overestimate:\n\n\npopulation mean:  54.73771716352954\nsample mean:  56.567779534464016\n\n\n\nBut since we know the exact inclusion probabilities up to a proportionality constant, we can apply inverse probability weighting to adjust for bias:\n\n# specify weights s.t. 6:1 female:male\nweight_df = pd.DataFrame(\n    data = {'sex': np.array(['male', 'female']),\n            'weight': np.array([1, 6])})\n\n# append weights to sample\nsamp_w = pd.merge(samp, weight_df, how = 'left', on = 'sex')\n\n# calculate inverse probability weightings\nsamp_w['correction_factor'] = (1/samp_w.weight)/np.sum(1/samp_w.weight)\n\n# multiply observed values by weightings\nsamp_w['weighted_length'] = samp_w.length*samp_w.correction_factor\n\n# take weighted average\nsamp_w.weighted_length.sum()\n\n54.40928091743469"
  },
  {
    "objectID": "slides/week3-sampling.html#bias-correction-example-1",
    "href": "slides/week3-sampling.html#bias-correction-example-1",
    "title": "Sampling and missingness",
    "section": "Bias correction example",
    "text": "Bias correction example\nHowever, even if we didn’t know the exact inclusion probabilities, we could estimate them from the sample:\n\nsamp.sex.value_counts()\n\nfemale    88\nmale      12\nName: sex, dtype: int64\n\n\n\nAnd use the same approach:\n\n# estimate factor by which F more likely than M\nratio = samp.sex.value_counts().loc['female']/samp.sex.value_counts().loc['male']\n\n# input as weights\nweight_df = pd.DataFrame(data = {'sex': np.array(['male', 'female']), 'weight': np.array([1, ratio])})\n\n# append weights to sample\nsamp_w = pd.merge(samp, weight_df, how = 'left', on = 'sex')\n\n# calculate inverse probability weightings\nsamp_w['correction_factor'] = (1/samp_w.weight)/np.sum(1/samp_w.weight)\n\n# multiply observed values by weightings\nsamp_w['weighted_length'] = samp_w.length*samp_w.correction_factor\n\n# take weighted average\nsamp_w.weighted_length.sum()\n\n54.082235672430265"
  },
  {
    "objectID": "slides/week3-sampling.html#remarks-on-ipw-and-bias-correction",
    "href": "slides/week3-sampling.html#remarks-on-ipw-and-bias-correction",
    "title": "Sampling and missingness",
    "section": "Remarks on IPW and bias correction",
    "text": "Remarks on IPW and bias correction\nInverse probability weighting can be applied to correct a wide range of estimators besides averages.\n\nIt is also applicable to adjust for bias due to missing data.\n\n\nIn principle, the technique is simple, but in practice, there are some common hurdles:\n\nusually inclusion probabilities are not known\nestimating inclusion probabilities can be difficult and messy"
  },
  {
    "objectID": "slides/week3-sampling.html#missingness",
    "href": "slides/week3-sampling.html#missingness",
    "title": "Sampling and missingness",
    "section": "Missingness",
    "text": "Missingness\nMissing data arise when one or more variable measurements fail for a subset of observations.\n\nThis can happen for a variety of reasons, but is very common in pratice due to, for instance:\n\nequipment failure;\nsample contamination or loss;\nrespondents leaving questions blank;\nattrition (dropping out) of study participants.\n\n\n\nMany researchers and data scientists ignore missingness by simply deleting affected observations, but this is bad practice! Missingness needs to be treated carefully."
  },
  {
    "objectID": "slides/week3-sampling.html#missing-representations",
    "href": "slides/week3-sampling.html#missing-representations",
    "title": "Sampling and missingness",
    "section": "Missing representations",
    "text": "Missing representations\nIt is standard practice to record observations with missingness but enter a special symbol (.., -, NA, etcetera) for missing values.\n\nIn python, missing values are mapped to a special float:\n\n\n\nfloat('nan')\n\nnan"
  },
  {
    "objectID": "slides/week3-sampling.html#missing-representations-1",
    "href": "slides/week3-sampling.html#missing-representations-1",
    "title": "Sampling and missingness",
    "section": "Missing representations",
    "text": "Missing representations\nHere is some made-up data with two missing values:\n\n\n\n\n\n\n\n\n\n\nvalue\n\n\nobs\n\n\n\n\n\n0\n-0.9286936933427271\n\n\n1\n-0.3088381742999848\n\n\n2\n-\n\n\n3\n-1.4345064041945543\n\n\n4\n0.03958917896644836\n\n\n5\n-\n\n\n6\n-0.5316890502224456\n\n\n7\n1.4734842645335422"
  },
  {
    "objectID": "slides/week3-sampling.html#missing-representations-2",
    "href": "slides/week3-sampling.html#missing-representations-2",
    "title": "Sampling and missingness",
    "section": "Missing representations",
    "text": "Missing representations\nIf we read in the file with an na_values argument, pandas will parse the specified characters as NaN:\n\n\nsome_data = pd.read_csv('data/some_data.csv', index_col = 'obs', na_values = '-')\nsome_data\n\n\n\n\n\n\n\n\nvalue\n\n\nobs\n\n\n\n\n\n0\n-0.928694\n\n\n1\n-0.308838\n\n\n2\nNaN\n\n\n3\n-1.434506\n\n\n4\n0.039589\n\n\n5\nNaN\n\n\n6\n-0.531689\n\n\n7\n1.473484"
  },
  {
    "objectID": "slides/week3-sampling.html#calculations-with-nans",
    "href": "slides/week3-sampling.html#calculations-with-nans",
    "title": "Sampling and missingness",
    "section": "Calculations with NaNs",
    "text": "Calculations with NaNs\nNaNs halt calculations on numpy arrays.\n\n# mean in numpy -- halt\nsome_data.values.mean()\n\nnan\n\n\n\nHowever, the default behavior in pandas is to ignore the NaN’s, which allows the computation to proceed:\n\n\n\n# mean in pandas -- ignore\nsome_data.mean()\n\nvalue   -0.281776\ndtype: float64"
  },
  {
    "objectID": "slides/week3-sampling.html#omitting-missing-values-alters-results",
    "href": "slides/week3-sampling.html#omitting-missing-values-alters-results",
    "title": "Sampling and missingness",
    "section": "Omitting missing values alters results",
    "text": "Omitting missing values alters results\nBut those missing values could have been anything. For example:\n\n\n# one counterfactual scenario\ncomplete_data = some_data.copy()\ncomplete_data.loc[[2, 5], 'value'] = [5, 6] \n\n\n\nNow the mean is:\n\ncomplete_data.mean()\n\nvalue    1.163668\ndtype: float64\n\n\n\n\nSo missing values can dramatically alter results if they are simply omitted from calculations!"
  },
  {
    "objectID": "slides/week3-sampling.html#the-missing-data-problem",
    "href": "slides/week3-sampling.html#the-missing-data-problem",
    "title": "Sampling and missingness",
    "section": "The missing data problem",
    "text": "The missing data problem\nIn a nutshell, the missing data problem is: how should missing values be handled in a data analysis?\n\n\nGetting the software to run is one thing, but this alone does not address the challenges posed by the missing data. Unless the analyst, or the software vendor, provides some way to work around the missing values, the analysis cannot continue because calculations on missing values are not possible. There are many approaches to circumvent this problem. Each of these affects the end result in a different way. (Stef van Buuren, 2018)\n\n\n\nThere’s no universal approach to the missing data problem. The choice of method depends on:\n\nthe analysis objective;\nthe missing data mechanism."
  },
  {
    "objectID": "slides/week3-sampling.html#missing-data-in-pstat100",
    "href": "slides/week3-sampling.html#missing-data-in-pstat100",
    "title": "Sampling and missingness",
    "section": "Missing data in PSTAT100",
    "text": "Missing data in PSTAT100\nWe won’t go too far into this topic in PSTAT 100. Our goal will be awareness-raising, specifically:\n\ncharacterizing types of missingness (missing data mechanisms);\nunderstanding missingness as a potential source of bias;\nbasic do’s and don’t’s when it comes to missingness.\n\n\nIf you are interested in the topic, Stef van Buuren’s Flexible Imputation of Missing Data (the source of one of your readings this week) provides an excellent introduction."
  },
  {
    "objectID": "slides/week3-sampling.html#missing-data-mechanisms",
    "href": "slides/week3-sampling.html#missing-data-mechanisms",
    "title": "Sampling and missingness",
    "section": "Missing data mechanisms",
    "text": "Missing data mechanisms\nMissing data mechanisms (like sampling mechanisms) are characterized by the probabilities that observations go missing.\n\nFor dataset \\(X = \\{x_{ij}\\}\\) comprising\n\n\\(n\\) rows/observations\n\\(p\\) columns/variables\n\n\n\ndenote the probability that a value goes missing as:\n\\[\nq_{ij} = P(x_{ij} \\text{ is missing})\n\\]"
  },
  {
    "objectID": "slides/week3-sampling.html#missing-completely-at-random",
    "href": "slides/week3-sampling.html#missing-completely-at-random",
    "title": "Sampling and missingness",
    "section": "Missing completely at random",
    "text": "Missing completely at random\nData are missing completely at random (MCAR) if the probabilities of missing entries are uniformly equal.\n\n\\[\nq_{ij} = q\n\\quad\\text{for all}\\quad\ni = 1, \\dots, n\n\\quad\\text{and}\\quad\nj = 1, \\dots, p\n\\]\n\n\nThis implies that the cause of missingness is unrelated to the data: missing values can be ignored. This is the easiest scenario to handle."
  },
  {
    "objectID": "slides/week3-sampling.html#missing-at-random",
    "href": "slides/week3-sampling.html#missing-at-random",
    "title": "Sampling and missingness",
    "section": "Missing at random",
    "text": "Missing at random\nData are missing at random (MAR) if the probabilities of missing entries depend on observed data.\n\n\\[\nq_{ij} = f(\\mathbf{x}_i)\n\\]\n\n\nThis implies that information about the cause of missingness is captured within the dataset. As a result:\n\nit is possible to estimate \\(q_{ij}\\)\nbias corrections using inverse probability weighting can be implemented"
  },
  {
    "objectID": "slides/week3-sampling.html#missing-not-at-random",
    "href": "slides/week3-sampling.html#missing-not-at-random",
    "title": "Sampling and missingness",
    "section": "Missing not at random",
    "text": "Missing not at random\nData are missing not at random (MNAR) if the probabilities of missing entries depend on unobserved data.\n\n\\[\nq_{ij} = f(z_i, x_{ij}) \\quad z_i \\text{ unknown}\n\\]\n\n\nThis implies that information about the cause of missingness is unavailable. This is the most complicated scenario."
  },
  {
    "objectID": "slides/week3-sampling.html#assessing-the-missing-data-mechanism",
    "href": "slides/week3-sampling.html#assessing-the-missing-data-mechanism",
    "title": "Sampling and missingness",
    "section": "Assessing the missing data mechanism",
    "text": "Assessing the missing data mechanism\nImportantly, there is no easy diagnostic check to distinguish MCAR, MAR, and MNAR without measuring some of the missing data.\n\nSo in practice, usually one has to make an informed assumption based on knowledge of the data collection process."
  },
  {
    "objectID": "slides/week3-sampling.html#example-gdp-data",
    "href": "slides/week3-sampling.html#example-gdp-data",
    "title": "Sampling and missingness",
    "section": "Example: GDP data",
    "text": "Example: GDP data\nIn the GDP growth data, growth measurements are missing for many countries before a certain year.\n\nWe might be able to hypothesize about why – perhaps a country didn’t exist or didn’t keep reliable records for a period of time.However, the data as they are contain no additional information that might explain the cause of missingness.\n\n\nSo these data are MNAR."
  },
  {
    "objectID": "slides/week3-sampling.html#simple-fixes",
    "href": "slides/week3-sampling.html#simple-fixes",
    "title": "Sampling and missingness",
    "section": "Simple fixes",
    "text": "Simple fixes\nThe easiest approach to missing data is to drop observations with missing values: df.dropna().\n\nImplicitly assumes data are MCAR\nInduces bias if data are MAR or MNAR\n\n\nAnother simple fix is mean imputation, filling in missing values with the mean of the corresponding variable: df.fillna().\n\nOnly a good idea if a very small proportion of values are missing\nInduces bias if data are MAR or MNAR"
  },
  {
    "objectID": "slides/week3-sampling.html#perils-of-mean-imputation",
    "href": "slides/week3-sampling.html#perils-of-mean-imputation",
    "title": "Sampling and missingness",
    "section": "Perils of mean imputation",
    "text": "Perils of mean imputation\n\nImputing too many missing values distorts the distribution of sample values."
  },
  {
    "objectID": "slides/week3-sampling.html#other-common-approaches-to-missingness",
    "href": "slides/week3-sampling.html#other-common-approaches-to-missingness",
    "title": "Sampling and missingness",
    "section": "Other common approaches to missingness",
    "text": "Other common approaches to missingness\nWhen data are MCAR or MAR, one can:\n\nmodel the probability of missingness and apply bias corrections to estimated quantities using inverse probability weighting\nmodel the variables with missing observations as functions of the other variables and perform model-based imputation"
  },
  {
    "objectID": "slides/week3-sampling.html#dos-and-donts",
    "href": "slides/week3-sampling.html#dos-and-donts",
    "title": "Sampling and missingness",
    "section": "Do’s and don’t’s",
    "text": "Do’s and don’t’s\nDo:\n\nAlways check for missing values upon import.\n\nTabulate the proportion of observations with missingness\nTabulate the proportion of values for each variable that are missing\n\nTake time to find out the reasons data are missing.\n\nDetermine which outcomes are coded as missing.\nInvestigate the physical mechanisms involved.\n\nReport missing data if they are present.\n\nDon’t:\n\nRely on software defaults for handling missing values.\nDrop missing values if data are not MCAR."
  },
  {
    "objectID": "slides/week8-mlr.html#announcements",
    "href": "slides/week8-mlr.html#announcements",
    "title": "Multiple regression",
    "section": "Announcements",
    "text": "Announcements\nNo class Monday 5/29 due to Memorial Day.\n\nAssignments due Tuesday instead.\nLate deadline moved to Thursday.\nWednesday office hour cancelled on 5/31."
  },
  {
    "objectID": "slides/week8-mlr.html#loose-ends",
    "href": "slides/week8-mlr.html#loose-ends",
    "title": "Multiple regression",
    "section": "Loose ends",
    "text": "Loose ends\nThe standard measure of predictive accuracy in regression is mean square error (MSE):\n\\[\nMSE(y, \\hat{y}) = \\frac{1}{n}\\sum_i (y_i - \\hat{y}_i)^2\n\\]\n\nestimates expected squared error, \\(\\mathbb{E}(y - \\hat{y})^2\\)\nbiased (underestimate on average) if fitted values are used\nunbiased if new observations are used\n\n\nTo avoid bias, it is common practice to partition data into nonoverlapping subsets:\n\none used to fit the model (‘training’ partition)\nanother used to evaluate predictions (‘testing’ or ‘validation’ partition)"
  },
  {
    "objectID": "slides/week8-mlr.html#measuring-predictive-accuracy",
    "href": "slides/week8-mlr.html#measuring-predictive-accuracy",
    "title": "Multiple regression",
    "section": "Measuring predictive accuracy",
    "text": "Measuring predictive accuracy\nPartition the data:\n\n# hold out 100 randomly selected rows\nnp.random.seed(51823)\nidx = np.random.choice(regdata.index.values, size = 100, replace = False).tolist()\ntest = regdata.loc[idx]\ntrain = regdata.drop(index = idx)\n\n\nFit to the training partition:\n\n# fit model to training subset\nx_train = sm.tools.add_constant(train.log_income)\ny_train = train.gap\nslr = sm.OLS(endog = y_train, exog = x_train)\n\n\n\nEvaluate on the test/validation partition:\n\n# compute predictions\nx_test = sm.tools.add_constant(test.log_income)\npreds = slr.fit().get_prediction(x_test)\ny_hat = preds.predicted_mean\n\n# mean square error\npred_errors = test.gap - y_hat\nmse = (pred_errors**2).mean()\nprint('root mean square error: ', np.sqrt(mse))\n\nroot mean square error:  0.14377667235816582"
  },
  {
    "objectID": "slides/week8-mlr.html#interpreting-mse",
    "href": "slides/week8-mlr.html#interpreting-mse",
    "title": "Multiple regression",
    "section": "Interpreting MSE",
    "text": "Interpreting MSE\nInterpretation:\n\nThe model predictions vary about observed values with a standard deviation of 0.144 (SD of national average)."
  },
  {
    "objectID": "slides/week8-mlr.html#dont-use-training-mse",
    "href": "slides/week8-mlr.html#dont-use-training-mse",
    "title": "Multiple regression",
    "section": "(Don’t use) training MSE",
    "text": "(Don’t use) training MSE\nCompare with MSE computed using fitted values:\n\n\nCode\n# note, these are just model residuals\nfit_errors = train.gap - slr.fit().fittedvalues\n\n# training rmse\nprint('rmse on training partition: ',np.sqrt((fit_errors**2).mean()))\nprint('rmse on test partition: ', np.sqrt(mse))\n\n\nrmse on training partition:  0.10843963485562962\nrmse on test partition:  0.14377667235816582\n\n\n\ntraining MSE is overly ‘optimistic’ – smaller than the proper estimate\nwon’t always be the case, but will be an underestimate on average across samples\n\n\nNote also that this is simply the estimate of the error variance, rescaled by \\(\\frac{n - 2}{n}\\).\n\nn, p = train.shape\nnp.sqrt(slr.fit().scale*(n - 2)/n)\n\n0.10843963485562962\n\n\n\n\nSince the model is fit by minimizing this quantity, out-of-sample predictions are absolutely necessary to get a good sense of the predictive reliability."
  },
  {
    "objectID": "slides/week8-mlr.html#multiple-regression",
    "href": "slides/week8-mlr.html#multiple-regression",
    "title": "Multiple regression",
    "section": "Multiple regression",
    "text": "Multiple regression\nThe simple linear regression model has just one explanatory variable:\n\\[\n(\\text{SLR}) \\qquad\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\quad\\begin{cases} i = 1, \\dots, n \\\\\\epsilon_i \\sim N\\left(0,\\sigma^2\\right)\\end{cases}\n\\]\n\nThe multiple linear regression model is a direct extension of the simple linear model to \\(p - 1\\) variables \\(x_{i1}, \\dots, x_{i, p - 1}\\):\n\\[\n(\\text{MLR})\\qquad\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_{p - 1} x_{i, p - 1} + \\epsilon_i \\qquad\\begin{cases} \\epsilon_i \\sim N(0, \\sigma^2) \\\\ i = 1, \\dots, n\\end{cases}\n\\]\n\n\\(p\\) is the number of (mean) parameters\n\\(p = 2\\) is SLR\n\\(p &gt; 2\\) is MLR\nWhat’s \\(p = 1\\)??"
  },
  {
    "objectID": "slides/week8-mlr.html#the-model-in-matrix-form",
    "href": "slides/week8-mlr.html#the-model-in-matrix-form",
    "title": "Multiple regression",
    "section": "The model in matrix form",
    "text": "The model in matrix form\nThe model in matrix form is \\(\\mathbf{y} = \\mathbf{X}\\beta + \\epsilon\\), where:\n\\[\n\\mathbf{y}: \\left[\\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array}\\right] \\; = \\;\n    \\mathbf{X}: \\left[\\begin{array}{cccc}\n        1 &x_{11} &\\cdots &x_{1, p - 1} \\\\\n        1 &x_{21} &\\cdots &x_{2, p - 1} \\\\\n        \\vdots &\\vdots &\\ddots &\\vdots \\\\\n        1 &x_{n1} &\\cdots &x_{n, p - 1}\n        \\end{array}\\right] \\; \\times \\;\n    \\beta: \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{p - 1} \\end{array} \\right] \\; + \\;\n    \\epsilon: \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right]\n\\]\n\nCarrying out the arithmetic on the right-hand side:\n\\[\n\\left[\\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array}\\right]_{\\;n \\times 1} \\quad = \\quad\n    \\left[\\begin{array}{c}\n        \\beta_0 + \\beta_1 x_{11} + \\cdots + \\beta_{p - 1} x_{1, p - 1} + \\epsilon_1 \\\\\n        \\beta_0 + \\beta_1 x_{21} + \\cdots + \\beta_{p - 1} x_{2, p - 1} + \\epsilon_2 \\\\\n        \\vdots \\\\\n        \\beta_0 + \\beta_1 x_{n1} + \\cdots + \\beta_{p - 1} x_{n, p - 1} + \\epsilon_n\n        \\end{array}\\right]_{\\;n \\times 1}\n\\]\n\n\nThis is exactly the model relationship as written before, enumerated for each \\(i\\)."
  },
  {
    "objectID": "slides/week8-mlr.html#example",
    "href": "slides/week8-mlr.html#example",
    "title": "Multiple regression",
    "section": "Example",
    "text": "Example\nLet’s consider the model you fit in lab:\n\\[\n(\\text{fertility rate})_i = \\beta_0 + \\beta_1 (\\text{HDI})_i + \\beta_2 (\\text{education})_i + \\epsilon_i\n\\]\n\nx_vars = fertility.loc[:, ['educ_expected_yrs_f', 'hdi']]\nx = sm.tools.add_constant(x_vars)\ny = fertility.fertility_total\n\nmlr = sm.OLS(endog = y, exog = x)"
  },
  {
    "objectID": "slides/week8-mlr.html#model-estimation",
    "href": "slides/week8-mlr.html#model-estimation",
    "title": "Multiple regression",
    "section": "Model estimation",
    "text": "Model estimation\nEstimation and uncertainty quantification are exactly the same as in the simple linear model.\n\nThe least squares estimates are: \\[\n\\hat{\\beta} = \\left[\\begin{array}{c}\\hat{\\beta}_0 \\\\ \\vdots \\\\ \\hat{\\beta}_{p - 1} \\end{array}\\right] = \\left(\\mathbf{X'X}\\right)^{-1}\\mathbf{X'y}\n\\]\n\n# retrieve coefficient estimates\nmlr.fit().params\n\nconst                  7.960371\neduc_expected_yrs_f   -0.201996\nhdi                   -4.132623\ndtype: float64\n\n\n\n\nThis is the unique minimizer of the residual variance:\n\\[\n\\hat{\\beta} = \\text{argmin}_{\\beta \\in \\mathbb{R}^p}\\left\\{ (\\mathbf{y} - \\mathbf{X}\\beta)'(\\mathbf{y} - \\mathbf{X}\\beta)\\right\\}\n\\]"
  },
  {
    "objectID": "slides/week8-mlr.html#model-estimation-1",
    "href": "slides/week8-mlr.html#model-estimation-1",
    "title": "Multiple regression",
    "section": "Model estimation",
    "text": "Model estimation\nAn estimate of the error variance is: \\[\n\\hat{\\sigma}^2\n    = \\frac{1}{n - p} \\sum_{i = 1}^n \\underbrace{\\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_{i1} - \\cdots - \\hat{\\beta}_{p - 1}x_{i, p - 1}\\right)^2}_{i\\text{th squared model residual}}\n\\]\n\n# retrieve error variance estimate\nmlr.fit().scale\n\n0.34690893559915764\n\n\n\nThere are several alternative ways of writing the estimator \\(\\hat{\\sigma}^2\\).\n\n\\(\\frac{1}{n - p} \\sum_i (y_i - \\hat{y}_i)^2\\)\n\\(\\frac{1}{n - p} \\sum_i e_i^2\\)\n\\(\\frac{1}{n - p}\\left(\\mathbf{y} - \\mathbf{X}\\hat{\\beta}\\right)'\\left(\\mathbf{y} - \\mathbf{X}\\hat{\\beta}\\right)\\)\n\\(\\frac{RSS}{n - p}\\)"
  },
  {
    "objectID": "slides/week8-mlr.html#variability-of-estimates",
    "href": "slides/week8-mlr.html#variability-of-estimates",
    "title": "Multiple regression",
    "section": "Variability of estimates",
    "text": "Variability of estimates\nThe variances and covariances of the coefficient estimates are found in exactly the same way as before, only now yield a \\(p \\times p\\) (instead of \\(2\\times 2\\)) matrix:\n\\[\n\\mathbf{V}\n= \\left[\\begin{array}{cccc}\n    \\text{var}\\hat{\\beta}_0\n        &\\text{cov}\\left(\\hat{\\beta}_0, \\hat{\\beta}_1\\right)\n        &\\cdots &\\text{cov}\\left(\\hat{\\beta}_0, \\hat{\\beta}_{p - 1}\\right) \\\\\n    \\text{cov}\\left(\\hat{\\beta}_0, \\hat{\\beta}_1\\right)\n        &\\text{var}\\hat{\\beta}_1\n        &\\cdots &\\text{cov}\\left(\\hat{\\beta}_1, \\hat{\\beta}_{p - 1}\\right) \\\\\n    \\vdots &\\vdots &\\ddots &\\vdots \\\\\n    \\text{cov}\\left(\\hat{\\beta}_0, \\hat{\\beta}_{p - 1}\\right)\n        &\\text{cov}\\left(\\hat{\\beta}_1, \\hat{\\beta}_{p - 1}\\right)\n        &\\cdots\n        &\\text{var}\\hat{\\beta}_{p - 1}\n    \\end{array}\\right]\n   = \\sigma^2 \\left(\\mathbf{X'X}\\right)^{-1}\n\\]"
  },
  {
    "objectID": "slides/week8-mlr.html#variability-of-estimates-1",
    "href": "slides/week8-mlr.html#variability-of-estimates-1",
    "title": "Multiple regression",
    "section": "Variability of estimates",
    "text": "Variability of estimates\nThis matrix is again estimated by plugging in \\(\\color{red}{\\hat{\\sigma}^2}\\) (the estimate) for \\(\\sigma^2\\): \\[\\hat{\\mathbf{V}}\n    = \\left[\\begin{array}{cccc}\n        \\color{blue}{\\hat{v}_{11}} & \\hat{v}_{12} & \\cdots & \\hat{v}_{1p} \\\\\n        \\hat{v}_{21} & \\color{blue}{\\hat{v}_{22}} & \\cdots & \\hat{v}_{2p} \\\\\n        \\vdots &\\vdots &\\ddots &\\vdots \\\\\n        \\hat{v}_{p1} & \\hat{v}_{p2} &\\cdots &\\color{blue}{\\hat{v}_{pp}} \\\\\n        \\end{array}\\right]\n    = \\color{red}{\\hat{\\sigma}^2}\\left(\\mathbf{X'X}\\right)^{-1}\n\\]\n\n# retrieve parameter variance-covariance estimate\nmlr.fit().cov_params()\n\n\n\n\n\n\n\n\nconst\neduc_expected_yrs_f\nhdi\n\n\n\n\nconst\n0.059995\n-0.001839\n-0.050256\n\n\neduc_expected_yrs_f\n-0.001839\n0.001780\n-0.025240\n\n\nhdi\n-0.050256\n-0.025240\n0.462611"
  },
  {
    "objectID": "slides/week8-mlr.html#standard-errors",
    "href": "slides/week8-mlr.html#standard-errors",
    "title": "Multiple regression",
    "section": "Standard errors",
    "text": "Standard errors\nThe square roots of the diagonal elements give standard errors: \\[\n\\text{SE}(\\hat{\\beta}_0) = \\sqrt{\\color{blue}{\\hat{v}_{11}}}\n    \\;,\\quad\n    \\text{SE}(\\hat{\\beta}_1) = \\sqrt{\\color{blue}{\\hat{v}_{22}}}\n    \\;,\\quad \\cdots \\quad\n    \\text{SE}(\\hat{\\beta}_{p - 1}) = \\sqrt{\\color{blue}{\\hat{v}_{pp}}}\n\\]\n\nnp.sqrt(mlr.fit().cov_params().values.diagonal())\n\narray([0.24493942, 0.042194  , 0.68015545])"
  },
  {
    "objectID": "slides/week8-mlr.html#model-fit-summary",
    "href": "slides/week8-mlr.html#model-fit-summary",
    "title": "Multiple regression",
    "section": "Model fit summary",
    "text": "Model fit summary\nJust as before, a table is helpful:\n\n\nCode\nrslt = mlr.fit()\ncoef_tbl = pd.DataFrame({'estimate': rslt.params.values,\n              'standard error': np.sqrt(rslt.cov_params().values.diagonal())},\n              index = x.columns)\ncoef_tbl.loc['error variance', 'estimate'] = rslt.scale\n\ncoef_tbl\n\n\n\n\n\n\n\n\n\nestimate\nstandard error\n\n\n\n\nconst\n7.960371\n0.244939\n\n\neduc_expected_yrs_f\n-0.201996\n0.042194\n\n\nhdi\n-4.132623\n0.680155\n\n\nerror variance\n0.346909\nNaN"
  },
  {
    "objectID": "slides/week8-mlr.html#variance-explained",
    "href": "slides/week8-mlr.html#variance-explained",
    "title": "Multiple regression",
    "section": "Variance explained",
    "text": "Variance explained\nThe \\(R^2\\) statistic – proportion of variance explained – is computed the same as before: \\[\n\\frac{\\hat{\\sigma}_\\text{raw}^2 - \\frac{n - 1}{n - p}\\hat{\\sigma}^2}{\\hat{\\sigma}_\\text{raw}^2}\n\\]\n\nrslt.rsquared\n\n0.7827796420988886"
  },
  {
    "objectID": "slides/week8-mlr.html#interpretation",
    "href": "slides/week8-mlr.html#interpretation",
    "title": "Multiple regression",
    "section": "Interpretation",
    "text": "Interpretation\nWhile the computations are essentially unchanged, the presence of multiple predictors alters the interpretation of the parameters.\n\nIncrementing, say, HDI by one unit to get an estimated change assumes the other variable is held constant: \\[\n\\mathbb{E}(\\text{fertility}) + \\beta_2 = \\beta_0 + \\beta_1 (\\text{education}) + \\beta_2 \\left[(\\text{HDI}) + 1\\right]\n\\]\n\nif education also changes, then the estimated change in fertility is no longer \\(\\beta_2\\)\n\n\n\nSo now the interpretation is: a 0.1 increase in HDI is associated with an estimated decrease in fertility rate of 0.413, after accounting for education."
  },
  {
    "objectID": "slides/week8-mlr.html#inference",
    "href": "slides/week8-mlr.html#inference",
    "title": "Multiple regression",
    "section": "Inference",
    "text": "Inference\nConfidence intervals are the same as before. A 95% interval is:\n\\[\n\\hat{\\beta}_j \\pm 2 SE(\\hat{\\beta}_j)\n\\]\n\n\nmlr.fit().conf_int().rename(columns = {0: 'lwr', 1: 'upr'})\n\n\n\n\n\n\n\n\nlwr\nupr\n\n\n\n\nconst\n7.475988\n8.444753\n\n\neduc_expected_yrs_f\n-0.285437\n-0.118554\n\n\nhdi\n-5.477671\n-2.787574"
  },
  {
    "objectID": "slides/week8-mlr.html#prediction",
    "href": "slides/week8-mlr.html#prediction",
    "title": "Multiple regression",
    "section": "Prediction",
    "text": "Prediction\nDitto predictions.\n\n\\(\\hat{y} = \\mathbf{x}'\\hat{\\beta}\\)\n95% interval for the mean: \\(\\widehat{\\mathbb{E}y} \\pm 2 SE\\left(\\widehat{\\mathbb{E}y}\\right)\\)\n95% interval for a predicted observation: \\(\\hat{y} \\pm 2SE(\\hat{y})\\)\n\n\n\nx_new = np.array([1, 10, 0.5])\nmlr.fit().get_prediction(x_new).summary_frame()\n\n\n\n\n\n\n\n\nmean\nmean_se\nmean_ci_lower\nmean_ci_upper\nobs_ci_lower\nobs_ci_upper\n\n\n\n\n0\n3.874104\n0.119361\n3.63806\n4.110147\n2.685664\n5.062544"
  },
  {
    "objectID": "slides/week8-mlr.html#model-visualizations",
    "href": "slides/week8-mlr.html#model-visualizations",
    "title": "Multiple regression",
    "section": "Model visualizations",
    "text": "Model visualizations\nVisualization gets a bit trickier because of the presence of that second variable. Here I plotted the marginal relationship with HDI for three quantiles of education.\n\n\nCode\n# set coordinates for grid mesh\neduc_gridpts = fertility.educ_expected_yrs_f.quantile([0.2, 0.5, 0.8]).values\nhdi_gridpts = np.linspace(fertility.hdi.min(), fertility.hdi.max(), num = 100)\n\n# create prediction grid\ng1, g2 = np.meshgrid(educ_gridpts, hdi_gridpts)\ngrid = np.array([g1.ravel(), g2.ravel()]).T\ngrid_df = pd.DataFrame(grid, columns = ['educ', 'hdi'])\n\n# format for input to get_prediction()\nx_pred = sm.tools.add_constant(grid_df)\n\n# compute predictions\npred_df = mlr.fit().get_prediction(x_pred).summary_frame()\n\n# append to grid\npred_df = pd.concat([grid_df, pred_df], axis = 1)\n\n# plot\nscatter = alt.Chart(fertility).mark_circle(opacity = 0.5).encode(\n    x = alt.X('hdi', \n        scale = alt.Scale(zero = False),\n        title = 'Human development index'),\n    y = alt.Y('fertility_total',\n        title = 'Fertility rate'),\n    color = alt.Color('educ_expected_yrs_f',\n        title = 'Education')\n)\n\nmodel = alt.Chart(pred_df).mark_line().encode(\n    x = 'hdi',\n    y = 'mean',\n    color = 'educ'\n)\n\n(scatter + model).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n).configure_legend(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week8-mlr.html#urban-tree-cover-data",
    "href": "slides/week8-mlr.html#urban-tree-cover-data",
    "title": "Multiple regression",
    "section": "Urban tree cover data",
    "text": "Urban tree cover data\nTree canopy is thought to reduce summer temperatures in urban areas. Consider tree cover and a few other variables measured on a random sample of ~2K census blocks with some canpoy cover in San Diego:\n\n\nCode\ntrees.head(3)\n\n\n\n\n\n\n\n\n\ncensus_block_GEOID\ntree_cover\nmean_summer_temp\nmean_income\npop_density\n\n\n\n\n20504\n6.073020e+13\n25.798277\n33.859524\n66738\nvery low\n\n\n20849\n6.073000e+13\n11.439114\n32.150000\n63189\nvery low\n\n\n10621\n6.073020e+13\n26.661660\n32.404167\n35099\nhigh\n\n\n\n\n\n\n\n\n\nMcDonald RI, Biswas T, Sachar C, Housman I, Boucher TM, Balk D, et al. (2021) The tree cover and temperature disparity in US urbanized areas: Quantifying the association with income across 5,723 communities. PLoS ONE 16(4). doi:10.1371/journal.pone.0249715."
  },
  {
    "objectID": "slides/week8-mlr.html#simple-regression-analysis",
    "href": "slides/week8-mlr.html#simple-regression-analysis",
    "title": "Multiple regression",
    "section": "Simple regression analysis",
    "text": "Simple regression analysis\nIs higher tree cover associated with lower summer temperatures?\n\n\n\n\nCode\nalt.Chart(trees).mark_circle(opacity = 0.3).encode(\n    x = alt.X('tree_cover', \n        title = 'Tree canopy cover (percent)'),\n    y = alt.Y('mean_summer_temp', \n        scale = alt.Scale(zero = False),\n        title = 'Average summer temperature (C)')\n).configure_axis(\n    titleFontSize = 16,\n    labelFontSize = 14\n)\n\n\n\n\n\n\n\n\n\n\nweak negative association\nlinear approximation seems reasonable\nlots of census blocks with low (&lt;10%) cover"
  },
  {
    "objectID": "slides/week8-mlr.html#simple-regression-analysis-1",
    "href": "slides/week8-mlr.html#simple-regression-analysis-1",
    "title": "Multiple regression",
    "section": "Simple regression analysis",
    "text": "Simple regression analysis\nWe can use SLR to quantify the association:\n\n# explanatory and response variables\nx = sm.tools.add_constant(trees.tree_cover)\ny = trees.mean_summer_temp\n\n# fit temp ~ cover\nslr = sm.OLS(endog = y, exog = x)\nslr.fit().params\n\nconst         34.994336\ntree_cover    -0.073114\ndtype: float64\n\n\n\nEach 13.7% increase in tree canopy cover is associated with an estimated 1-degree decrease in mean summer surface temperatures.\n\n\n(How did I get 13.7%?)"
  },
  {
    "objectID": "slides/week8-mlr.html#simple-regression-analysis-2",
    "href": "slides/week8-mlr.html#simple-regression-analysis-2",
    "title": "Multiple regression",
    "section": "Simple regression analysis",
    "text": "Simple regression analysis\nConfidence intervals:\n\n\nCode\nslr.fit().conf_int().rename(columns = {0: 'lwr', 1: 'upr'})\n\n\n\n\n\n\n\n\n\nlwr\nupr\n\n\n\n\nconst\n34.876501\n35.112170\n\n\ntree_cover\n-0.083032\n-0.063195\n\n\n\n\n\n\n\n\n\nInterpretation: with 95% confidence, a 10% increase in tree cover is associated with an estimated decrease in mean summer surface temperatures between 0.6 and 0.8 degrees celsius."
  },
  {
    "objectID": "slides/week8-mlr.html#considering-more-variables",
    "href": "slides/week8-mlr.html#considering-more-variables",
    "title": "Multiple regression",
    "section": "Considering more variables",
    "text": "Considering more variables\nIs tree cover associated with affluence and population density?\n\n\n\n\nCode\nalt.Chart(trees).mark_circle(opacity = 0.3).encode(\n    x = alt.X('mean_income', \n        scale = alt.Scale(type = 'log'),\n        title = 'Mean income (USD)'),\n    y = alt.Y('tree_cover', \n        scale = alt.Scale(type = 'log'),\n        title = 'Tree cover (percent)')\n).configure_axis(\n    titleFontSize = 16,\n    labelFontSize = 14\n)\n\n\n\n\n\n\n\n\n\n\nmaybe with income, not clear about density\nto answer exactly, want to model tree_cover as a function of mean_income and pop_density"
  },
  {
    "objectID": "slides/week8-mlr.html#handling-categorical-variables",
    "href": "slides/week8-mlr.html#handling-categorical-variables",
    "title": "Multiple regression",
    "section": "Handling categorical variables",
    "text": "Handling categorical variables\nThe population density is recorded as a categorical variable:\n\n\nCode\nregdata = trees.loc[:, ['tree_cover', 'mean_income', 'pop_density']]\nregdata['log_cover'] = np.log(regdata.tree_cover)\nregdata['log_income'] = np.log(regdata.mean_income)\nregdata.drop(columns = {'tree_cover', 'mean_income'}, inplace = True)\nregdata.head(4)\n\n\n\n\n\n\n\n\n\npop_density\nlog_cover\nlog_income\n\n\n\n\n20504\nvery low\n3.250308\n11.108530\n\n\n20849\nvery low\n2.437039\n11.053886\n\n\n10621\nhigh\n3.283227\n10.465928\n\n\n9031\nlow\n2.311344\n10.309253\n\n\n\n\n\n\n\n\nOn face value, it might seem we could simply fit this model: \\[\n\\underbrace{\\log(\\text{cover}_i)}_{y_i}\n    = \\beta_0 +\n        \\beta_1 \\underbrace{\\log(\\text{income}_i)}_{x_{i1}} +\n        \\beta_2 \\underbrace{\\text{density}_i}_{x_{i2}} + \\epsilon_i\n    \\qquad i = 1, \\dots, 2000\n\\]\n\n\nBut this doesn’t quite make sense, because the values of \\(\\text{density}_i\\) would be words!"
  },
  {
    "objectID": "slides/week8-mlr.html#indicator-variable-encoding",
    "href": "slides/week8-mlr.html#indicator-variable-encoding",
    "title": "Multiple regression",
    "section": "Indicator variable encoding",
    "text": "Indicator variable encoding\nThe solution to this issue is to encode each level of the categorical variable separately using a set of indicator variables.\n\nFor instance, to indicate whether a census block is of low population density, we can use the indicator:\n\\[\nI(\\text{density $=$ low}) = \\begin{cases} 1 \\text{ if population density is low} \\\\ 0 \\text{ otherwise}\\end{cases}\n\\]\n\n\nWe can encode the levels of pop_density using a collection of indicators:\n\n\nCode\n# create indicator variables\ndensity_encoded = pd.get_dummies(regdata.pop_density, drop_first = True)\n\n# display example values corresponding to categorical variable\npd.concat([regdata[['pop_density']], density_encoded], axis = 1).groupby('pop_density').head(2)\n\n\n\n\n\n\n\n\n\npop_density\nlow\nmedium\nhigh\n\n\n\n\n20504\nvery low\n0\n0\n0\n\n\n20849\nvery low\n0\n0\n0\n\n\n10621\nhigh\n0\n0\n1\n\n\n9031\nlow\n1\n0\n0\n\n\n1004\nhigh\n0\n0\n1\n\n\n15939\nlow\n1\n0\n0\n\n\n6647\nmedium\n0\n1\n0\n\n\n17963\nmedium\n0\n1\n0"
  },
  {
    "objectID": "slides/week8-mlr.html#the-mlr-model-with-indicators",
    "href": "slides/week8-mlr.html#the-mlr-model-with-indicators",
    "title": "Multiple regression",
    "section": "The MLR model with indicators",
    "text": "The MLR model with indicators\nThe model with the encoded population density variable is:\n\\[\n\\underbrace{\\log(\\text{cover}_i)}_{y_i}\n    = \\beta_0 +\n        \\beta_1 \\underbrace{\\log(\\text{income}_i)}_{x_{i1}} +\n        \\beta_2 \\underbrace{\\text{low}_i}_{x_{i2}} +\n        \\beta_3 \\underbrace{\\text{med}_i}_{x_{i3}} +\n        \\beta_4 \\underbrace{\\text{high}_i}_{x_{i4}} +\n        \\epsilon_i\n\\]\n\nThe effect is that the model has different intercepts for each population density.\n\\[\n\\begin{align*}\n\\text{density $=$ very low}\n    &\\quad\\Longrightarrow\\quad\n    \\mathbb{E}\\log(\\text{cover}_i)\n        = \\underbrace{\\beta_0}_\\text{intercept} + \\beta_1\\log(\\text{income}_i) \\\\\n\\text{density $=$ low}\n    &\\quad\\Longrightarrow\\quad\n    \\mathbb{E}\\log(\\text{cover}_i)\n        = \\underbrace{\\beta_0 + \\beta_2}_\\text{intercept} + \\beta_1\\log(\\text{income}_i) \\\\\n\\text{density $=$ medium}\n    &\\quad\\Longrightarrow\\quad\n    \\mathbb{E}\\log(\\text{cover}_i)\n        = \\underbrace{\\beta_0 + \\beta_3}_\\text{intercept} + \\beta_1\\log(\\text{income}_i) \\\\\n\\text{density $=$ high}\n    &\\quad\\Longrightarrow\\quad\n    \\mathbb{E}\\log(\\text{cover}_i)\n        = \\underbrace{\\beta_0 + \\beta_4}_\\text{intercept} + \\beta_1\\log(\\text{income}_i)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/week8-mlr.html#in-matrix-form",
    "href": "slides/week8-mlr.html#in-matrix-form",
    "title": "Multiple regression",
    "section": "In matrix form",
    "text": "In matrix form\nThe explanatory variable matrix \\(\\mathbf{X}\\) for this model will be of the form:\n\\[\n\\mathbf{X} = \\left[\\begin{array}{c:cccc}\n    1 &\\log(\\text{income}_{1}) &\\text{low}_1 &\\text{med}_1 &\\text{high}_1 \\\\\n    1 &\\log(\\text{income}_{2}) &\\text{low}_2 &\\text{med}_2 &\\text{high}_2 \\\\\n    \\vdots &\\vdots &\\vdots &\\vdots &\\vdots \\\\\n    1 &\\log(\\text{income}_{2000}) &\\text{low}_{2000} &\\text{med}_{2000} &\\text{high}_{2000}\n    \\end{array}\\right]\n\\]\n\n\n\nCode\n# form explanatory variable matrix\nx_vars = pd.concat([regdata.log_income, density_encoded], axis = 1)\nx = sm.tools.add_constant(x_vars)\nx.head(4)\n\n\n\n\n\n\n\n\n\nconst\nlog_income\nlow\nmedium\nhigh\n\n\n\n\n20504\n1.0\n11.108530\n0\n0\n0\n\n\n20849\n1.0\n11.053886\n0\n0\n0\n\n\n10621\n1.0\n10.465928\n0\n0\n1\n\n\n9031\n1.0\n10.309253\n1\n0\n0"
  },
  {
    "objectID": "slides/week8-mlr.html#fit-summary",
    "href": "slides/week8-mlr.html#fit-summary",
    "title": "Multiple regression",
    "section": "Fit summary",
    "text": "Fit summary\nThe remaining calculations are all the same as before. Here is the model fit summary:\n\n\nCode\n# form explanatory variable matrix\nx_vars = pd.concat([regdata.log_income, density_encoded], axis = 1)\nx = sm.tools.add_constant(x_vars)\ny = regdata.log_cover\n\n# fit model\nmlr = sm.OLS(endog = y, exog = x)\nrslt = mlr.fit()\n\n# summarize fit\ncoef_tbl = pd.DataFrame({\n    'estimate': rslt.params,\n    'standard error': np.sqrt(rslt.cov_params().values.diagonal())\n}, index = x.columns.values)\ncoef_tbl.loc['error variance', 'estimate'] = rslt.scale\n\ncoef_tbl\n\n\n\n\n\n\n\n\n\nestimate\nstandard error\n\n\n\n\nconst\n-4.751870\n0.621673\n\n\nlog_income\n0.631469\n0.058290\n\n\nlow\n-0.414486\n0.070483\n\n\nmedium\n-0.675626\n0.079552\n\n\nhigh\n-0.606980\n0.101708\n\n\nerror variance\n1.552578\nNaN\n\n\n\n\n\n\n\n\nincome is positively associated with tree cover\nbut how do you interpret the coefficients for the indicator variables?"
  },
  {
    "objectID": "slides/week8-mlr.html#interpreting-indicator-coefficients",
    "href": "slides/week8-mlr.html#interpreting-indicator-coefficients",
    "title": "Multiple regression",
    "section": "Interpreting indicator coefficients",
    "text": "Interpreting indicator coefficients\nEach level of the categorical variable has its own intercept:\n\\[\n\\begin{align*}\n\\text{very low density:}\\quad &\\mathbb{E}\\log(\\text{cover}) = \\beta_0 + \\beta_1 \\log(\\text{income}) \\\\\n\\text{low density:}\\quad &\\mathbb{E}\\log(\\text{cover}) = (\\beta_0 + \\beta_2) + \\beta_1 \\log(\\text{income}) \\\\\n\\text{medium density:}\\quad &\\mathbb{E}\\log(\\text{cover}) = (\\beta_0 + \\beta_3) + \\beta_1 \\log(\\text{income}) \\\\\n\\text{high density:}\\quad &\\mathbb{E}\\log(\\text{cover}) = (\\beta_0 + \\beta_4) + \\beta_1 \\log(\\text{income})\n\\end{align*}\n\\]\n\n\\(\\beta_0\\) is the intercept when density is very low – this gets called the reference level\n\\(\\beta_2, \\beta_3, \\beta_4\\) are interpreted relative to the reference level:\n\n\\(\\beta_2\\) is the difference in expected log cover between low density and very low density blocks, after accounting for income\n\\(\\beta_3\\) is the difference in expected log cover between medium density and very low density blocks, after accounting for income\n\\(\\beta_4\\) is the difference in expected log cover between high density and very low density blocks, after accounting for income"
  },
  {
    "objectID": "slides/week8-mlr.html#interpreting-estimates",
    "href": "slides/week8-mlr.html#interpreting-estimates",
    "title": "Multiple regression",
    "section": "Interpreting estimates",
    "text": "Interpreting estimates\n\n\n\n\n\n\n\n\n\nestimate\nstandard error\n\n\n\n\nconst\n-4.751870\n0.621673\n\n\nlog_income\n0.631469\n0.058290\n\n\nlow\n-0.414486\n0.070483\n\n\nmedium\n-0.675626\n0.079552\n\n\nhigh\n-0.606980\n0.101708\n\n\nerror variance\n1.552578\nNaN\n\n\n\n\n\n\n\n\neach doubling of mean income is associated with an estimated 55% increase in median tree cover, after accounting for population density\ncensus blocks with higher population densities are estimated as having a median tree canopy up to 50% lower than census blocks with very low population densities, after accounting for mean income"
  },
  {
    "objectID": "slides/week8-mlr.html#prediction-1",
    "href": "slides/week8-mlr.html#prediction-1",
    "title": "Multiple regression",
    "section": "Prediction",
    "text": "Prediction\nPrediction works the same way, but we need to supply the indicator encoding rather than the categorical variable level.\n\nx_new = np.array([1, np.log(115000), 0, 1, 0])\npred = rslt.get_prediction(x_new)\nnp.exp(pred.summary_frame())\n\n\n\n\n\n\n\n\nmean\nmean_se\nmean_ci_lower\nmean_ci_upper\nobs_ci_lower\nobs_ci_upper\n\n\n\n\n0\n6.895105\n1.102304\n5.696153\n8.346419\n0.594349\n79.990893\n\n\n\n\n\n\n\nCheck your understanding and fill in the blanks:\n\nthe median tree cover for a _______ density census block with mean income _______ is estimated to be between _______ and _______ percent\nthe tree cover for a _______ density census block with mean income _______ is estimated to be between _______ and _______ percent"
  },
  {
    "objectID": "slides/week8-mlr.html#comments-on-scope-of-inference",
    "href": "slides/week8-mlr.html#comments-on-scope-of-inference",
    "title": "Multiple regression",
    "section": "Comments on scope of inference",
    "text": "Comments on scope of inference\nThe data in this case study are from a random sample of census blocks in the San Diego urban area.\n\nThey are therefore representative of all census blocks in the San Diego urban area.\n\n\n\nThe model approximates the actual associations between summer temperatures, tree cover, income, and population density in the region."
  },
  {
    "objectID": "slides/week3-casestudy.html#the-miller-case",
    "href": "slides/week3-casestudy.html#the-miller-case",
    "title": "Case study on sampling and missingness",
    "section": "The Miller case",
    "text": "The Miller case\nOn November 21, 2020, a professor at Williams College, Steven Miller, filed an affidavit alleging that an analysis of phone surveys showed that among registered republican voters in PA:\n\n~40K mail ballots were fraudlently requested;\n~48K mail ballots were not counted.\n\n\n\n“President Donald J. Trump amplified the statement in a tweet, the Chairman of the Federal Elections Commission (FEC) referenced the statement as indicative of fraud, and a conservative group prominently featured it in a legal brief seeking to overturn the Pennsylvania election results.” (Samuel Wolf, Williams Record, 11/25/20)\n\n\n\nThe Miller affidavit was criticized by statisticians as incorrect, irresponsible, and unethical.\n\n\nWe’ll focus on the first claim."
  },
  {
    "objectID": "slides/week3-casestudy.html#the-calculation",
    "href": "slides/week3-casestudy.html#the-calculation",
    "title": "Case study on sampling and missingness",
    "section": "The calculation",
    "text": "The calculation\nOn a purely mathematical level, the calculations were straightforward.\n\n\\(N = 165,412\\) mail ballots were requested by registered republicans but not returned.\nPhone surveys of \\(n = 2250\\) of those voters identified \\(556\\) who claimed not to request ballots.\n\n\n\\[\n\\text{sample proportion} \\times \\text{population size} = \\frac{556}{2250} \\times 165,412 = 40,875\n\\]"
  },
  {
    "objectID": "slides/week3-casestudy.html#the-flawed-assumption",
    "href": "slides/week3-casestudy.html#the-flawed-assumption",
    "title": "Case study on sampling and missingness",
    "section": "The flawed assumption",
    "text": "The flawed assumption\nThe key issue was a single flawed assumption:\n\n\n“The analysis is predicated on the assumption that the responders are a representative sample of the population of registered Republicans in Pennsylvania for whom a mail-in ballot was requested but not counted, and responded accurately to the questions during the phone calls.”” (Miller affidavit)\n\n\n\nEssentially, two critical mistakes were made in the analysis:\n\nFailure to critically assess the sampling design and scope of inference.\nIgnoring missing data.\n\n\n\nMiller is a number theorist, not a trained survey statistician, so on some level his mistakes were understandable, but they did a lot of damage. He issued an apology in short order."
  },
  {
    "objectID": "slides/week3-casestudy.html#the-survey",
    "href": "slides/week3-casestudy.html#the-survey",
    "title": "Case study on sampling and missingness",
    "section": "The survey",
    "text": "The survey\n\nThere were 165,412 unreturned mail ballots requested by registered republicans in PA.\n\n\nThose voters were surveyed by phone by Matt Braynard’s private firm External Affairs on behalf of the Voter Integrity Fund.\n\n\nWe don’t really know how they obtained and selected phone numbers or exactly what the survey procedure was, but here’s what we do know:\n\n~23K individuals were called on Nov. 9-10.\nThe ~2.5K who answered were asked if they were the registered voter or a family member.\nIf they said yes, they were asked if they requested a ballot.\nThose who requested a ballot were asked if they mailed it."
  },
  {
    "objectID": "slides/week3-casestudy.html#potential-sources-of-bias",
    "href": "slides/week3-casestudy.html#potential-sources-of-bias",
    "title": "Case study on sampling and missingness",
    "section": "Potential sources of bias",
    "text": "Potential sources of bias\nThere are several obvious sampling problems.\n\nUndisclosed selection mechanism\nNarrow snapshot in time; 9th and 10th were a Monday and Tuesday, less likely to reach workers.\nUnknown at this time whether mail ballots were ultimately returned and counted or not by this time, so the frame doesn’t align with the population.\n\n\nThere are also some obvious measurement problems.\n\nFamily members could answer on behalf of one another and may give incorrect answers.\nRespondents may not be aware that they requested a ballot, as you don’t have to file an explicit request in Pennsylvania (there’s a checkbox on the voter registration form).\nVoters who believed they did not request a ballot were not asked if they recieved and/or returned one."
  },
  {
    "objectID": "slides/week3-casestudy.html#survey-schematic",
    "href": "slides/week3-casestudy.html#survey-schematic",
    "title": "Case study on sampling and missingness",
    "section": "Survey schematic",
    "text": "Survey schematic"
  },
  {
    "objectID": "slides/week3-casestudy.html#sampling-design",
    "href": "slides/week3-casestudy.html#sampling-design",
    "title": "Case study on sampling and missingness",
    "section": "Sampling design",
    "text": "Sampling design\nPopulation: republicans registered to vote in PA who had unreturned mail ballots issued\n\nSampling frame: unknown; source of phone numbers unspecified.\n\n\nSample: 2684 registered republicans or family members of registered repbulicans who had a mail ballot officially requested in PA and answered survey calls on Nov. 9 or 10.\n\n\nSampling mechanism: nonrandom; depends on availability during calling hours on Monday and Tuesday, language spoken, and willingness to talk.\n\n\nThis is not a representative sample of any meaningful population."
  },
  {
    "objectID": "slides/week3-casestudy.html#missingness",
    "href": "slides/week3-casestudy.html#missingness",
    "title": "Case study on sampling and missingness",
    "section": "Missingness",
    "text": "Missingness\nRespondents hung up at every stage of the survey. This is not at random – individuals who do not believe there were any irregularities in mail ballots are less likely to talk or continue talking.\n\nSo data are MNAR and over-represent people more likely to claim they never requested a ballot."
  },
  {
    "objectID": "slides/week3-casestudy.html#the-analysis",
    "href": "slides/week3-casestudy.html#the-analysis",
    "title": "Case study on sampling and missingness",
    "section": "The analysis",
    "text": "The analysis\nMiller first calculated the proportion of respondents who reported not requesting ballots among those who did not hang up after the first question.\n\\[\n\\left(\\frac{556}{1150 + 556 + 544}\\right) = 0.2471\n\\]\n\nThen he extrapolated that the estimated number of fraudulent requests was:\n\\[\n0.2471 \\times 165,412 = 40,875\n\\]\n\n\nThe two main problems with this are:\n\nnonrandom sampling \\(\\Longrightarrow\\) no scope of inference\nno adjustment for nonresponse (i.e., missing data)"
  },
  {
    "objectID": "slides/week3-casestudy.html#simulation",
    "href": "slides/week3-casestudy.html#simulation",
    "title": "Case study on sampling and missingness",
    "section": "Simulation",
    "text": "Simulation\nIt’s not too tricky to envision sources of bias that would affect the results.\n\nAssume that:\n\nrespondents all know whether they actually requested a ballot\nrespondents tell the truth\nrespondents who didn’t request a ballot are more likely to be reached\nrespondents who did request a ballot are more likely to hang up during the interview\n\n\n\nThen we can show through a simple simulation that an actual fraud rate of under 1% will be estimated at over 20% almost all the time."
  },
  {
    "objectID": "slides/week3-casestudy.html#simulated-population",
    "href": "slides/week3-casestudy.html#simulated-population",
    "title": "Case study on sampling and missingness",
    "section": "Simulated population",
    "text": "Simulated population\nFirst let’s generate a population of 150K voters.\n\n\nnp.random.seed(41021)\n\n# proportion of fraudlent requests\ntrue_prop = 0.009\n\n# generate population of voters who had unreturned mail ballots\nN = 150000\npopulation = pd.DataFrame(data = {'requested': np.ones(N)})\n\n# how many didn't request mail ballots?\nnum_nrequest = round(N*true_prop) - 1\n\n# set the 'requested' indicator to zero for the top chunk of data\npopulation.iloc[0:num_nrequest, 0] = 0"
  },
  {
    "objectID": "slides/week3-casestudy.html#simulated-sample",
    "href": "slides/week3-casestudy.html#simulated-sample",
    "title": "Case study on sampling and missingness",
    "section": "Simulated sample",
    "text": "Simulated sample\nThen let’s introduce sampling weights based on the conditional probability that an individual will talk with the interviewer given whether they requested a ballot or not.\n\n\n# assume respondents tell the truth\np_request = 1 - true_prop\np_nrequest = true_prop\n\n# non-requesters are more likely to talk by this factor\ntalk_factor = 15\n\n# observed response rate\np_talk = 0.09\n\n# conditional response rates\np_talk_request = p_talk/(p_request + talk_factor*p_nrequest) \np_talk_nrequest = talk_factor*p_talk_request\n\n# append conditional response rates as weights\npopulation.loc[population.requested == 1, 'sample_weight'] = p_talk_request\npopulation.loc[population.requested == 0, 'sample_weight'] = p_talk_nrequest\n\n# draw weighted sample\nnp.random.seed(41923)\nsamp_complete = population.sample(n = 2500, replace = False, weights = 'sample_weight')\n\n\n\nThink of the weights as conditional response rates."
  },
  {
    "objectID": "slides/week3-casestudy.html#simulated-missing-mechanism",
    "href": "slides/week3-casestudy.html#simulated-missing-mechanism",
    "title": "Case study on sampling and missingness",
    "section": "Simulated missing mechanism",
    "text": "Simulated missing mechanism\nThen let’s introduce missing values at different rates for respondents who requested a ballot and respondents who didn’t.\n\n\n# requesters are more likely to hang up by this factor\nmissing_factor = 10\n\n# overall nonresponse rate\np_missing = 0.6\n\n# conditional probabilities of missing given request status\np_missing_nrequest = p_missing/(p_nrequest + missing_factor*p_request) \np_missing_request = missing_factor*p_missing_nrequest\n\n# append missingness weights to sample\nsamp_complete.loc[samp_complete.requested == 1, 'missing_weight'] = p_missing_request\nsamp_complete.loc[samp_complete.requested == 0, 'missing_weight'] = p_missing_nrequest\n\n# make a copy of the sample\nsamp_incomplete = samp_complete.copy()\n\n# input missing values at random\nnp.random.seed(41923)\nsamp_incomplete['missing'] = np.random.binomial(n = 1, p = samp_incomplete.missing_weight.values)\nsamp_incomplete.loc[samp_incomplete.missing == 1, 'requested'] = float('nan')"
  },
  {
    "objectID": "slides/week3-casestudy.html#simulated-result",
    "href": "slides/week3-casestudy.html#simulated-result",
    "title": "Case study on sampling and missingness",
    "section": "Simulated result",
    "text": "Simulated result\nIf we then drop all the missing values and calculate the proportion of respondents who didn’t request a ballot, we get:\n\n\n# compute mean after dropping missing values\n1 - samp_incomplete.requested.mean()\n\n0.2395470383275261\n\n\n\n\nSo Miller’s result is expected if the sampling and missing mechanisms introduce bias, even if the true rate of fraudulent requests is under 1% – on the order of 1,000 ballots."
  },
  {
    "objectID": "slides/week3-casestudy.html#in-class-activity",
    "href": "slides/week3-casestudy.html#in-class-activity",
    "title": "Case study on sampling and missingness",
    "section": "In class activity",
    "text": "In class activity\nOpen the notebook and try it for yourself.\n\nSome comments:\n\nI chose these settings specifically to replicate the \\(24\\%\\) estimate.\nI inflated the nonresponse rate relative to the survey to achieve this.\nThe simulation is not an exact model of the actual survey; the survey has other sources of bias.\nYou should try inputting settings that you think are realistic. You’ll still see significant bias."
  },
  {
    "objectID": "slides/week3-casestudy.html#professional-ethics",
    "href": "slides/week3-casestudy.html#professional-ethics",
    "title": "Case study on sampling and missingness",
    "section": "Professional ethics",
    "text": "Professional ethics\nThe American Statistical Association publishes ethical guidelines for statistical practice. The Miller case violated a large number of these, most prominently, that an ethical practitioner:\n\nReports the sources and assessed adequacy of the data, accounts for all data considered in a study, and explains the sample(s) actually used.\nIn publications and reports, conveys the findings in ways that are both honest and meaningful to the user/reader. This includes tables, models, and graphics.\nIn publications or testimony, identifies the ultimate financial sponsor of the study, the stated purpose, and the intended use of the study results.\nWhen reporting analyses of volunteer data or other data that may not be representative of a defined population, includes appropriate disclaimers and, if used, appropriate weighting."
  },
  {
    "objectID": "slides/week7-mlr.html",
    "href": "slides/week7-mlr.html",
    "title": "Data Science Concepts and Analysis",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport altair as alt\nimport sklearn.linear_model as lm\nfrom sklearn.preprocessing import add_dummy_feature\nfrom sklearn.metrics import r2_score\nalt.data_transformers.disable_max_rows()\n\nDataTransformerRegistry.enable('default')"
  },
  {
    "objectID": "slides/week7-mlr.html#this-week-multiple-linear-regression",
    "href": "slides/week7-mlr.html#this-week-multiple-linear-regression",
    "title": "Data Science Concepts and Analysis",
    "section": "This week: multiple linear regression",
    "text": "This week: multiple linear regression\nObjective: extend the simple linear model to multiple explanatory variables.\n\nReview of the simple linear model\n\nThe simple linear model in matrix form\nEstimation and uncertainty quantification\nParameter interpretation\n\nMultiple linear regression\n\nThe linear model in matrix form\nEstimation and uncertainty quantification\n\nCase study: urban tree cover\n\nBackground\nModel 1: summer temperatures, tree cover, and income\n\nmodel fitting calculations, step by step\nmodel visualization\ninterpretation of results\n\nModel 2: adding population density\n\ncategorical variable encodings\nresults and interpretation"
  },
  {
    "objectID": "slides/week7-mlr.html#review-of-the-simple-linear-model",
    "href": "slides/week7-mlr.html#review-of-the-simple-linear-model",
    "title": "Data Science Concepts and Analysis",
    "section": "Review of the simple linear model",
    "text": "Review of the simple linear model\n\nThe simple linear model in matrix form\nEstimation and uncertainty quantification\nParameter interpretation"
  },
  {
    "objectID": "slides/week7-mlr.html#the-simple-linear-model",
    "href": "slides/week7-mlr.html#the-simple-linear-model",
    "title": "Data Science Concepts and Analysis",
    "section": "The simple linear model",
    "text": "The simple linear model\nThe simple linear model describes a quantitative variable \\(y\\) as a linear function of another variable \\(x\\) and a random error for \\(n\\) observations:\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\quad\\begin{cases} i = 1, \\dots, n \\\\\\epsilon_i \\sim N\\left(0,\\sigma^2\\right)\\end{cases}\\]\n\n\\(y_i\\) is the response variable\n\\(x_i\\) is the explanatory variable\n\\(\\epsilon_i\\) is the error\n\\(\\beta_0, \\beta_1, \\sigma^2\\) are the model parameters\n\n\\(\\beta_0\\) is the intercept\n\\(\\beta_1\\) is the coefficient\n\\(\\sigma^2\\) is the error variance"
  },
  {
    "objectID": "slides/week7-mlr.html#estimation",
    "href": "slides/week7-mlr.html#estimation",
    "title": "Data Science Concepts and Analysis",
    "section": "Estimation",
    "text": "Estimation\nThe parameters \\(\\beta_0, \\beta_1\\) are estimated by ordinary least squares (OLS), which are best under many conditions.\nThe OLS estimates have a closed form:\n\\[\\hat{\\beta} = \\left[\\begin{array}{c}\\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\end{array}\\right] = \\left(\\mathbf{X'X}\\right)^{-1}\\mathbf{X'y}\\]\nAn estimate of the error variance is:\n\\[\\hat{\\sigma}^2\n    = \\frac{1}{n - 2} \\sum_{i = 1}^n \\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i\\right)^2\n    = \\frac{1}{n - 2}\\underbrace{\\left(\\mathbf{y} - \\mathbf{X}\\hat{\\beta}\\right)'\\left(\\mathbf{y} - \\mathbf{X}\\hat{\\beta}\\right)}_\\text{measure of residual variation}\\]"
  },
  {
    "objectID": "slides/week7-mlr.html#uncertainty-quantification",
    "href": "slides/week7-mlr.html#uncertainty-quantification",
    "title": "Data Science Concepts and Analysis",
    "section": "Uncertainty quantification",
    "text": "Uncertainty quantification\nModel uncertainty can be thought of in terms of the variation in parameter estimates: estimates that vary a lot from sample to sample are less certain.\nThe variances and covariances of the estimates have a closed form:\n\\[\\mathbf{V} = \\left[\\begin{array}{cc}\n    \\text{var}\\hat{\\beta}_0 & \\text{cov}\\left(\\hat{\\beta}_0, \\hat{\\beta}_1\\right) \\\\\n    \\text{cov}\\left(\\hat{\\beta}_0, \\hat{\\beta}_1\\right) &\\text{var}\\hat{\\beta}_1\n    \\end{array}\\right]\n   = \\sigma^2 \\left(\\mathbf{X'X}\\right)^{-1}\\]\nThis matrix is estimated by plugging in \\(\\color{red}{\\hat{\\sigma}^2}\\) (the estimate) for \\(\\sigma^2\\):\n\\[\\hat{\\mathbf{V}}\n    = \\left[\\begin{array}{cc}\n        \\color{blue}{\\hat{v}_{11}} & \\hat{v}_{12} \\\\\n        \\hat{v}_{21} & \\color{blue}{\\hat{v}_{22}}\n        \\end{array}\\right]\n    = \\color{red}{\\hat{\\sigma}^2}\\left(\\mathbf{X'X}\\right)^{-1}\\]\nThe square roots of the diagonal elements give estimated standard deviations, which are known as standard errors:\n\\[\\text{SE}(\\hat{\\beta}_0) = \\sqrt{\\color{blue}{\\hat{v}_{11}}}\n    \\qquad\\text{and}\\qquad\n    \\text{SE}(\\hat{\\beta}_1) = \\sqrt{\\color{blue}{\\hat{v}_{22}}}\\]\nFor about 95% of samples, estimates will vary within about 2 standard errors."
  },
  {
    "objectID": "slides/week7-mlr.html#parameter-interpretations",
    "href": "slides/week7-mlr.html#parameter-interpretations",
    "title": "Data Science Concepts and Analysis",
    "section": "Parameter interpretations",
    "text": "Parameter interpretations\nParameter interpretations are based on the observation that under the model:\n\\[\\mathbb{E}y_i\n    = \\mathbb{E}\\left(\\beta_0 + \\beta_1 x_i + \\epsilon_i\\right)\n    = \\beta_0 + \\beta_1 x_i + \\mathbb{E}\\left(\\epsilon_i\\right)\n    = \\beta_0 + \\beta_1 x_i\\]\n\n(Intercept) [at \\(x_i = 0\\)] the mean [response variable] is estimated to be [\\(\\hat{\\beta}_0\\) units].\n\n\\[x_i = 0 \\quad\\Longrightarrow\\quad \\mathbb{E}y_i = \\beta_0 + \\beta_1 (0) = \\beta_0\\]\n\n(Slope) Every [one-unit increase in \\(x_i\\)] is associated with an estimated change in mean [response variable] of [\\(\\hat{\\beta}_1\\) units].\n\n\\[\\text{increment $x$} \\quad\\Longrightarrow\\quad \\beta_0 + \\beta_1 (x_i + 1) = \\beta_0 + \\beta_1 x_i + \\beta_1 = \\mathbb{E}y_i + \\beta_1\\]\n\n# import grade-aggregated seda data from hw2\nseda = pd.read_csv('data/seda.csv')\n\n# filter to math and remove NaNs\nregdata = seda[seda.subject == 'math'].dropna()\nregdata.head()\n\n# save explanatory variable and response variable separately as arrays\nx = add_dummy_feature(regdata[['log_income']], value = 1)\ny = regdata.gap.values\n\n# configure regression module\nslr = lm.LinearRegression(fit_intercept = False)\n\n# fit slr model\nslr.fit(x, y)\n\n# fitted values\nregdata['fitted'] = slr.predict(x)\n\n# residuals\nresid = y - regdata.fitted\n\n# residual variance\nn, p = x.shape\nsigmasqhat = (n - 1)*resid.var()/(n - p)\n\n# coefficient variances/covariances\ncoef_vcov = np.linalg.inv(x.transpose().dot(x))*(sigmasqhat)\n\n# coefficient standard errors\ncoef_se = np.sqrt(coef_vcov.diagonal())\n\n# summary table\ncoef_summary = pd.DataFrame(\n    {'estimate': slr.coef_,\n     'standard error': coef_se},\n    index = ['intercept', 'log median income']\n)\n\n# fitted std errors\nregdata['fit_se'] = np.sqrt(x.dot(np.linalg.inv(x.transpose().dot(x))).dot(x.transpose()).diagonal()*sigmasqhat)\n\n# simple scatterplot of math gap vs district income\nbase = alt.Chart(regdata).encode(\n    x = alt.X('log_income', scale = alt.Scale(zero = False))\n)\n\n# plot\nscatter = base.mark_point().encode(y = 'gap')\nline = base.mark_line(color = 'red').encode(y = 'fitted')\nband = base.transform_calculate(\n    lwr = 'datum.fitted - 2*datum.fit_se',\n    upr = 'datum.fitted + 2*datum.fit_se'\n).mark_errorband(color = 'grey').encode(y = 'lwr:Q', y2 = 'upr:Q')"
  },
  {
    "objectID": "slides/week7-mlr.html#example-from-last-time",
    "href": "slides/week7-mlr.html#example-from-last-time",
    "title": "Data Science Concepts and Analysis",
    "section": "Example from last time",
    "text": "Example from last time\nLast time we fit a simple linear model to the SEDA math gap data. ‘Fitting’ the model means computing the estimates.\n\n(scatter + line + band).properties(width = 300, height = 200)\n\n\n\n\n\n\n\ncoef_summary\n\n\n\n\n\n\n\n\nestimate\nstandard error\n\n\n\n\nintercept\n-1.356170\n0.130697\n\n\nlog median income\n0.121057\n0.011843"
  },
  {
    "objectID": "slides/week7-mlr.html#multiple-linear-regression",
    "href": "slides/week7-mlr.html#multiple-linear-regression",
    "title": "Data Science Concepts and Analysis",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nThe linear model in matrix form\nEstimation and uncertainty quantification"
  },
  {
    "objectID": "slides/week7-mlr.html#extending-the-simple-linear-model",
    "href": "slides/week7-mlr.html#extending-the-simple-linear-model",
    "title": "Data Science Concepts and Analysis",
    "section": "Extending the simple linear model",
    "text": "Extending the simple linear model\nThe simple linear model was:\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\quad\\begin{cases} i = 1, \\dots, n \\\\\\epsilon_i \\sim N\\left(0,\\sigma^2\\right)\\end{cases}\n    \\qquad\\text{(simple linear model)}\\]\nIt’s called ‘simple’ because it only has a single explanatory variable \\(x_i\\).\nThe linear model is a direct extension of the simple linear model to \\(p - 1\\) variables \\(x_{i1}, \\dots, x_{i, p - 1}\\):\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_{p - 1} x_{i, p - 1} + \\epsilon_i \\qquad\\begin{cases} \\epsilon_i \\sim N(0, \\sigma^2) \\\\ i = 1, \\dots, n\\end{cases}\n    \\qquad\\text{(linear model)}\\]\nOther names: this is sometimes also called the multiple regression model or multiple linear model."
  },
  {
    "objectID": "slides/week7-mlr.html#the-linear-model-in-matrix-form",
    "href": "slides/week7-mlr.html#the-linear-model-in-matrix-form",
    "title": "Data Science Concepts and Analysis",
    "section": "The linear model in matrix form",
    "text": "The linear model in matrix form\nThe linear model is often written observation-wise in indexed form as above: \\(i\\) indexes the observations. However, it’s much more concise in matrix form:\n\\[\\mathbf{y} = \\mathbf{X}\\beta + \\epsilon\\]\nThis is shorthand for:\n\\[\\mathbf{y}: \\left[\\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array}\\right] \\; = \\;\n    \\mathbf{X}: \\left[\\begin{array}{cccc}\n        1 &x_{11} &\\cdots &x_{1, p - 1} \\\\\n        1 &x_{21} &\\cdots &x_{2, p - 1} \\\\\n        \\vdots &\\vdots &\\ddots &\\vdots \\\\\n        1 &x_{n1} &\\cdots &x_{n, p - 1}\n        \\end{array}\\right] \\; \\times \\;\n    \\beta: \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{p - 1} \\end{array} \\right] \\; + \\;\n    \\epsilon: \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right]\\]\nCarrying out the arithmetic on the right-hand side:\n\\[\\left[\\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array}\\right]_{\\;n \\times 1} \\quad = \\quad\n    \\left[\\begin{array}{c}\n        \\beta_0 + \\beta_1 x_{11} + \\cdots + \\beta_{p - 1} x_{1, p - 1} + \\epsilon_1 \\\\\n        \\beta_0 + \\beta_1 x_{21} + \\cdots + \\beta_{p - 1} x_{2, p - 1} + \\epsilon_2 \\\\\n        \\vdots \\\\\n        \\beta_0 + \\beta_1 x_{n1} + \\cdots + \\beta_{p - 1} x_{n, p - 1} + \\epsilon_n\n        \\end{array}\\right]_{\\;n \\times 1}\\]\nThis is exactly the model relationship, written for each \\(i\\)."
  },
  {
    "objectID": "slides/week7-mlr.html#good-news",
    "href": "slides/week7-mlr.html#good-news",
    "title": "Data Science Concepts and Analysis",
    "section": "Good news!",
    "text": "Good news!\nEstimation and uncertainty quantification are exactly the same as in the simple linear model.\nThe OLS estimates are:\n\\[\\hat{\\beta} = \\left[\\begin{array}{c}\\hat{\\beta}_0 \\\\ \\vdots \\\\ \\hat{\\beta}_{p - 1} \\end{array}\\right] = \\left(\\mathbf{X'X}\\right)^{-1}\\mathbf{X'y}\\]\nAn estimate of the error variance is:\n\\[\\hat{\\sigma}^2\n    = \\frac{1}{n - p} \\sum_{i = 1}^n \\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_{i1} - \\cdots - \\hat{\\beta}_{p - 1}x_{i, p - 1}\\right)^2\n    = \\frac{1}{n - p}\\underbrace{\\left(\\mathbf{y} - \\mathbf{X}\\hat{\\beta}\\right)'\\left(\\mathbf{y} - \\mathbf{X}\\hat{\\beta}\\right)}_\\text{measure of residual variation}\\]\nThe simple linear model was the special case with \\(p = 2\\)."
  },
  {
    "objectID": "slides/week7-mlr.html#uncertainty-quantification-1",
    "href": "slides/week7-mlr.html#uncertainty-quantification-1",
    "title": "Data Science Concepts and Analysis",
    "section": "Uncertainty quantification",
    "text": "Uncertainty quantification\nIn the case of the multiple linear model, the variances and covariances are a \\(p \\times p\\) (instead of \\(2\\times 2\\)) matrix:\n\\[\\mathbf{V} = \\left[\\begin{array}{cccc}\n    \\text{var}\\hat{\\beta}_0\n        &\\text{cov}\\left(\\hat{\\beta}_0, \\hat{\\beta}_1\\right)\n        &\\cdots &\\text{cov}\\left(\\hat{\\beta}_0, \\hat{\\beta}_{p - 1}\\right) \\\\\n    \\text{cov}\\left(\\hat{\\beta}_0, \\hat{\\beta}_1\\right)\n        &\\text{var}\\hat{\\beta}_1\n        &\\cdots &\\text{cov}\\left(\\hat{\\beta}_1, \\hat{\\beta}_{p - 1}\\right) \\\\\n    \\vdots &\\vdots &\\ddots &\\vdots \\\\\n    \\text{cov}\\left(\\hat{\\beta}_0, \\hat{\\beta}_{p - 1}\\right)\n        &\\text{cov}\\left(\\hat{\\beta}_1, \\hat{\\beta}_{p - 1}\\right)\n        &\\cdots\n        &\\text{var}\\hat{\\beta}_{p - 1}\n    \\end{array}\\right]\n   = \\sigma^2 \\left(\\mathbf{X'X}\\right)^{-1}\\]\nThis matrix is again estimated by plugging in \\(\\color{red}{\\hat{\\sigma}^2}\\) (the estimate) for \\(\\sigma^2\\):\n\\[\\hat{\\mathbf{V}}\n    = \\left[\\begin{array}{cccc}\n        \\color{blue}{\\hat{v}_{11}} & \\hat{v}_{12} & \\cdots & \\hat{v}_{1p} \\\\\n        \\hat{v}_{21} & \\color{blue}{\\hat{v}_{22}} & \\cdots & \\hat{v}_{2p} \\\\\n        \\vdots &\\vdots &\\ddots &\\vdots \\\\\n        \\hat{v}_{p1} & \\hat{v}_{p2} &\\cdots &\\color{blue}{\\hat{v}_{pp}} \\\\\n        \\end{array}\\right]\n    = \\color{red}{\\hat{\\sigma}^2}\\left(\\mathbf{X'X}\\right)^{-1}\\]\nThe square roots of the diagonal elements give standard errors:\n\\[\\text{SE}(\\hat{\\beta}_0) = \\sqrt{\\color{blue}{\\hat{v}_{11}}}\n    \\;,\\quad\n    \\text{SE}(\\hat{\\beta}_1) = \\sqrt{\\color{blue}{\\hat{v}_{22}}}\n    \\;,\\quad \\cdots \\quad\n    \\text{SE}(\\hat{\\beta}_{p - 1}) = \\sqrt{\\color{blue}{\\hat{v}_{pp}}}\\]"
  },
  {
    "objectID": "slides/week7-mlr.html#even-better-news",
    "href": "slides/week7-mlr.html#even-better-news",
    "title": "Data Science Concepts and Analysis",
    "section": "Even better news!",
    "text": "Even better news!\nAll the computations in sklearn are exactly the same.\nSo let’s have a look at some examples."
  },
  {
    "objectID": "slides/week7-mlr.html#case-study-urban-tree-cover",
    "href": "slides/week7-mlr.html#case-study-urban-tree-cover",
    "title": "Data Science Concepts and Analysis",
    "section": "Case study: urban tree cover",
    "text": "Case study: urban tree cover\n\nBackground\nModel 1: summer temperatures, tree cover, and income\n\nmodel fitting calculations, step by step\nmodel visualization\ninterpretation of results\n\nModel 2: adding population density\n\ncategorical variable encodings\nresults and interpretation"
  },
  {
    "objectID": "slides/week7-mlr.html#urban-tree-cover-data",
    "href": "slides/week7-mlr.html#urban-tree-cover-data",
    "title": "Data Science Concepts and Analysis",
    "section": "Urban tree cover data",
    "text": "Urban tree cover data\nThe following are data on urban tree cover in the San Diego area:\n\nnp.random.seed(51421)\ntrees = pd.read_csv('data/utc-sd.csv').sample(n = 2000).rename(\n    columns = {'fc_in_BG_g': 'tree_cover', 'IncPr': 'mean_income', 'SurfaceTem': 'mean_summer_temp'}\n)\n\ntrees['income_level'] = trees.IncomeGrp.astype('category').cat.rename_categories({1: 'very low', 2: 'low', 3: 'medium', 4: 'high'})\ntrees['pop_density'] = trees.PopDensGrp.astype('category').cat.rename_categories({1: 'very low', 2: 'low', 3: 'medium', 4: 'high'})\n\ntrees = trees.drop(columns = ['TreeTarget', 'IncomeGrp', 'PopDensGrp'])\n\ntrees = trees[trees.mean_summer_temp &gt; 0]\n\n\ntrees.head(4)\n\n\n\n\n\n\n\n\nName\ncensus_block_GEOID\ntree_cover\nmean_summer_temp\nmean_income\nincome_level\npop_density\n\n\n\n\n11013\nSan Diego, CA\n6.073010e+13\n0.158259\n31.986364\n40951\nmedium\nhigh\n\n\n18997\nSan Diego, CA\n6.073020e+13\n0.013488\n34.068851\n51502\nhigh\nvery low\n\n\n8786\nSan Diego, CA\n6.073010e+13\n0.052844\n37.098611\n28454\nlow\nlow\n\n\n11163\nSan Diego, CA\n6.073000e+13\n0.217973\n33.213636\n40229\nmedium\nmedium\n\n\n\n\n\n\n\nSource: McDonald RI, Biswas T, Sachar C, Housman I, Boucher TM, Balk D, et al. (2021) The tree cover and temperature disparity in US urbanized areas: Quantifying the association with income across 5,723 communities. PLoS ONE 16(4): e0249715. doi:10.1371/journal.pone.0249715."
  },
  {
    "objectID": "slides/week7-mlr.html#observational-units",
    "href": "slides/week7-mlr.html#observational-units",
    "title": "Data Science Concepts and Analysis",
    "section": "Observational units",
    "text": "Observational units\nThe observational units are census blocks.\nCensus blocks are pretty small, about the size of a city block. To get an idea of the geographic scale, here’s a map of census tracts in San Francisco. Each tract contains several census blocks:\n\nThe data comprise observations on a random sample of 1,998 census blocks in San Diego."
  },
  {
    "objectID": "slides/week7-mlr.html#tree-cover-and-summer-temperatures",
    "href": "slides/week7-mlr.html#tree-cover-and-summer-temperatures",
    "title": "Data Science Concepts and Analysis",
    "section": "Tree cover and summer temperatures",
    "text": "Tree cover and summer temperatures\nTree canopy mitigates temperature in urban areas.\n\n\n# source code\nfig1 = alt.Chart(trees).mark_circle(opacity = 0.6).encode(\n    x = alt.X('tree_cover', scale = alt.Scale(type = 'pow', exponent = 1/2, zero = False)),\n    y = alt.Y('mean_summer_temp', scale = alt.Scale(zero = False)),\n    color = alt.Color('mean_income', scale = alt.Scale(scheme = 'redblue', reverse = True, type = 'log'))\n)\n\n# fig1.properties(width = 500, height = 350)\n\nFor this reason, urban forestry projects have been advocated as a strategy for mitigating the effects of climate change."
  },
  {
    "objectID": "slides/week7-mlr.html#modeling-objectives",
    "href": "slides/week7-mlr.html#modeling-objectives",
    "title": "Data Science Concepts and Analysis",
    "section": "Modeling objectives",
    "text": "Modeling objectives\nHere we’ll try to quantify the association between temperature, tree cover, income, and population density using multiple regression.\nLet’s start with a MLR model with just two explanatory variables, tree cover and income:\n\\[\\underbrace{\\text{temp}_i}_{y_i}\n    = \\beta_0 + \\beta_1 \\underbrace{\\text{cover}_i}_{x_{i1}} + \\beta_2 \\underbrace{\\text{income}_i}_{x_{i2}} + \\epsilon_i\n    \\qquad i = 1, \\dots, 1998\\]\n\ny = trees.mean_summer_temp.values\nx_df = trees[['tree_cover', 'mean_income']]\nx_df.head(3)\n\n\n\n\n\n\n\n\ntree_cover\nmean_income\n\n\n\n\n11013\n0.158259\n40951\n\n\n18997\n0.013488\n51502\n\n\n8786\n0.052844\n28454"
  },
  {
    "objectID": "slides/week7-mlr.html#explanatory-variable-matrix",
    "href": "slides/week7-mlr.html#explanatory-variable-matrix",
    "title": "Data Science Concepts and Analysis",
    "section": "Explanatory variable matrix",
    "text": "Explanatory variable matrix\nWe’ll need to create an explanatory variable matrix of the form:\n\\[\\mathbf{X} = \\left[\\begin{array}{ccc}\n    1 &\\text{cover}_1 &\\text{income}_1 \\\\\n    1 &\\text{cover}_2 &\\text{income}_2 \\\\\n    \\vdots &\\vdots &\\vdots \\\\\n    1 &\\text{cover}_{1998} &\\text{income}_{1998}\n    \\end{array}\\right]\\]\nSo we just need to add a column of ones to x_df:\n\n# add column of ones (for intercept)\nx_mx = add_dummy_feature(x_df, value = 1)\n\n# preview\nx_mx[0:3]\n\narray([[1.00000000e+00, 1.58258747e-01, 4.09510000e+04],\n       [1.00000000e+00, 1.34878570e-02, 5.15020000e+04],\n       [1.00000000e+00, 5.28437900e-02, 2.84540000e+04]])"
  },
  {
    "objectID": "slides/week7-mlr.html#model-fitting",
    "href": "slides/week7-mlr.html#model-fitting",
    "title": "Data Science Concepts and Analysis",
    "section": "Model fitting",
    "text": "Model fitting\nSince we’ve added an intercept column, the fitting module should be configured not to fit an intercept separately.\n\n# fit first model\nmlr = lm.LinearRegression(fit_intercept = False)\nmlr.fit(x_mx, y)\nmlr.coef_\n\narray([ 3.65559763e+01, -3.14810456e+00, -5.02075353e-05])"
  },
  {
    "objectID": "slides/week7-mlr.html#error-variance-estimate",
    "href": "slides/week7-mlr.html#error-variance-estimate",
    "title": "Data Science Concepts and Analysis",
    "section": "Error variance estimate",
    "text": "Error variance estimate\nNext we’ll calclulate \\(\\hat{\\sigma}^2\\). For this we need the fitted values:\n\\[\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\beta}\\]\nAnd residuals:\n\\[\\mathbf{e} = \\left(\\mathbf{y} - \\mathbf{X}\\hat{\\beta}\\right) = \\mathbf{y} - \\hat{\\mathbf{y}}\\]\n\n# fitted values and residuals\nfitted = mlr.predict(x_mx)\nresid = y - fitted\n\nThen the error variance estimate is:\n\\[\\hat{\\sigma}^2\n= \\frac{1}{n - p}\\left(\\mathbf{y} - \\mathbf{X}\\hat{\\beta}\\right)'\\left(\\mathbf{y} - \\mathbf{X}\\hat{\\beta}\\right)\n= \\frac{1}{n - p}\\mathbf{e'e}\n= \\frac{n - 1}{n - p}S^2_e\\]\n\n# error variance estimate\nn, p = x_mx.shape\nsigmasqhat = resid.var()*(n - 1)/(n - p)\nsigmasqhat\n\n2.729420783462529"
  },
  {
    "objectID": "slides/week7-mlr.html#uncertainty-quantification-2",
    "href": "slides/week7-mlr.html#uncertainty-quantification-2",
    "title": "Data Science Concepts and Analysis",
    "section": "Uncertainty quantification",
    "text": "Uncertainty quantification\nLastly, we’ll compute the coefficient estimate standard errors:\n\\[\\sqrt{\\hat{v}_{jj}} \\quad\\text{from}\\quad \\hat{\\mathbf{V}} = \\hat{\\sigma}^2\\left(\\mathbf{X'X}\\right)^{-1}\\]\n\nxtx = x_mx.transpose().dot(x_mx) # X'X\nxtxinv = np.linalg.inv(xtx) # (X'X)^{-1}\nvhat = sigmasqhat*xtxinv # V\nvhat\n\narray([[ 6.72240283e-03, -6.63125605e-03, -1.25915361e-07],\n       [-6.63125605e-03,  2.23049324e-01, -2.70769667e-07],\n       [-1.25915361e-07, -2.70769667e-07,  3.80729809e-12]])\n\n\n\ncoef_se = np.sqrt(vhat.diagonal()) # (v_jj)^{1/2}\ncoef_se\n\narray([8.19902606e-02, 4.72280980e-01, 1.95122989e-06])"
  },
  {
    "objectID": "slides/week7-mlr.html#quality-of-fit-r2",
    "href": "slides/week7-mlr.html#quality-of-fit-r2",
    "title": "Data Science Concepts and Analysis",
    "section": "Quality of fit: \\(R^2\\)",
    "text": "Quality of fit: \\(R^2\\)\nThere are many metrics that measure fit quality. The most common is the proportion of variation explained by the model, which is denoted by \\(R^2\\):\n\\[R^2\n    = \\frac{\\text{reduction in variation}}{\\text{total variation}}\n    = \\frac{\\mathbf{\\tilde{y}'\\tilde{y}} - \\mathbf{e'e}}{\\mathbf{\\tilde{y}'\\tilde{y}}}\n    \\qquad \\text{where} \\qquad\n    \\mathbf{\\tilde{y}} = \\mathbf{y} - \\bar{\\mathbf{y}}\\]\n\n# 'by hand'\ny_ctr = y - y.mean()\n(y_ctr.transpose().dot(y_ctr) - resid.transpose().dot(resid))/(y_ctr.transpose().dot(y_ctr))\n\n0.30684982283103096\n\n\n\n# using sklearn.metrics.r2_score\nr2_score(y, fitted)\n\n0.3068498228310309\n\n\nInterpretation: 30% of variation in mean summer temperature is explained by tree cover and income."
  },
  {
    "objectID": "slides/week7-mlr.html#reporting-model-fit",
    "href": "slides/week7-mlr.html#reporting-model-fit",
    "title": "Data Science Concepts and Analysis",
    "section": "Reporting model fit",
    "text": "Reporting model fit\nNow we’ve computed relevant quantities – estimates and standard errors – but we have yet to report these in an organized fashion.\nTypically, these are displayed in a table.\n\nmlr_summary = pd.DataFrame(\n    {'estimate': np.append(mlr.coef_, sigmasqhat), \n     'standard error': np.append(coef_se, float('nan'))},\n    index = ['interept', 'cover', 'income', 'error variance']\n)\n\nmlr_summary\n\n\n\n\n\n\n\n\nestimate\nstandard error\n\n\n\n\ninterept\n36.555976\n0.081990\n\n\ncover\n-3.148105\n0.472281\n\n\nincome\n-0.000050\n0.000002\n\n\nerror variance\n2.729421\nNaN\n\n\n\n\n\n\n\nAlong with \\(R^2\\):\n\nr2_score(y, fitted)\n\n0.3068498228310309"
  },
  {
    "objectID": "slides/week7-mlr.html#visualization",
    "href": "slides/week7-mlr.html#visualization",
    "title": "Data Science Concepts and Analysis",
    "section": "Visualization",
    "text": "Visualization\nHere are trend lines and error bands for three values of income (10th, 50th, and 90th percentiles).\n\nThe error bands represent the variation of the lines – how much they might change if we sampled different census blocks. (Not the variation of temperatures.)\n\n# make univariate grids\ntree_grid = np.linspace(trees.tree_cover.min(), trees.tree_cover.max(), 100)\nincome_grid = trees.mean_income.quantile([0.1, 0.5, 0.9]).values\n\n# make mesh grid -- all combinations of univariate grid values\ntx, ix = np.meshgrid(tree_grid, income_grid)\nx_grid_mx = np.vstack([np.repeat(1, 300), tx.reshape(300), ix.reshape(300)]).transpose()\ngrid_df = pd.DataFrame(x_grid_mx, columns = ['intercept', 'tree_cover', 'mean_income'])\n\n# add predictions and standard errors\ngrid_df['mean_summer_temp'] = mlr.predict(x_grid_mx)\ngrid_df['fit_se'] = np.sqrt(sigmasqhat*(x_grid_mx.dot(xtxinv).dot(x_grid_mx.transpose()).diagonal()))\n\n# base layer\nbase = alt.Chart(grid_df).encode(\n    x = alt.X('tree_cover',  scale = alt.Scale(type = 'pow', exponent = 1/2, zero = False)),\n    color = alt.Color('mean_income', scale = alt.Scale(scheme = 'orangered', reverse = True, type = 'log'))\n)\n\n# regression lines\nlines = base.mark_line().encode(\n    y = alt.Y('mean_summer_temp', scale = alt.Scale(zero = False))\n)\n\n# uncertainty bands\nbands = base.transform_calculate(\n    upr = 'datum.mean_summer_temp + 2*datum.fit_se',\n    lwr = 'datum.mean_summer_temp - 2*datum.fit_se'\n).mark_errorband(opacity = 0.3).encode(\n    y = alt.Y('lwr:Q', title = 'mean_summer_temp'),\n    y2 = 'upr:Q'\n)\n\n# model visualization\nfig2 = fig1.mark_point(opacity = 0.2) + bands + lines\n\n# fig2.properties(width = 800, height = 500)"
  },
  {
    "objectID": "slides/week7-mlr.html#interpretation",
    "href": "slides/week7-mlr.html#interpretation",
    "title": "Data Science Concepts and Analysis",
    "section": "Interpretation",
    "text": "Interpretation\nSo what have we learned from this exercise?\n\nmlr_summary\n\n\n\n\n\n\n\n\nestimate\nstandard error\n\n\n\n\ninterept\n36.555976\n0.081990\n\n\ncover\n-3.148105\n0.472281\n\n\nincome\n-0.000050\n0.000002\n\n\nerror variance\n2.729421\nNaN\n\n\n\n\n\n\n\n\nAmong census blocks in San Diego, a 1% increase in tree canopy cover is associated with a decrease in average summer temperatures of 3.15 degrees Celsius, after accounting for mean income of the census block.\nAmong census blocks in San Diego, a $10K increase in mean income is associated with a decrease in average summer temperatures of 0.5 degrees Celsius, after accounting for tree canopy cover.\nThere’s still lots of unexplained local variation in average summer temperatures; low \\(R^2\\)."
  },
  {
    "objectID": "slides/week7-mlr.html#explaining-associations",
    "href": "slides/week7-mlr.html#explaining-associations",
    "title": "Data Science Concepts and Analysis",
    "section": "Explaining associations",
    "text": "Explaining associations\nThere are various mechanisms by which tree canopy reduces temperatures: providing shade; improving air quality. But what explains the weaker association between income and temperature?\nWell, one possibility is that property values are higher near the ocean.\n\nSo income may simply be a proxy for distance to the ocean; and temperatures are cooler near the ocean. This is an excellent example of confounding!"
  },
  {
    "objectID": "slides/week7-mlr.html#confounding",
    "href": "slides/week7-mlr.html#confounding",
    "title": "Data Science Concepts and Analysis",
    "section": "Confounding",
    "text": "Confounding\nIn statistical jargon, we’d say that proximity to the ocean is a confounding factor:\n\nit is correlated with higher incomes (the explanatory variable);\nit is also correlated with lower temperatures (the response);\nand so it ‘explains away’ the apparent association.\n\n\nThis kind of phenomenon only affects interpretation; we should still keep income in the model."
  },
  {
    "objectID": "slides/week7-mlr.html#mlr-with-categorical-variables",
    "href": "slides/week7-mlr.html#mlr-with-categorical-variables",
    "title": "Data Science Concepts and Analysis",
    "section": "MLR with categorical variables",
    "text": "MLR with categorical variables\nWhat if we also incorporated the population density factor (a categorical variable)?\n\ntrees.head(4)\n\n\n\n\n\n\n\n\nName\ncensus_block_GEOID\ntree_cover\nmean_summer_temp\nmean_income\nincome_level\npop_density\n\n\n\n\n11013\nSan Diego, CA\n6.073010e+13\n0.158259\n31.986364\n40951\nmedium\nhigh\n\n\n18997\nSan Diego, CA\n6.073020e+13\n0.013488\n34.068851\n51502\nhigh\nvery low\n\n\n8786\nSan Diego, CA\n6.073010e+13\n0.052844\n37.098611\n28454\nlow\nlow\n\n\n11163\nSan Diego, CA\n6.073000e+13\n0.217973\n33.213636\n40229\nmedium\nmedium\n\n\n\n\n\n\n\nWell, it might be natural to think we could just append this column as another variable \\(x_{i3}\\):\n\\[\\underbrace{\\text{temp}_i}_{y_i}\n    = \\beta_0 +\n        \\beta_1 \\underbrace{\\text{cover}_i}_{x_{i1}} +\n        \\beta_2 \\underbrace{\\text{income}_i}_{x_{i2}} +\n        \\beta_3 \\underbrace{\\text{density}_i}_{x_{i3}} + \\epsilon_i\n    \\qquad i = 1, \\dots, 1998\\]\nBut this doesn’t quite make sense, because the values of \\(\\text{density}_i\\) would be words! So…\n\\[\\beta_3 \\times \\text{low} = \\; ?\\]\nSo we’ll need to represent the categorical variable differently to include it in the model."
  },
  {
    "objectID": "slides/week7-mlr.html#indicator-variable-encoding",
    "href": "slides/week7-mlr.html#indicator-variable-encoding",
    "title": "Data Science Concepts and Analysis",
    "section": "Indicator variable encoding",
    "text": "Indicator variable encoding\nThe solution to this issue is to encode each level of the categorical variable using an indicator: a function whose value is zero or one to indicate a condition.\nIf we want to indicate whether a census block is of low population density, we can use the indicator:\n\\[I(\\text{density $=$ low}) = \\begin{cases} 1 \\text{ if population density is low} \\\\ 0 \\text{ otherwise}\\end{cases}\\]\nWe can encode the levels of pop_density using a collection of indicators:\n\ndensity_encoded = pd.get_dummies(trees.pop_density, drop_first = True)\npd.concat([trees[['pop_density']], density_encoded], axis = 1).head(4)\n\n\n\n\n\n\n\n\npop_density\nlow\nmedium\nhigh\n\n\n\n\n11013\nhigh\n0\n0\n1\n\n\n18997\nvery low\n0\n0\n0\n\n\n8786\nlow\n1\n0\n0\n\n\n11163\nmedium\n0\n1\n0\n\n\n\n\n\n\n\nThis captures all the information about the categorical variable in quantitative terms."
  },
  {
    "objectID": "slides/week7-mlr.html#the-mlr-model-with-indicators",
    "href": "slides/week7-mlr.html#the-mlr-model-with-indicators",
    "title": "Data Science Concepts and Analysis",
    "section": "The MLR model with indicators",
    "text": "The MLR model with indicators\nThe model with the encoded population density variable is:\n\\[\\underbrace{\\text{temp}_i}_{y_i}\n    = \\beta_0 +\n        \\beta_1 \\underbrace{\\text{cover}_i}_{x_{i1}} +\n        \\beta_2 \\underbrace{\\text{income}_i}_{x_{i2}} +\n        \\beta_3 \\underbrace{\\text{low}_i}_{x_{i3}} +\n        \\beta_4 \\underbrace{\\text{med}_i}_{x_{i4}} +\n        \\beta_5 \\underbrace{\\text{high}_i}_{x_{i5}} +\n        \\epsilon_i\n    \\qquad i = 1, \\dots, 1998\\]\nThe effect of doing this is to allow the model to have different intercepts for each population density group.\n\\[\\text{density $=$ very low}\n    \\quad\\Longrightarrow\\quad\n    \\mathbb{E}\\text{temp}_i\n        = \\underbrace{\\beta_0}_\\text{intercept} + \\beta_1\\text{cover}_i + \\beta_2\\text{income}_i \\\\\n  \\text{density $=$ low}\n    \\quad\\Longrightarrow\\quad\n    \\mathbb{E}\\text{temp}_i\n        = \\underbrace{\\beta_0 + \\beta_3}_\\text{intercept} + \\beta_1\\text{cover}_i + \\beta_2\\text{income}_i \\\\\n  \\text{density $=$ med}\n    \\quad\\Longrightarrow\\quad\n    \\mathbb{E}\\text{temp}_i\n        = \\underbrace{\\beta_0 + \\beta_4}_\\text{intercept} + \\beta_1\\text{cover}_i + \\beta_2\\text{income}_i \\\\\n  \\text{density $=$ high}\n    \\quad\\Longrightarrow\\quad\n    \\mathbb{E}\\text{temp}_i\n        = \\underbrace{\\beta_0 + \\beta_5}_\\text{intercept} + \\beta_1\\text{cover}_i + \\beta_2\\text{income}_i\\]"
  },
  {
    "objectID": "slides/week7-mlr.html#in-matrix-form",
    "href": "slides/week7-mlr.html#in-matrix-form",
    "title": "Data Science Concepts and Analysis",
    "section": "In matrix form",
    "text": "In matrix form\nThe explanatory variable matrix \\(\\mathbf{X}\\) for this full model (‘full’ because it includes all variables) will be of the form:\n\\[\\mathbf{X} = \\left[\\begin{array}{c:ccccc}\n    1 &\\text{cover}_{1} &\\text{income}_{1} &\\text{low}_1 &\\text{med}_1 &\\text{high}_1 \\\\\n    1 &\\text{cover}_{2} &\\text{income}_{2} &\\text{low}_2 &\\text{med}_2 &\\text{high}_2 \\\\\n    \\vdots &\\vdots &\\vdots &\\vdots &\\vdots &\\vdots \\\\\n    1 &\\text{cover}_{1998} &\\text{income}_{1998} &\\text{low}_{1998} &\\text{med}_{1998} &\\text{high}_{1998}\n    \\end{array}\\right]\\]\nHere’s everything to the right of the dashed partition:\n\nx_full_df = pd.concat([x_df, density_encoded], axis = 1)\nx_full_df.head(4)\n\n\n\n\n\n\n\n\ntree_cover\nmean_income\nlow\nmedium\nhigh\n\n\n\n\n11013\n0.158259\n40951\n0\n0\n1\n\n\n18997\n0.013488\n51502\n0\n0\n0\n\n\n8786\n0.052844\n28454\n1\n0\n0\n\n\n11163\n0.217973\n40229\n0\n1\n0"
  },
  {
    "objectID": "slides/week7-mlr.html#fit-summary",
    "href": "slides/week7-mlr.html#fit-summary",
    "title": "Data Science Concepts and Analysis",
    "section": "Fit summary",
    "text": "Fit summary\nThe remaining calculations are all the same as before. (You can see the source code for these slides if you’re interested in reviewing the details.)\nHere is the model fit summary:\n\n# fit model\nx_full_mx = add_dummy_feature(x_full_df)\nmlr_full = lm.LinearRegression(fit_intercept = False)\nmlr_full.fit(x_full_mx, y)\n\n# store dimensions of explanatory variable matrix\nn, p_full = x_full_mx.shape\n\n# compute residual variance\nsigmasqhat_full = ((n - 1)/(n - p_full)) * (y - mlr_full.predict(x_full_mx)).var()\n\n# compute standard errors\nvhat_full = np.linalg.inv(x_full_mx.transpose().dot(x_full_mx)) * sigmasqhat_full\ncoef_full_se = np.sqrt(vhat_full.diagonal())\n\n# construct coefficient table\nmlr_full_summary = pd.DataFrame(\n    data = {'estimate': np.append(mlr_full.coef_, sigmasqhat_full), \n            'standard error': np.append(coef_full_se, float('nan'))},\n    index= ['intercept', 'cover', 'income', 'low density', 'medium density', 'high density', 'error variance']\n)\n\n\n# print\nmlr_full_summary\n\n\n\n\n\n\n\n\nestimate\nstandard error\n\n\n\n\nintercept\n36.593462\n0.114941\n\n\ncover\n-3.177783\n0.492391\n\n\nincome\n-0.000051\n0.000002\n\n\nlow density\n0.151946\n0.095099\n\n\nmedium density\n-0.128064\n0.107888\n\n\nhigh density\n-0.149409\n0.132829\n\n\nerror variance\n2.719294\nNaN\n\n\n\n\n\n\n\n\nEstimates for the cover and income coefficients are about the same.\nThe population density variable changes the intercept by about \\(\\pm 0.15\\) degrees Celsius, depending on the density level.\nSo the association between temperature and population density appears negligible."
  },
  {
    "objectID": "slides/week7-mlr.html#model-visualization",
    "href": "slides/week7-mlr.html#model-visualization",
    "title": "Data Science Concepts and Analysis",
    "section": "Model visualization",
    "text": "Model visualization\nThe estimated trends and error bands for the same three income levels and each level of population density are shown below, without the data scatter:\n\n\n# make univariate grids\ntree_grid = np.linspace(trees.tree_cover.min(), trees.tree_cover.max(), 100)\nincome_grid = trees.mean_income.quantile([0.1, 0.5, 0.9]).values\ndens_grid = trees.pop_density.cat.categories.values\n\n# make mesh grid -- all combinations of univariate grid values\ntx, ix, dx = np.meshgrid(tree_grid, income_grid, dens_grid)\nx_grid_mx = np.vstack([np.repeat(1, 1200), tx.reshape(1200), ix.reshape(1200), dx.reshape(1200)]).transpose()\ngrid_df = pd.DataFrame(x_grid_mx, columns = ['intercept', 'tree_cover', 'mean_income', 'pop_density'])\ngrid_mx = pd.concat(\n    [grid_df, pd.get_dummies(grid_df.pop_density, drop_first = True)],\n    axis = 1\n).drop(columns = 'pop_density').astype('float64').values\n\n# add predictions and standard errors\ngrid_df['mean_summer_temp'] = mlr_full.predict(grid_mx)\ngrid_df['fit_se'] = np.sqrt(grid_mx.dot(vhat_full).dot(grid_mx.transpose()).diagonal())\ngrid_df['density_order'] = grid_df.pop_density.replace({'very low': 1, 'low': 2, 'medium': 3, 'high': 4})\n\n# base layer\nbase = alt.Chart(grid_df).encode(\n    x = alt.X('tree_cover',  scale = alt.Scale(type = 'pow', exponent = 1/2, zero = False)),\n    color = alt.Color('mean_income', scale = alt.Scale(scheme = 'orangered', reverse = True, type = 'log'))\n)\n\n# regression lines\nlines = base.mark_line().encode(\n    y = alt.Y('mean_summer_temp', scale = alt.Scale(zero = False)),\n    strokeDash = 'pop_density'\n)\n\n# uncertainty bands\nbands = base.transform_calculate(\n    upr = 'datum.mean_summer_temp + 2*datum.fit_se',\n    lwr = 'datum.mean_summer_temp - 2*datum.fit_se'\n).mark_errorband(opacity = 0.3).encode(\n    y = alt.Y('lwr:Q', title = 'mean_summer_temp'),\n    y2 = 'upr:Q'\n)\n\n# model visualization\nfig3 = bands + lines\n\n# fig3.properties(\n#     width = 125,\n#     height = 200\n# ).facet(\n#     column = alt.Column('pop_density', \n#                         sort = {'field': 'density_order',\n#                                 'order': 'descending'})\n# )"
  },
  {
    "objectID": "slides/week7-mlr.html#comments-on-scope-of-inference",
    "href": "slides/week7-mlr.html#comments-on-scope-of-inference",
    "title": "Data Science Concepts and Analysis",
    "section": "Comments on scope of inference",
    "text": "Comments on scope of inference\nThe data in this case study are from a random sample of census blocks in the San Diego urban area.\nThey are therefore representative of all census blocks in the San Diego urban area (population).\nSo the results are generliazable, meaning: * The model approximates the actual associations between summer temperatures, tree cover, income, and population density in the region."
  },
  {
    "objectID": "slides/week7-mlr.html#summary",
    "href": "slides/week7-mlr.html#summary",
    "title": "Data Science Concepts and Analysis",
    "section": "Summary",
    "text": "Summary\nThis week focused on extending the simple linear model to multiple explanatory variables.\n\nThe linear model represents a quantitative response variable \\(y_i\\) as a linear function of several explanatory variables and a random error:\n\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_{p - 1} x_{i, p - 1} + \\epsilon_i\\]\n\nWhen represented in matrix form \\(\\mathbf{y} = \\mathbf{X}\\beta + \\epsilon\\), all calculations are the same as in the simple linear model.\n\nOLS estimates: \\(\\left(\\mathbf{X'X}\\right)^{-1}\\mathbf{X'y}\\)\nError variance: \\(\\hat{\\sigma}^2 = \\frac{1}{n - p}\\left(\\mathbf{y} - \\mathbf{X}\\hat{\\beta}\\right)'\\left(\\mathbf{y} - \\mathbf{X}\\hat{\\beta}\\right)\\)\nUncertainty quantification: \\(\\hat{\\mathbf{V}} = \\hat{\\sigma}^2\\left(\\mathbf{X'X}\\right)^{-1}\\)\n\nModel fitting and interpretation was illustrated in a case study of urban tree cover in San Diego.\n\nEstimation and uncertainty quantification calculations\nModel visualization\nEncoding categorical variables\nParameter interpretation"
  },
  {
    "objectID": "slides/week4-graphics.html#this-week-data-visualization",
    "href": "slides/week4-graphics.html#this-week-data-visualization",
    "title": "Statistical graphics",
    "section": "This week: data visualization",
    "text": "This week: data visualization\n\nUses of data visualization\n\nExploration\nPresentation\n\nStatistical graphics\n\nGraphical elements: axes, geometric objects, aesthetic attributes, and text\nBuilding graphics: mapping data to graphical elements\n\nSurvey of common graphics\n\none- and two-variable displays\n\nPrinciples of effective visualization\n\nEffective uses of aesthetics and layout\nCommon blunders"
  },
  {
    "objectID": "slides/week4-graphics.html#figure-credits",
    "href": "slides/week4-graphics.html#figure-credits",
    "title": "Statistical graphics",
    "section": "Figure credits",
    "text": "Figure credits\nMany of the figures from this week’s slides are from Claus Wilke’s Fundamentals of Data Visualization."
  },
  {
    "objectID": "slides/week4-graphics.html#notice-your-reaction",
    "href": "slides/week4-graphics.html#notice-your-reaction",
    "title": "Statistical graphics",
    "section": "Notice your reaction",
    "text": "Notice your reaction\n\n\nThis is great for a paper or technical report, but it takes effort to discern patterns; I’d much rather see a few plots, like achievement vs. year by grade and gender."
  },
  {
    "objectID": "slides/week4-graphics.html#uses-of-graphics",
    "href": "slides/week4-graphics.html#uses-of-graphics",
    "title": "Statistical graphics",
    "section": "Uses of graphics",
    "text": "Uses of graphics\nThere is a broad distinction between:\n\nexploratory graphics, which are intended to be seen only by analysts; and\npresentation graphics, which are intended to be seen by an audience.\n\n\nExploratory graphics are made quickly in large volumes, and usually not formatted too carefully. Think of them like the pages of a sketchbook.\n\n\nPresentation graphics are made slowly with great attention to detail. Think of them as exhibition artworks.\n\n\nThe two are not mutually exclusive: an especially helpful exploratory graphic is often worth developing as a presentation graphic to help an audience understand ‘what the data look like’."
  },
  {
    "objectID": "slides/week4-graphics.html#elements-of-statistical-graphics",
    "href": "slides/week4-graphics.html#elements-of-statistical-graphics",
    "title": "Statistical graphics",
    "section": "Elements of statistical graphics",
    "text": "Elements of statistical graphics\nStatistical graphics are actually quite simple. They consist of the following four elements:\n\nAxes\n\nReferences for all other graphical elements.\n\nGeometric objects\n\nPoints, lines, curves, filled regions, etc.\n\nAesthetic attributes\n\nColor, shape, size, opacity/transparency.\n\nText\n\nLabels, legends, and titles."
  },
  {
    "objectID": "slides/week4-graphics.html#axes",
    "href": "slides/week4-graphics.html#axes",
    "title": "Statistical graphics",
    "section": "Axes",
    "text": "Axes\nWe are all familiar with axes. The word axis literally means axle: an axis is an object that other things turn around.\n\nIn statistical graphics, axes establish positional references for locating any geometric object – line, point, polygon – on the graphic."
  },
  {
    "objectID": "slides/week4-graphics.html#geometric-objects",
    "href": "slides/week4-graphics.html#geometric-objects",
    "title": "Statistical graphics",
    "section": "Geometric objects",
    "text": "Geometric objects\nGeometric objects are the things depicted on a plot, whatever those may be; typically points, lines, polygons, and shapes."
  },
  {
    "objectID": "slides/week4-graphics.html#aesthetic-attributes",
    "href": "slides/week4-graphics.html#aesthetic-attributes",
    "title": "Statistical graphics",
    "section": "Aesthetic attributes",
    "text": "Aesthetic attributes\nFor us, aesthetics will mean qualities of geometric objects, like color or transparency.\n\nPrimary aesthetics in statistical graphics are:\n\nShape (for points)\nColor\nSize\nOpacity/transparency"
  },
  {
    "objectID": "slides/week4-graphics.html#text",
    "href": "slides/week4-graphics.html#text",
    "title": "Statistical graphics",
    "section": "Text",
    "text": "Text\nText is used to label axes, objects, legends, and specify titles.\n\nText may seem innocuous, but it is what creates story – text gives a plot its plot!"
  },
  {
    "objectID": "slides/week4-graphics.html#statistical-graphics-are-mappings",
    "href": "slides/week4-graphics.html#statistical-graphics-are-mappings",
    "title": "Statistical graphics",
    "section": "Statistical graphics are mappings",
    "text": "Statistical graphics are mappings\nStatistical graphics are mappings of dataframe columns and attributes to graphical elements: axes, geometric objects, and aesthetic attributes.\n\nFor a simple example, consider the following time series of Cuba’s population by year:\n\n\n\n\n\n\n\n\n\n\n\nMappings:\n\npopulation \\(\\longrightarrow\\) y coordinate of axis;\nyear \\(\\longrightarrow\\) x coordinate of axis;\nobservations \\(\\longrightarrow\\) line"
  },
  {
    "objectID": "slides/week4-graphics.html#mapping-columns-to-aesthetics",
    "href": "slides/week4-graphics.html#mapping-columns-to-aesthetics",
    "title": "Statistical graphics",
    "section": "Mapping columns to aesthetics",
    "text": "Mapping columns to aesthetics\nNow consider aggregated populations by global region and year:\n\n\n\n\n\n\n\n\n\n\nMappings:\n\npopulation \\(\\longrightarrow\\) y\nyear \\(\\longrightarrow\\) x\nregion \\(\\longrightarrow\\) color\nobservations \\(\\longrightarrow\\) line (groupwise by color)"
  },
  {
    "objectID": "slides/week4-graphics.html#using-aesthetics",
    "href": "slides/week4-graphics.html#using-aesthetics",
    "title": "Statistical graphics",
    "section": "Using aesthetics",
    "text": "Using aesthetics\nThe ability to map variables to the elements of a graphic is essential because it means we can display more than two variables at a time by leveraging aesthetic attributes.\n\nFor example, in lab you’ll begin with this scatterplot:\n\n\n\n\n\n\n\n\n\nEach point represents a country in a particular year. The graphic shows that life expectancy increases with GDP per capita."
  },
  {
    "objectID": "slides/week4-graphics.html#using-aesthetics-1",
    "href": "slides/week4-graphics.html#using-aesthetics-1",
    "title": "Statistical graphics",
    "section": "Using aesthetics",
    "text": "Using aesthetics\nIn the lab you’ll add aesthetic mappings step by step until arriving at this plot:\n\n\n\n\n\n\n\n\nThis figure displays the same x-y relationship as before, but together with time, continental region, and population.\n\n\nVerges on too complex."
  },
  {
    "objectID": "slides/week4-graphics.html#using-graphics-for-discovery",
    "href": "slides/week4-graphics.html#using-graphics-for-discovery",
    "title": "Statistical graphics",
    "section": "Using graphics for discovery",
    "text": "Using graphics for discovery\nFurther incorporating sex shows that GDP per capita is associated with differential life expectancy gaps between men and women:\n\n\n\n\n\n\n\n\nIn other words, on average women outlive men by longer in wealtheir countries.\n\nMaybe just by an additional 2-3 years in wealthier countries\nClear pattern but lots of variation for a given GDP/capita"
  },
  {
    "objectID": "slides/week4-graphics.html#altair",
    "href": "slides/week4-graphics.html#altair",
    "title": "Statistical graphics",
    "section": "Altair",
    "text": "Altair\nAltair, a python library, creates graphics exactly as described above: mapping columns of a dataframe to graphical elements.\n\nIt has a somewhat idiosyncratic syntactical pattern involving a “chart”, “marks”, and “encodings”:\n\n\n\n\n\n\n\n\nAltair syntax\nExample handle\nOperation\n\n\n\n\nChart\nalt.Chart(df)\nCoerces a dataframe df to a chart object\n\n\nMark\nmark_point()\nSpecifies a geometric object\n\n\nEncoding\nencode(x = ..., y = ..., color = ...)\nMaps columns of df to objects and aesthetics"
  },
  {
    "objectID": "slides/week4-graphics.html#basic-use-of-syntax",
    "href": "slides/week4-graphics.html#basic-use-of-syntax",
    "title": "Statistical graphics",
    "section": "Basic use of syntax",
    "text": "Basic use of syntax\nA chart specification, mark(s), and encodings are chained together to make a graphic.\n\n\n\nCode\nalt.Chart( \n    popregion \n).mark_line( \n).encode(\n    x = 'Year:T', y = 'Population', color = 'Region' \n).properties(\n    width = 500, height = 100\n).configure_axis(\n    labelFontSize = 16, titleFontSize = 16\n).configure_legend(\n    labelFontSize = 16, titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week4-graphics.html#choice-of-scale",
    "href": "slides/week4-graphics.html#choice-of-scale",
    "title": "Statistical graphics",
    "section": "Choice of scale",
    "text": "Choice of scale\nThe choice of scales for each mapping can either reveal or obscure patterns in data.\n\nWhen population is mapped onto a logarithmic rather than linear scale, rates of increase become evident in less populous regions:\n\n\n\n\nCode\nalt.Chart( \n    popregion\n).mark_line( \n).encode( \n    x = 'Year:T',\n    y = alt.Y('Population', scale = alt.Scale(type = 'log')), # change axis scale\n    color = 'Region'\n).properties(\n    width = 350, height = 100 \n).configure_axis(\n    labelFontSize = 16, titleFontSize = 16\n).configure_legend(\n    labelFontSize = 16, titleFontSize = 16\n)\n\n\n\n\n\n\n\n\n\nNote: scale is adjusted at the encoding level by alt.Y(...); every encoding channel has an analogous function, e.g., alt.X(...), alt.Color(...), alt.Shape(...), etc., with optional scale arguments."
  },
  {
    "objectID": "slides/week4-graphics.html#common-statistical-graphics",
    "href": "slides/week4-graphics.html#common-statistical-graphics",
    "title": "Statistical graphics",
    "section": "Common statistical graphics",
    "text": "Common statistical graphics\nBroadly, the most common statistical graphics can be divided according to the number of variables that form their primary display. The uses listed below are not exclusive, just some of the most common.\n\nOne-variable graphics are used to visualize distributions.\nTwo-variable graphics are used to visualize relationships.\nThree-variable graphics are used to visualize spatial data, matrices, and a collection of other data types.\n\n\n\nMost graphics you’ll encounter are grouped one- or two-variable graphics with superpositions of geometric objects differentiating observed from inferred values – e.g., scatterplots with points color-coded by another (grouping) variable and trend lines."
  },
  {
    "objectID": "slides/week4-graphics.html#single-variable-graphics",
    "href": "slides/week4-graphics.html#single-variable-graphics",
    "title": "Statistical graphics",
    "section": "Single-variable graphics",
    "text": "Single-variable graphics\nSingle-variable graphics usually display the distribution of values of a single variable.\n\n\n\n\nCommon single-variable graphics\n\n\n\n\nHistograms and smoothed density plots show shape but depend on arbitrary binning/smoothing parameters.\n\n\nCDF and quantile plots show the distribution exactly but are harder to interpret."
  },
  {
    "objectID": "slides/week4-graphics.html#histograms",
    "href": "slides/week4-graphics.html#histograms",
    "title": "Statistical graphics",
    "section": "Histograms",
    "text": "Histograms\nHistograms show the relative frequencies of values of a single variable.\n\n\nCode\nalt.Chart(\n    popcountry.loc[\"1970\"]\n).mark_bar().encode(\n    x = alt.X('log(Population)', \n            bin = alt.Bin(maxbins = 50)),\n    y = 'count()'\n).properties(\n    height = 150,\n    title = 'National populations in 1970'\n).properties(\n    width = 500, height = 300 \n).configure_axis(\n    labelFontSize = 16, titleFontSize = 16\n).configure_legend(\n    labelFontSize = 16, titleFontSize = 16\n).configure_title(\n    fontSize = 16\n)\n\n\n\n\n\n\n\n\nThe main advantage of the histogram is it shows the shape of a distribution."
  },
  {
    "objectID": "slides/week4-graphics.html#bin-widths",
    "href": "slides/week4-graphics.html#bin-widths",
    "title": "Statistical graphics",
    "section": "Bin widths",
    "text": "Bin widths\nThe main downside is that the shape depends on bin width, which is an arbitrary parameter.\n\n\n\n\nCode\nalt.Chart(\n    popcountry.loc[\"1970\"]\n).mark_bar().encode(\n    x = alt.X('log(Population)', \n            bin = alt.Bin(maxbins = 10)),\n    y = 'count()'\n).properties(\n    height = 150,\n    title = 'National populations in 1970'\n).properties(\n    width = 350, height = 100 \n).configure_axis(\n    labelFontSize = 16, titleFontSize = 16\n).configure_legend(\n    labelFontSize = 16, titleFontSize = 16\n).configure_title(\n    fontSize = 16\n)\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(\n    popcountry.loc[\"1970\"]\n).mark_bar().encode(\n    x = alt.X('log(Population)', \n            bin = alt.Bin(maxbins = 50)),\n    y = 'count()'\n).properties(\n    height = 150,\n    title = 'National populations in 1970'\n).properties(\n    width = 350, height = 100 \n).configure_axis(\n    labelFontSize = 16, titleFontSize = 16\n).configure_legend(\n    labelFontSize = 16, titleFontSize = 16\n).configure_title(\n    fontSize = 16\n)\n\n\n\n\n\n\n\n\n\n\nAlways experiment with multiple bin widths to ensure you don’t overlook any important details such as outliers, multiple modes, etc.\n\nBinning at left is too coarse, obscures outlying values\nBinning at right is good"
  },
  {
    "objectID": "slides/week4-graphics.html#density-plots",
    "href": "slides/week4-graphics.html#density-plots",
    "title": "Statistical graphics",
    "section": "Density plots",
    "text": "Density plots\nDenisty plots are smoothed histograms – we’ll discuss further next week. They also require some arbitrary choices that affect the appearance.\n\nDensity plots with different smoothing kernels and bandwidths"
  },
  {
    "objectID": "slides/week4-graphics.html#more-single-variable-graphics",
    "href": "slides/week4-graphics.html#more-single-variable-graphics",
    "title": "Statistical graphics",
    "section": "More single-variable graphics",
    "text": "More single-variable graphics\nGrouped single-variable graphics allow visualization of multiple distributions.\n\n\n\n\nGrouped single-variable graphics"
  },
  {
    "objectID": "slides/week4-graphics.html#boxplots",
    "href": "slides/week4-graphics.html#boxplots",
    "title": "Statistical graphics",
    "section": "Boxplots",
    "text": "Boxplots\nBoxplots display data quantiles and outliers, conveying skewness and range.\n\n\nDue to their compactness, they are useful for comparing multiple distributions."
  },
  {
    "objectID": "slides/week4-graphics.html#many-boxplots",
    "href": "slides/week4-graphics.html#many-boxplots",
    "title": "Statistical graphics",
    "section": "Many boxplots",
    "text": "Many boxplots\nSingle-variable graphics are not necessarily limited to univariate data; one might want to compare distributions using the same single-variable displays shown groupwise.\n\n\n\nCode\nalt.Chart(\n    popcountry.reset_index()\n).mark_boxplot(\n    outliers = True, size = 7\n).encode(\n    x = 'Year:T', \n    y = alt.Y('Population', scale = alt.Scale(type = 'log')) \n).properties(\n    width = 600\n).configure_axis(\n    labelFontSize = 16, titleFontSize = 16\n).configure_legend(\n    labelFontSize = 16, titleFontSize = 16\n).configure_title(\n    fontSize = 16\n)"
  },
  {
    "objectID": "slides/week4-graphics.html#multiple-histograms",
    "href": "slides/week4-graphics.html#multiple-histograms",
    "title": "Statistical graphics",
    "section": "Multiple histograms",
    "text": "Multiple histograms\nHistograms aren’t well-suited to comparing distributions. Do not stack histograms.\n\n\n\n\n\nStacked histograms\n\n\n\n\n\n\nOverlaid histograms\n\n\n\n\n\nStacked histograms do not preserve the shape of distributions (except whichever one is on the bottom).\n\n\nOverlaid histograms are visually messy due to color blending."
  },
  {
    "objectID": "slides/week4-graphics.html#multiple-histograms-1",
    "href": "slides/week4-graphics.html#multiple-histograms-1",
    "title": "Statistical graphics",
    "section": "Multiple histograms",
    "text": "Multiple histograms\nHere’s a creative solution, but one that will only work for comparing two distributions."
  },
  {
    "objectID": "slides/week4-graphics.html#alternatives-to-multiple-histograms",
    "href": "slides/week4-graphics.html#alternatives-to-multiple-histograms",
    "title": "Statistical graphics",
    "section": "Alternatives to multiple histograms",
    "text": "Alternatives to multiple histograms\nDensity plots are better alternatives to stacked histograms for a small-ish number of distributions.\n\nOverlapping densities"
  },
  {
    "objectID": "slides/week4-graphics.html#visualizing-many-distributions",
    "href": "slides/week4-graphics.html#visualizing-many-distributions",
    "title": "Statistical graphics",
    "section": "Visualizing many distributions",
    "text": "Visualizing many distributions\nRidge plots are good options for comparing a large number of distributions at once.\n\nRidge plot: temperatures in Nebraska in 2016."
  },
  {
    "objectID": "slides/week4-graphics.html#visualizing-many-distributions-1",
    "href": "slides/week4-graphics.html#visualizing-many-distributions-1",
    "title": "Statistical graphics",
    "section": "Visualizing many distributions",
    "text": "Visualizing many distributions\nRidge plots are good options for comparing a large number of distributions at once.\n\nAnother ridge plot: movie lengths by year."
  },
  {
    "objectID": "slides/week4-graphics.html#two-variable-graphics",
    "href": "slides/week4-graphics.html#two-variable-graphics",
    "title": "Statistical graphics",
    "section": "Two-variable graphics",
    "text": "Two-variable graphics\nTwo-variable graphics are all about displaying relationships, usually with scatter or lines.\n\n\n\n\nBasic two-variable scatterplots"
  },
  {
    "objectID": "slides/week4-graphics.html#scatterplots",
    "href": "slides/week4-graphics.html#scatterplots",
    "title": "Statistical graphics",
    "section": "Scatterplots",
    "text": "Scatterplots\nScatterplots display relationships between two variables.\n\n\nCode\nalt.Chart(\n    lifegdp.loc[2015]\n).mark_point().encode(\n    x = alt.X('GDP per capita', scale = alt.Scale(type = 'log')),\n    y = alt.Y('All', scale = alt.Scale(zero = False), \n    title = 'Life expectancy at birth')\n).properties(\n    width = 600\n).configure_axis(\n    labelFontSize = 16, titleFontSize = 16\n).configure_legend(\n    labelFontSize = 16, titleFontSize = 16\n).configure_title(\n    fontSize = 16\n)\n\n\n\n\n\n\n\n\nYou’ll make extensive use of scatter and bubble plots for displaying this relationship in lab 4."
  },
  {
    "objectID": "slides/week4-graphics.html#alternatives-to-bar-plots",
    "href": "slides/week4-graphics.html#alternatives-to-bar-plots",
    "title": "Statistical graphics",
    "section": "Alternatives to bar plots",
    "text": "Alternatives to bar plots\nBar plots usually depict amount or magnitude.\n\n\nCode\nalt.Chart(\n    popregion\n).mark_bar(\n).encode(\n    x = 'Year:T',\n    y = alt.Y('Population:Q'),\n    color = 'Region'\n).properties(\n    width = 600\n).configure_axis(\n    labelFontSize = 16, titleFontSize = 16\n).configure_legend(\n    labelFontSize = 16, titleFontSize = 16\n).configure_title(\n    fontSize = 16\n)"
  },
  {
    "objectID": "slides/week4-graphics.html#alternatives-to-bar-plots-1",
    "href": "slides/week4-graphics.html#alternatives-to-bar-plots-1",
    "title": "Statistical graphics",
    "section": "Alternatives to bar plots",
    "text": "Alternatives to bar plots\n\nThere is almost always a better alternative to a bar chart\n\nWith reference to the last example, which are we more interested in:\n\nPopulation growth by region?\nRegional share of global population?\n\n\nAs an aside, this depends on what story the plot is intended to tell and how it fits into the broader data analysis."
  },
  {
    "objectID": "slides/week4-graphics.html#alternatives-to-bar-plots-2",
    "href": "slides/week4-graphics.html#alternatives-to-bar-plots-2",
    "title": "Statistical graphics",
    "section": "Alternatives to bar plots",
    "text": "Alternatives to bar plots\nIf it’s population growth by region, the line plot from earlier is cleaner.\n\n\n\nCode\nalt.Chart( \n    popregion\n).mark_line( \n).encode( \n    x = 'Year:T',\n    y = alt.Y('Population', scale = alt.Scale(type = 'log')), # change axis scale\n    color = 'Region'\n).properties(\n    width = 400, height = 300 \n).configure_axis(\n    labelFontSize = 16, titleFontSize = 16\n).configure_legend(\n    labelFontSize = 16, titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week4-graphics.html#alternatives-to-bar-plots-3",
    "href": "slides/week4-graphics.html#alternatives-to-bar-plots-3",
    "title": "Statistical graphics",
    "section": "Alternatives to bar plots",
    "text": "Alternatives to bar plots\nIf it’s the relative share of the globabl population in each region over time, an area chart is cleaner.\n\n\nCode\nalt.Chart(\n    popregion\n).mark_area(\n).encode(\n    x = \"Year:T\",\n    y = alt.Y(\"Population:Q\", \n        stack = \"normalize\",\n        scale = alt.Scale(type = 'sqrt'),\n        title = \"Proportion of global population\"),\n    color = \"Region:N\"\n).properties(\n    width = 600\n).configure_axis(\n    labelFontSize = 16, titleFontSize = 16\n).configure_legend(\n    labelFontSize = 16, titleFontSize = 16\n).configure_title(\n    fontSize = 16\n)"
  },
  {
    "objectID": "slides/week4-graphics.html#but-if-you-insist-on-bars",
    "href": "slides/week4-graphics.html#but-if-you-insist-on-bars",
    "title": "Statistical graphics",
    "section": "But if you insist on bars…",
    "text": "But if you insist on bars…\n… there are some rules of thumb to keep in mind for bar plots:\n\norient axes so labels are legible\ndon’t stack; use side-by-side bars instead\nstart bars at zero, so that their height is proportional to the quantity of interest\narrange bar order sensibly; if the categories are ordered, arrange by order, and otherwise, sort by bar height"
  },
  {
    "objectID": "slides/week4-graphics.html#axis-orientation-for-barplots",
    "href": "slides/week4-graphics.html#axis-orientation-for-barplots",
    "title": "Statistical graphics",
    "section": "Axis orientation for barplots",
    "text": "Axis orientation for barplots\nIf you have to tilt your head, there’s a better orientation available.\n\n\n\n\n\nAwkward axis orientation.\n\n\n\n\n\n\nGood axis orientation."
  },
  {
    "objectID": "slides/week4-graphics.html#ordering-of-bars",
    "href": "slides/week4-graphics.html#ordering-of-bars",
    "title": "Statistical graphics",
    "section": "Ordering of bars",
    "text": "Ordering of bars\nFor categorical bar plots, order bars by height.\n\n\n\n\n\nMessy order\n\n\n\n\n\n\nOrdered by bar height"
  },
  {
    "objectID": "slides/week4-graphics.html#ordering-of-bars-1",
    "href": "slides/week4-graphics.html#ordering-of-bars-1",
    "title": "Statistical graphics",
    "section": "Ordering of bars",
    "text": "Ordering of bars\nBut don’t order by height if the categories themselves are ordered.\n\n\n\n\n\nOrdered categories\n\n\n\n\n\n\nJumbled order"
  },
  {
    "objectID": "slides/week4-graphics.html#group-dont-stack",
    "href": "slides/week4-graphics.html#group-dont-stack",
    "title": "Statistical graphics",
    "section": "Group don’t stack",
    "text": "Group don’t stack\nStacked bars are not an effective means of comparing distributions – group and use side-by-side bars instead.\n\nGrouped bar plot"
  },
  {
    "objectID": "slides/week4-graphics.html#always-start-at-zero",
    "href": "slides/week4-graphics.html#always-start-at-zero",
    "title": "Statistical graphics",
    "section": "Always start at zero",
    "text": "Always start at zero\nBar height should be proportional to the quantity of interest.\n\n\n\n\n\nBars start near the minimum observed value.\n\n\n\n\n\n\nBars start at zero."
  },
  {
    "objectID": "slides/week4-graphics.html#but-dont-take-up-all-the-space",
    "href": "slides/week4-graphics.html#but-dont-take-up-all-the-space",
    "title": "Statistical graphics",
    "section": "But don’t take up all the space",
    "text": "But don’t take up all the space\nIf your bars occupy almost the entire plot, there’s probably a better alternative. Try dots.\n\n\n\n\n\nBars do a poor job of conveying differences in life expectancy\n\n\n\n\n\n\nDots make this clearer, since the axis can start away from zero"
  },
  {
    "objectID": "slides/week4-graphics.html#other-common-visuzlizations",
    "href": "slides/week4-graphics.html#other-common-visuzlizations",
    "title": "Statistical graphics",
    "section": "Other common visuzlizations",
    "text": "Other common visuzlizations\nSmoothing scatterplots helps to visualize trends. Next week we’ll discuss this in detail.\n\nScatterplot smoothing"
  },
  {
    "objectID": "slides/week4-graphics.html#other-common-visualizations",
    "href": "slides/week4-graphics.html#other-common-visualizations",
    "title": "Statistical graphics",
    "section": "Other common visualizations",
    "text": "Other common visualizations\nHeatmaps are a common choice for displaying amounts in two-way groupings or for visualizing matrices.\n\nHeatmap"
  },
  {
    "objectID": "slides/week4-graphics.html#small-but-important-choices",
    "href": "slides/week4-graphics.html#small-but-important-choices",
    "title": "Statistical graphics",
    "section": "Small but important choices",
    "text": "Small but important choices\nHow to order the countries? Depends on what feature you wish to emphasize.\n\nCountries reordered by internet use in 2016.\nAre you more interested in present internet use, or early/late adoption?"
  },
  {
    "objectID": "slides/week4-graphics.html#other-common-visualizations-1",
    "href": "slides/week4-graphics.html#other-common-visualizations-1",
    "title": "Statistical graphics",
    "section": "Other common visualizations",
    "text": "Other common visualizations\nChloropleth maps are the most common display of spatial data.\n\nChloropleth map"
  },
  {
    "objectID": "slides/week4-graphics.html#what-makes-visualizations-effective",
    "href": "slides/week4-graphics.html#what-makes-visualizations-effective",
    "title": "Statistical graphics",
    "section": "What makes visualizations effective?",
    "text": "What makes visualizations effective?\n\nNovel. Novel visuals don’t need to elicit superlative reactions, but they should (if only subtly) surprise and spark interest to some extent.\nInformative. Informative visuals make information apparent. In a way they are unambiguous.\nEfficient. Efficient visuals have an accessible message. They use space economically but without becoming overly complicated.\nPleasant. Visuals should be nice to look at!\n\n\nNext time we’ll discuss principles of visualizaiton"
  },
  {
    "objectID": "slides/week2-transform.html#recap-tidy-data",
    "href": "slides/week2-transform.html#recap-tidy-data",
    "title": "Dataframe Transformations",
    "section": "Recap: tidy data",
    "text": "Recap: tidy data\nThe tidy standard consists in matching semantics and structure.\n\nA dataset is tidy if:\n\nEach variable is a column.\nEach observation is a row.\nEach table contains measurements on only one type of observational unit."
  },
  {
    "objectID": "slides/week2-transform.html#why-tidy",
    "href": "slides/week2-transform.html#why-tidy",
    "title": "Dataframe Transformations",
    "section": "Why tidy?",
    "text": "Why tidy?\n\nWhy use the tidy standard? Wouldn’t any system of organization do just as well?\n\n\nThe tidy standard has three main advantages:\n\nHaving a consistent system of organization makes it easier to focus on analysis and exploration. (True of any system)\nMany software tools are designed to work with tidy data inputs. (Tidy only)\nTransformation of tidy data is especially natural in most computing environments due to vectorized operations. (Tidy only)"
  },
  {
    "objectID": "slides/week2-transform.html#transformations",
    "href": "slides/week2-transform.html#transformations",
    "title": "Dataframe Transformations",
    "section": "Transformations",
    "text": "Transformations\nTransformations of data frames are operations that modify the shape or values of a data frame. These include:\n\nSlicing rows and columns by index\nFiltering rows by logical conditions\nDefining new variables from scratch or by operations on existing variables\nAggregations (min, mean, max, etc.)"
  },
  {
    "objectID": "slides/week2-transform.html#slicing",
    "href": "slides/week2-transform.html#slicing",
    "title": "Dataframe Transformations",
    "section": "Slicing",
    "text": "Slicing\nSlicing refers to retrieving a (usually contiguous) subset (a ‘slice’) of rows/columns from a data frame.\n\nUses:\n\ndata inspection/retrieval\nsubsetting for further analysis/manipulation\ndata display"
  },
  {
    "objectID": "slides/week2-transform.html#data-display",
    "href": "slides/week2-transform.html#data-display",
    "title": "Dataframe Transformations",
    "section": "Data display",
    "text": "Data display\nRecall the UN Development data:\n\n# preview UN data -- note indexed by country\nundev.head(3)\n\n\n\n\n\n\n\n\ntotal_pop\nurban_pct_pop\npop_under5\npop_15to64\npop_over65\ngender_inequality\nparliament_pct_women\nlabor_participation_women\nlabor_participation_men\n\n\ncountry\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfghanistan\n38.0\n25.8\n5.6\n20.9\n1.0\n0.655\n27.2\n21.6\n74.7\n\n\nAlbania\n2.9\n61.2\n0.2\n2.0\n0.4\n0.181\n29.5\n46.7\n64.6\n\n\nAlgeria\n43.1\n73.2\n5.0\n27.1\n2.8\n0.429\n21.5\n14.6\n67.4\n\n\n\n\n\n\n\n\nAside: .head() is a slicing operation – it returns the ‘top’ slice of rows."
  },
  {
    "objectID": "slides/week2-transform.html#data-inspectionretrieval",
    "href": "slides/week2-transform.html#data-inspectionretrieval",
    "title": "Dataframe Transformations",
    "section": "Data inspection/retrieval",
    "text": "Data inspection/retrieval\nTo inspect the percentage of women in parliament in Mexico, slice accordingly:\n\n\nundev.loc[['Mexico'], ['parliament_pct_women']]\n\n\n\n\n\n\n\n\nparliament_pct_women\n\n\ncountry\n\n\n\n\n\nMexico\n48.4"
  },
  {
    "objectID": "slides/week2-transform.html#review-.loc-and-.iloc",
    "href": "slides/week2-transform.html#review-.loc-and-.iloc",
    "title": "Dataframe Transformations",
    "section": "Review: .loc and .iloc",
    "text": "Review: .loc and .iloc\nThe primary slicing functions in pandas are\n\n.loc (location) to slice by index\n.iloc (integer location) to slice by position\n\n\n\n# .iloc equivalent of previous slice\nundev.iloc[[111], [6]]\n\n\n\n\n\n\n\n\nparliament_pct_women\n\n\ncountry\n\n\n\n\n\nMexico\n48.4\n\n\n\n\n\n\n\n\n\nCheck your understanding: which row in the dataframe is the observation for Mexico?\n\n\nIf a single index rather than a list is provided – e.g., Mexico rather than [Mexico], – these functions will return the raw value as a float rather than a dataframe.\n\nundev.loc['Mexico', 'parliament_pct_women']\n\n48.4"
  },
  {
    "objectID": "slides/week2-transform.html#larger-slices",
    "href": "slides/week2-transform.html#larger-slices",
    "title": "Dataframe Transformations",
    "section": "Larger slices",
    "text": "Larger slices\nMore typically, a slice will be a contiguous chunk of rows and columns.\n\nSlicing operations can interpret start:end as shorthand for a range of indices.\n\nundev.loc['Mexico':'Mongolia', ['parliament_pct_women']]\n\n\n\n\n\n\n\n\nparliament_pct_women\n\n\ncountry\n\n\n\n\n\nMexico\n48.4\n\n\nMicronesia (Federated States of)\n0.0\n\n\nMoldova (Republic of)\n25.7\n\n\nMongolia\n17.3\n\n\n\n\n\n\n\n\n\nNote: start:end is inclusive of both endpoints with .loc, but not inclusive of the right endpoint with .iloc. Get in the habit of double-checking results."
  },
  {
    "objectID": "slides/week2-transform.html#defining-new-variables",
    "href": "slides/week2-transform.html#defining-new-variables",
    "title": "Dataframe Transformations",
    "section": "Defining new variables",
    "text": "Defining new variables\nVectorization of operations in pandas and numpy make tidy data especially nice to manipulate mathematically. For example:\n\n\nweather2['TRANGE'] = weather2.TMAX - weather2.TMIN\nweather2.loc[0:3, ['TMAX', 'TMIN', 'TRANGE']]\n\n\n\n\n\n\n\n\nTMAX\nTMIN\nTRANGE\n\n\n\n\n0\n65\n37\n28\n\n\n1\n62\n38\n24\n\n\n2\n60\n42\n18\n\n\n3\n72\n43\n29\n\n\n\n\n\n\n\n\n\nThis computes \\(t_{min, i} - t_{max, i}\\) for all observations \\(i = 1, \\dots, n\\).\n\n\nCheck your understanding: express this calculation as a linear algebra arithmetic operation."
  },
  {
    "objectID": "slides/week2-transform.html#your-turn",
    "href": "slides/week2-transform.html#your-turn",
    "title": "Dataframe Transformations",
    "section": "Your turn",
    "text": "Your turn\nLet’s take another example – consider this slice of the undev data:\n\n\n\n\n\n\n\n\n\ntotal_pop\nurban_pct_pop\n\n\ncountry\n\n\n\n\n\n\nAfghanistan\n38.0\n25.8\n\n\nAlbania\n2.9\n61.2\n\n\nAlgeria\n43.1\n73.2\n\n\n\n\n\n\n\n\nWith your neighbor, write a line of code that calculates the percentage of the population living in rural areas."
  },
  {
    "objectID": "slides/week2-transform.html#filtering",
    "href": "slides/week2-transform.html#filtering",
    "title": "Dataframe Transformations",
    "section": "Filtering",
    "text": "Filtering\nFiltering refers to removing a subset of rows based on one or more conditions. (Think of “filtering out” certain rows.)\n\nFor example, suppose we wanted to retrieve only the countries with populations exceeding 1Bn people:\n\nundev[undev.total_pop &gt; 1000]\n\n\n\n\n\n\n\n\ntotal_pop\nurban_pct_pop\npop_under5\npop_15to64\npop_over65\ngender_inequality\nparliament_pct_women\nlabor_participation_women\nlabor_participation_men\n\n\ncountry\n\n\n\n\n\n\n\n\n\n\n\n\n\nChina\n1433.8\n60.3\n85.0\n1014.0\n164.5\n0.168\n24.9\n60.5\n75.3\n\n\nIndia\n1366.4\n34.5\n116.8\n915.6\n87.1\n0.488\n13.5\n20.5\n76.1"
  },
  {
    "objectID": "slides/week2-transform.html#filtering-1",
    "href": "slides/week2-transform.html#filtering-1",
    "title": "Dataframe Transformations",
    "section": "Filtering",
    "text": "Filtering\nTechnically, filtering works by slicing according to a long logical vector with one entry per row specifying whether to retain (True) or drop (False).\n\nundev.total_pop &gt; 1000\n\ncountry\nAfghanistan                           False\nAlbania                               False\nAlgeria                               False\nAndorra                               False\nAngola                                False\n                                      ...  \nVenezuela (Bolivarian Republic of)    False\nViet Nam                              False\nYemen                                 False\nZambia                                False\nZimbabwe                              False\nName: total_pop, Length: 189, dtype: bool"
  },
  {
    "objectID": "slides/week2-transform.html#a-small-puzzle",
    "href": "slides/week2-transform.html#a-small-puzzle",
    "title": "Dataframe Transformations",
    "section": "A small puzzle",
    "text": "A small puzzle\nConsider a random filter:\n\nrandom_filter = np.random.binomial(n = 1, p = 0.03, size = undev.shape[0]).astype('bool')\n\nrandom_filter\n\narray([False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False,  True, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False,  True, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False,  True, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n        True, False, False, False, False, False, False, False, False,\n       False,  True, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False,  True, False, False, False, False, False])\n\n\n\n\nHow many rows will undev[random_filter] have?\nHow many rows should this random filtering produce on average?"
  },
  {
    "objectID": "slides/week2-transform.html#logical-comparisons",
    "href": "slides/week2-transform.html#logical-comparisons",
    "title": "Dataframe Transformations",
    "section": "Logical comparisons",
    "text": "Logical comparisons\nAny of the following relations can be used to define filtering conditions\n\n\n\nSymbol\nUsage\nMeaning\n\n\n\n\n==\na == b\nDoes a equal b?\n\n\n&lt;=\na &lt;= b\nIs a less than or equal to b?\n\n\n&gt;=\na &gt;= b\nIs a greater than or equal to b?\n\n\n&lt;\na &lt; b\nIs a less than b?\n\n\n&gt;\na &gt; b\nIs a greater than b?\n\n\n~\n~p\nReturns negation of p\n\n\n|\np | q\np OR q\n\n\n&\np & q\np AND q\n\n\n^\np ^ q\np XOR q (exclusive or)"
  },
  {
    "objectID": "slides/week2-transform.html#aggregation",
    "href": "slides/week2-transform.html#aggregation",
    "title": "Dataframe Transformations",
    "section": "Aggregation",
    "text": "Aggregation\nAggregation refers to any operation that combines many values into fewer values.\n\nCommon aggregation operations include:\n\nsummation \\(\\sum_{i} x_i\\)\naveraging \\(n^{-1} \\sum_i x_i\\)\nextrema \\(\\text{min}_i x_i\\) and \\(\\text{max}_i x_i\\)\nstatistics: median, variance, standard deviation, mean absolute deviation, order statistics, quantiles"
  },
  {
    "objectID": "slides/week2-transform.html#aggregation-vs.-other-transformations",
    "href": "slides/week2-transform.html#aggregation-vs.-other-transformations",
    "title": "Dataframe Transformations",
    "section": "Aggregation vs. other transformations",
    "text": "Aggregation vs. other transformations\nAggregations reduce the number of values, whereas other transformations do not.\n\nA bit more formally:\n\naggregations map larger sets of values to smaller sets of values\ntransformations map sets of values to sets of the same size\n\n\n\nCheck your understanding:\n\nis \\((f*g)(x_i) = \\int f(h)g(x_i - h)dh\\) an aggregation?\nis \\(f(x_1, x_2, \\dots, x_n) = \\left(\\prod_i x_i\\right)^{\\frac{1}{n}}\\) an aggregation?"
  },
  {
    "objectID": "slides/week2-transform.html#aggregation-1",
    "href": "slides/week2-transform.html#aggregation-1",
    "title": "Dataframe Transformations",
    "section": "Aggregation?",
    "text": "Aggregation?\n\nGaussian blur."
  },
  {
    "objectID": "slides/week2-transform.html#example-aggregations",
    "href": "slides/week2-transform.html#example-aggregations",
    "title": "Dataframe Transformations",
    "section": "Example aggregations",
    "text": "Example aggregations\nIn numpy, the most common aggregations are implemented as functions:\n\n\n\nnumpy\nfunction\n\n\n\n\nnp.sum()\n\\(\\sum_i x_i\\)\n\n\nnp.max()\n\\(\\text{max}(x_1, \\dots, x_n)\\)\n\n\nnp.min()\n\\(\\text{min}(x_1, \\dots, x_n)\\)\n\n\nnp.median()\n\\(\\text{median}(x_1, \\dots, x_n)\\)\n\n\nnp.mean()\n\\(n^{-1}\\sum_{i = 1}^n x_i\\)\n\n\nnp.var()\n\\((n - 1)^{-1}\\sum_{i = 1}^n (x_i - \\bar{x})^2\\)\n\n\nnp.std()\n\\(\\sqrt{(n - 1)^{-1}\\sum_{i = 1}^n (x_i - \\bar{x})^2}\\)\n\n\nnp.prod()\n\\(\\prod_i x_i\\)\n\n\nnp.percentile()\n\\(\\hat{F}^{-1}(q)\\)"
  },
  {
    "objectID": "slides/week2-transform.html#argmin-and-argmax",
    "href": "slides/week2-transform.html#argmin-and-argmax",
    "title": "Dataframe Transformations",
    "section": "Argmin and argmax",
    "text": "Argmin and argmax\n\\(\\text{argmax}_D f(x)\\) refers to the value or values in the domain \\(D\\) of \\(f\\) at which the function attains its maximum – the argument in \\(D\\) maximizing \\(f\\).\n\nSimilarly, \\(\\text{argmax}_i x_i\\) refers to the index (or indices, if ties) of the largest value in the set \\(\\{x_i\\}\\).\n\n\nCheck your understanding: what does the following return?\n\nnp.array([1, 5, 10, 2]).argmin()"
  },
  {
    "objectID": "slides/week2-transform.html#argmin-and-argmax-1",
    "href": "slides/week2-transform.html#argmin-and-argmax-1",
    "title": "Dataframe Transformations",
    "section": "Argmin and argmax",
    "text": "Argmin and argmax\nThese index retrieval functions can be handy for slicing rows of interest.\n\nFor example, which country had the largest percentage of women in parliament in the year the UN development data was collected?\n\n\n\nundev.index[undev.parliament_pct_women.argmax()]\n\n'Rwanda'\n\n\n\n\nAnd what were the observations?\n\n\n\nundev.iloc[undev.parliament_pct_women.argmax(), :]\n\ntotal_pop                    12.600\nurban_pct_pop                17.300\npop_under5                    1.800\npop_15to64                    7.200\npop_over65                    0.400\ngender_inequality             0.402\nparliament_pct_women         55.700\nlabor_participation_women    83.900\nlabor_participation_men      83.400\nName: Rwanda, dtype: float64"
  },
  {
    "objectID": "slides/week2-transform.html#dataframe-aggregations",
    "href": "slides/week2-transform.html#dataframe-aggregations",
    "title": "Dataframe Transformations",
    "section": "Dataframe aggregations",
    "text": "Dataframe aggregations\nIn pandas, the numpy aggregation operations are available as dataframe methods that apply the corresponding operation over each column:\n\n# mean of every column\nundev.mean()\n\ntotal_pop                    40.423810\nurban_pct_pop                58.660847\npop_under5                    3.666120\npop_15to64                   27.250820\npop_over65                    3.797814\ngender_inequality             0.344154\nparliament_pct_women         23.093048\nlabor_participation_women    52.139888\nlabor_participation_men      72.470787\ndtype: float64"
  },
  {
    "objectID": "slides/week2-transform.html#row-wise-aggregation",
    "href": "slides/week2-transform.html#row-wise-aggregation",
    "title": "Dataframe Transformations",
    "section": "Row-wise aggregation",
    "text": "Row-wise aggregation\nIn general, supplying the argument axis = 1 will compute rowwise aggregations. For example:\n\n# sum `pop_under5`, `pop_15to64`, and `pop_over65`\nundev.iloc[:, 2:5].sum(axis = 1).head(3)\n\ncountry\nAfghanistan    27.5\nAlbania         2.6\nAlgeria        34.9\ndtype: float64\n\n\n\nThis facilitates, for example:\n\nundev['pop_5to14'] = undev.total_pop - undev.iloc[:, 2:5].sum(axis = 1)"
  },
  {
    "objectID": "slides/week2-transform.html#argminidxmin-and-argmaxidxmax",
    "href": "slides/week2-transform.html#argminidxmin-and-argmaxidxmax",
    "title": "Dataframe Transformations",
    "section": "Argmin/idxmin and argmax/idxmax",
    "text": "Argmin/idxmin and argmax/idxmax\nIn pandas, np.argmin() and np.argmax() are implemented as pd.df.idxmin() and pd.df.idxmax().\n\nundev.idxmax()\n\ntotal_pop                                     China\nurban_pct_pop                Hong Kong, China (SAR)\npop_under5                                    India\npop_15to64                                    China\npop_over65                                    China\ngender_inequality                             Yemen\nparliament_pct_women                         Rwanda\nlabor_participation_women                    Rwanda\nlabor_participation_men                       Qatar\npop_5to14                                     India\ndtype: object"
  },
  {
    "objectID": "slides/week2-transform.html#other-functions",
    "href": "slides/week2-transform.html#other-functions",
    "title": "Dataframe Transformations",
    "section": "Other functions",
    "text": "Other functions\nPandas has a wide array of other aggregation and transformation functions. To show just one example:\n\n## slice weather data\nweather4 = weather1.set_index('DATE').iloc[:, 2:4]\nweather4.head(2)\n\n\n\n\n\n\n\n\nTMAX\nTMIN\n\n\nDATE\n\n\n\n\n\n\n1/1/2021\n65\n37\n\n\n1/2/2021\n62\n38\n\n\n\n\n\n\n\n\n\n# rolling average\nweather4.rolling(window = 7).mean().head(10)\n\n\n\n\n\n\n\n\nTMAX\nTMIN\n\n\nDATE\n\n\n\n\n\n\n1/1/2021\nNaN\nNaN\n\n\n1/2/2021\nNaN\nNaN\n\n\n1/3/2021\nNaN\nNaN\n\n\n1/4/2021\nNaN\nNaN\n\n\n1/5/2021\nNaN\nNaN\n\n\n1/6/2021\nNaN\nNaN\n\n\n1/7/2021\n66.285714\n39.571429\n\n\n1/8/2021\n68.285714\n39.428571\n\n\n1/9/2021\n69.571429\n39.571429\n\n\n1/10/2021\n70.571429\n38.857143"
  },
  {
    "objectID": "slides/week2-transform.html#check-your-understanding",
    "href": "slides/week2-transform.html#check-your-understanding",
    "title": "Dataframe Transformations",
    "section": "Check your understanding",
    "text": "Check your understanding\nInterpret this result:\n\nweather4.rolling(window = 7).mean().idxmax()\n\nTMAX    1/20/2021\nTMIN    3/24/2021\ndtype: object\n\n\n(The weather data is January through March.)"
  },
  {
    "objectID": "slides/week2-transform.html#custom-functions",
    "href": "slides/week2-transform.html#custom-functions",
    "title": "Dataframe Transformations",
    "section": "Custom functions",
    "text": "Custom functions\nSee the documentation for a comprehensive list of transformations and aggregations.\n\nIf pandas doesn’t have a method for an operation you’re wanting to perform, you can implement custom transformations/aggregations with:\n\npd.df.apply() or pd.df.transform() apply a function row-wise or column-wise\npd.df.agg() or pd.df.aggregate()"
  },
  {
    "objectID": "slides/week2-transform.html#custom-functions-1",
    "href": "slides/week2-transform.html#custom-functions-1",
    "title": "Dataframe Transformations",
    "section": "Custom functions",
    "text": "Custom functions\nHere’s an example:\n\n\n\n\n\n\n\n\n\n1961\n1962\n1963\n1964\n1965\n1966\n1967\n1968\n1969\n1970\n...\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n\n\nCountry Name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArgentina\n5.427843\n-0.852022\n-5.308197\n10.130298\n10.569433\n-0.659726\n3.191997\n4.822501\n9.679526\n3.045643\n...\n10.125398\n6.003952\n-1.026420\n2.405324\n-2.512615\n2.731160\n-2.080328\n2.818503\n-2.565352\n-2.088015\n\n\nAustralia\n2.485769\n1.296087\n6.214630\n6.978522\n5.983506\n2.382458\n6.302620\n5.095814\n7.044329\n7.172187\n...\n2.067417\n2.462756\n3.918163\n2.584898\n2.533115\n2.192647\n2.770652\n2.300611\n2.949286\n2.160956\n\n\n\n\n2 rows × 59 columns\n\n\n\n\n\n# convert percentages to proportions\ngdp_prop = gdp.transform(lambda x: x/100 + 1)\n\n# compute geometric mean\ngdp_prop.aggregate(\n    lambda x: np.prod(x)**(1/len(x)), \n    axis = 1).head(4)\n\nCountry Name\nArgentina    1.022831\nAustralia    1.034228\nAustria      1.027254\nBurundi      1.023854\ndtype: float64"
  },
  {
    "objectID": "slides/week2-transform.html#your-turn-1",
    "href": "slides/week2-transform.html#your-turn-1",
    "title": "Dataframe Transformations",
    "section": "Your turn",
    "text": "Your turn\nHere’s the country with the highest annualized GDP growth for the period 1961-2019:\n\n\nCountry Name\nBotswana    1.079442\ndtype: float64\n\n\n\nHow did I find this? Suppose that the result on the previous slide were stored as gdp_annualized. Write a line of code that generates the result shown above."
  },
  {
    "objectID": "slides/week2-transform.html#grouped-aggregations",
    "href": "slides/week2-transform.html#grouped-aggregations",
    "title": "Dataframe Transformations",
    "section": "Grouped aggregations",
    "text": "Grouped aggregations\nSuppose we wanted to compute annualized growth by decade for each country.\n\nTo do so, we’d compute the same aggregation (geometric mean) repeatedly for subsets of data values. This is called a grouped aggregation.\n\n\nUsually, one defines a grouping of dataframe rows using columns in the dataset. For example:\n\ngdp_decades.head(4)\n\n\n\n\n\n\n\n\nCountry Name\ngrowth\ndecade\n\n\n\n\n0\nArgentina\n1.054278\n1960\n\n\n1\nAustralia\n1.024858\n1960\n\n\n2\nAustria\n1.055380\n1960\n\n\n3\nBurundi\n0.862539\n1960\n\n\n\n\n\n\n\n\n\nHow should the rows be grouped?"
  },
  {
    "objectID": "slides/week2-transform.html#groupby",
    "href": "slides/week2-transform.html#groupby",
    "title": "Dataframe Transformations",
    "section": ".groupby",
    "text": ".groupby\nIn pandas, df.groupby('COLUMN') defines a grouping of dataframe rows in which each group is a set of rows with the same value of 'COLUMN'.\n\nThere will be exactly as many groups as the number of unique values in 'COLUMN'.\nMultiple columns may be specified to define a grouping, e.g., df.groupby(['COL1', 'COL2'])\nSubsequent operations will be performed group-wise"
  },
  {
    "objectID": "slides/week2-transform.html#annualized-gdp-growth-by-decade",
    "href": "slides/week2-transform.html#annualized-gdp-growth-by-decade",
    "title": "Dataframe Transformations",
    "section": "Annualized GDP growth by decade",
    "text": "Annualized GDP growth by decade\nReturning to our example:\n\ngdp_anngrowth = gdp_decades.groupby(\n    ['Country Name', 'decade']\n    ).aggregate(\n    lambda x: np.prod(x)**(1/len(x))\n    )\n\ngdp_anngrowth\n\n\n\n\n\n\n\n\n\ngrowth\n\n\nCountry Name\ndecade\n\n\n\n\n\nAlgeria\n1960\n1.030579\n\n\n1970\n1.068009\n\n\n1980\n1.027661\n\n\n1990\n1.015431\n\n\n2000\n1.038750\n\n\n...\n...\n...\n\n\nZimbabwe\n1970\n1.038505\n\n\n1980\n1.051066\n\n\n1990\n1.027630\n\n\n2000\n0.944765\n\n\n2010\n1.055855\n\n\n\n\n714 rows × 1 columns"
  },
  {
    "objectID": "slides/week2-transform.html#your-turn-2",
    "href": "slides/week2-transform.html#your-turn-2",
    "title": "Dataframe Transformations",
    "section": "Your turn",
    "text": "Your turn\nHow do you find the country with the highest annualized GDP growth for each decade?\n\nWrite a line of code that would perform this calculation.\n\ngdp_anngrowth...\n\n\n\n\n\n\n\n\n\n\n\n\ngrowth\n\n\ndecade\n\n\n\n\n\n1960\n(Iran, Islamic Rep., 1960)\n\n\n1970\n(Botswana, 1970)\n\n\n1980\n(Botswana, 1980)\n\n\n1990\n(China, 1990)\n\n\n2000\n(Myanmar, 2000)\n\n\n2010\n(China, 2010)"
  },
  {
    "objectID": "slides/week2-transform.html#recap",
    "href": "slides/week2-transform.html#recap",
    "title": "Dataframe Transformations",
    "section": "Recap",
    "text": "Recap\n\nIn tidy data, rows and columns correspond to observations and variables.\n\nThis provides a standard dataset structure that facilitates exploration and analysis.\nMany datasets are not stored in this format.\nTransformation operations are a lot easier with tidy data, due in part to the way tools in pandas are designed.\n\nTransformations are operations that modify the shape or values of dataframes. We discussed\n\nslicing\nfiltering\ncreating new variables\naggregations (mean, min, max, argmin, etc.)\ngrouped aggregations\n\nDataframe manipulations will be used throughout the course to tidy up data and perform various inspections and summaries."
  },
  {
    "objectID": "slides/week2-transform.html#up-next",
    "href": "slides/week2-transform.html#up-next",
    "title": "Dataframe Transformations",
    "section": "Up next",
    "text": "Up next\nWe started en media res at this stage of the lifecyle (tidy) so that you could start developing skills that would enable you to jump right into playing with datasets.\n\nNext week, we’ll backtrack to the data collection and assessment stages of a project and discuss:\n\nsampling\nscope of inference\ndata assessment\nmissing data"
  },
  {
    "objectID": "slides/week7-lse.html#this-week-the-simple-linear-model",
    "href": "slides/week7-lse.html#this-week-the-simple-linear-model",
    "title": "Modeling concepts; least squares",
    "section": "This week: the simple linear model",
    "text": "This week: the simple linear model\n\nStatistical models\n\nWhat makes a model ‘statistical’?\nGoals of modeling: prediction, description, and inference\nWhen to avoid models\n\nThe simple linear regression model\n\nLine of best fit by least squares\nA model for the error distribution\nThe simple linear model\nInterpretation of estimates\nUncertainty quantification"
  },
  {
    "objectID": "slides/week7-lse.html#what-makes-a-model",
    "href": "slides/week7-lse.html#what-makes-a-model",
    "title": "Modeling concepts; least squares",
    "section": "What makes a model?",
    "text": "What makes a model?\n\nA model is an idealized representation of a system. You likely use models every day. For instance, a weather forecast is [based on] a model.\n\nIn the context of quantitative methods a model is typically a mathematical representation of some system.\n\nQuick discussion:\n\nwhat are some examples of models you’ve encountered?\nin what sense are they models?\nare they statistical models?"
  },
  {
    "objectID": "slides/week7-lse.html#what-makes-a-model-statistical",
    "href": "slides/week7-lse.html#what-makes-a-model-statistical",
    "title": "Modeling concepts; least squares",
    "section": "What makes a model ‘statistical’?",
    "text": "What makes a model ‘statistical’?\nOne straightforward view is that a statistical model is simply a probabilistic representation of a data-generating process. In other words, a probability distribution.\n\nFor a probability distribution to provide a sensible description of a data generating process:\n\none needs to be able to at least imagine collecting multiple datasets with the same basic structure\nobservations must be subject to randomness in some sense\n\n\n\nThat’s why sampling is so important to statisticians. Probabilistic sampling ensures:\n\nthe process by which data are collected is fixed and repeatable\nthe method of selection and measurement of observational units induces randomness in the data"
  },
  {
    "objectID": "slides/week7-lse.html#univariate-models",
    "href": "slides/week7-lse.html#univariate-models",
    "title": "Modeling concepts; least squares",
    "section": "Univariate models",
    "text": "Univariate models\nSuppose you have a dataset comprising the number of meteorites that hit earth on each of 225 days. A very simple model is that the counts are independent Poisson random variables.\n\n\n\n\n\nthe model is parametric, with a single parameter \\(\\lambda\\), given by the Poisson distribution: \\(f(x; \\lambda) = \\lambda^{x} e^{-\\lambda}/x!\\)\nsince \\(\\mathbb{E}X = \\lambda\\) and observations are assumed independent and identically distributed, the model is ‘fitted’ by estimating \\(\\lambda = \\bar{x}\\)"
  },
  {
    "objectID": "slides/week7-lse.html#simple-example",
    "href": "slides/week7-lse.html#simple-example",
    "title": "Modeling concepts; least squares",
    "section": "Simple example",
    "text": "Simple example\nHowever, the negative binomial distribution provides a better ‘fit’:\n\n\nperhaps not surprising, considering it has two parameters"
  },
  {
    "objectID": "slides/week7-lse.html#why-model-the-data-generating-process",
    "href": "slides/week7-lse.html#why-model-the-data-generating-process",
    "title": "Modeling concepts; least squares",
    "section": "Why model the data-generating process?",
    "text": "Why model the data-generating process?\nA good description of a data-generating process usually captures two aspects of a system:\n\nthe deterministic aspects, allowing one to identify structure in the data; and\nthe random aspects or ‘noise’, allowing one to quantify uncertainty.\n\n\nIn our toy example, the better model captured both the most common value (a kind of structure) and the variation (noise)."
  },
  {
    "objectID": "slides/week7-lse.html#modeling-goals",
    "href": "slides/week7-lse.html#modeling-goals",
    "title": "Modeling concepts; least squares",
    "section": "Modeling goals",
    "text": "Modeling goals\nModels serve one of three main purposes:\n\nPrediction: predict new data before it is observed.\nInference: make conclusions about a larger population than the observed data.\nDescription: less common, but sometimes models provide a convenient description of observed data.\n\n\nWe probably wouldn’t use a univariate model to make specific predictions, but we could for instance estimate the probability that over 40 meteorites (rarely observed) hit earth any given day."
  },
  {
    "objectID": "slides/week7-lse.html#models-youve-seen-already",
    "href": "slides/week7-lse.html#models-youve-seen-already",
    "title": "Modeling concepts; least squares",
    "section": "Models you’ve seen already",
    "text": "Models you’ve seen already\nThe exploratory analysis techniques you’ve seen are actually very flexible models often used for descriptive purposes.\n\nKernel density estimates are models for univariate data\nLOESS curves are models for trends in bivariate data\nPrincipal components are models for correlation structures\n\n\nIt’s a little tricky to see, but these correspond to classes of probability distributions rather than specific ones. That’s why they’re so flexible."
  },
  {
    "objectID": "slides/week7-lse.html#its-okay-not-to-model-data",
    "href": "slides/week7-lse.html#its-okay-not-to-model-data",
    "title": "Modeling concepts; least squares",
    "section": "It’s okay not to model data",
    "text": "It’s okay not to model data\nThere are a lot of situations when modeling simply isn’t appropriate or feasible.\nSketchy sampling: every statistical model makes some assumptions about the data collection process. These don’t always need to hold exactly, but models could be untenable if:\n\nthe way data were collected is highly opaque or nonsystematic\nthe sampling design or measurement procedures have serious flaws or inconsistencies\n\n\nSparse data: model fitting is sensitive to the specific dataset observed, and may be unreliable if it’s too sensitive. This commonly arises when:\n\nthe model has a lot of parameters\nthe dataset contains few observations (relative to the number of parameters)"
  },
  {
    "objectID": "slides/week7-lse.html#comment",
    "href": "slides/week7-lse.html#comment",
    "title": "Modeling concepts; least squares",
    "section": "Comment",
    "text": "Comment\n\nThese univariate models usually don’t occur to us as models in the fullest sense, because:\n\nNo deterministic structure\nAll variation is random\n\n\nFor this reason you can’t do much with them, so they seem a bit uninteresting."
  },
  {
    "objectID": "slides/week7-lse.html#constructing-more-interesting-models",
    "href": "slides/week7-lse.html#constructing-more-interesting-models",
    "title": "Modeling concepts; least squares",
    "section": "Constructing more interesting models",
    "text": "Constructing more interesting models\nMany models also involve an interesting deterministic component.\n\nA frequent strategy is to form a regression model:\n\nassume a distributional form for a quantity of interest\n‘regress’ the mean or other parameters of the distribution ‘on’ other variables – i.e., express the former as a function of the latter\n\n\n\nFor instance:\n\n\\(\\#\\text{meteors each day} \\sim \\text{poisson}(\\lambda)\\)\n\\(\\lambda = g(\\text{month})\\)"
  },
  {
    "objectID": "slides/week7-lse.html#linear-models",
    "href": "slides/week7-lse.html#linear-models",
    "title": "Modeling concepts; least squares",
    "section": "Linear models",
    "text": "Linear models\nA linear regression model is one in which the deterministic component is linear: the mean of a variable of interest is a linear function of one or more other variables.\nFor instance:\n\nYou may not have realized it at the time, but those lines are simple linear regression models: they describe the mean gaps as linear functions of log median income."
  },
  {
    "objectID": "slides/week7-lse.html#remarks-on-terminology",
    "href": "slides/week7-lse.html#remarks-on-terminology",
    "title": "Modeling concepts; least squares",
    "section": "Remarks on terminology",
    "text": "Remarks on terminology\n\nA linear regression model is simple if it involves only one variable of interest \\(Y\\) and one additional variable \\(X\\)\n\n\\(Y\\) is the ‘response’, ‘dependent’, or ‘endogenous’ variable\n\\(X\\) is the ‘explanatory’, ‘independent’, or ‘exogenous’ variable\n\nThe multiple linear regression model involves multiple explanatory variables \\(X_1, X_2, \\dots\\)\nMultivariate linear regression models involve multiple responses \\(Y_1, Y_2, \\dots\\)\n“Linear regression model” is usually taken to mean a model in which \\(Y\\) is assumed to be normal, given \\(X\\)"
  },
  {
    "objectID": "slides/week7-lse.html#applications-of-linear-models",
    "href": "slides/week7-lse.html#applications-of-linear-models",
    "title": "Modeling concepts; least squares",
    "section": "Applications of linear models",
    "text": "Applications of linear models\nLinear models can be used for prediction, inference, or both.\n\nPredict the gender achievement gaps for a new district based on median income in the district.\nQuantify the association between median income and achievement gaps.\n\n\nWe’re going to talk in detail about the model itself:\n\ndefinition\nestimation\nassumptions"
  },
  {
    "objectID": "slides/week7-lse.html#data-setting",
    "href": "slides/week7-lse.html#data-setting",
    "title": "Modeling concepts; least squares",
    "section": "Data setting",
    "text": "Data setting\nLet’s first introduce the kind of data that the simple linear model describes.\n\nThere are two variables, \\(X\\) and \\(Y\\).\nThe data values are \\(n\\) observations of these two variables: \\[\n(x_1, y_1), \\dots, (x_n, y_n)\n\\]\n\n\nThe notation in tuples indicates the pairing of the values when measured on the same observational unit. If it helps, think of them as rows of an \\(n\\times 2\\) dataframe:\n\n\n\n\\(X\\)\n\\(Y\\)\n\n\n\n\n\\(x_1\\)\n\\(y_1\\)\n\n\n\\(x_2\\)\n\\(y_2\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(x_n\\)\n\\(y_n\\)"
  },
  {
    "objectID": "slides/week7-lse.html#data-setting-1",
    "href": "slides/week7-lse.html#data-setting-1",
    "title": "Modeling concepts; least squares",
    "section": "Data setting",
    "text": "Data setting\nThe notation above is just a mathematical description of data that looks like this:\n\nIn our notation, \\(X\\) would represent log median income, and \\(Y\\) would represent the math gap."
  },
  {
    "objectID": "slides/week7-lse.html#data-setting-2",
    "href": "slides/week7-lse.html#data-setting-2",
    "title": "Modeling concepts; least squares",
    "section": "Data setting",
    "text": "Data setting\nThe example SEDA data in tabular form are:\n\n\nCode\n# import grade-aggregated seda data from hw2\nseda = pd.read_csv('data/seda.csv')\n\n# filter to math and remove NaNs\nregdata = seda[seda.subject == 'math'].dropna().drop(columns = 'subject').set_index('id')\n\n# dimensions\nn, p = regdata.shape\n\n# print\nregdata.head(3)\n\n\n\n\n\n\n\n\n\nlog_income\ngap\n\n\nid\n\n\n\n\n\n\n600001\n11.392048\n-0.562855\n\n\n600006\n11.607236\n0.061163\n\n\n600011\n10.704570\n-0.015417\n\n\n\n\n\n\n\n\nThe tuples would be: \\[\n(\\text{log_income}_1, \\text{gap}_1)\\;,\\; (\\text{log_income}_2, \\text{gap}_2)\\;,\\; \\dots\\;,\\; (\\text{log_income}_{625}, \\text{gap}_{625})\n\\]\n\n\nOr more specifically: \\[\n(11.392, -0.563)\\;,\\; (11.607, 0.061)\\;,\\; \\dots\\;,\\; (11.229, -0.040)\n\\]"
  },
  {
    "objectID": "slides/week7-lse.html#lines-and-data",
    "href": "slides/week7-lse.html#lines-and-data",
    "title": "Modeling concepts; least squares",
    "section": "Lines and data",
    "text": "Lines and data\nA line in slope-intercept form is given by the equation: \\[\ny = ax + b\n\\]\n\nData values never fall exactly on a line. So in general for every \\(a, b\\): \\[\ny_i \\neq a x_i + b\n\\]\n\n\nBut we can describe any dataset as a line and a ‘residual’: \\[\ny_i = \\underbrace{a x_i + b}_\\text{line} \\underbrace{+\\;e_i}_\\text{residual}\n\\]"
  },
  {
    "objectID": "slides/week7-lse.html#lines-and-data-1",
    "href": "slides/week7-lse.html#lines-and-data-1",
    "title": "Modeling concepts; least squares",
    "section": "Lines and data",
    "text": "Lines and data\nHere’s a picture:\n\n\nEach residual is simply the vertical distance of a value of \\(Y\\) from the line: \\[\n\\color{grey}{e_i} = \\color{blue}{y_i} - \\color{red}{(a x_i + b)}\n\\]"
  },
  {
    "objectID": "slides/week7-lse.html#many-possible-lines",
    "href": "slides/week7-lse.html#many-possible-lines",
    "title": "Modeling concepts; least squares",
    "section": "Many possible lines",
    "text": "Many possible lines\nThis makes it possible to express \\(Y\\) as a linear function of \\(X\\).\n\nHowever, the mathematical description is somewhat tautological, since for any \\(a, b\\), there are residuals \\(e_1, \\dots, e_n\\) such that \\[\ny_i = a x_i + b + e_i\n\\]\n\n\nIn other words, there are infinitely many possible lines. So which values of \\(a\\) and \\(b\\) should be chosen for a given set of data values?"
  },
  {
    "objectID": "slides/week7-lse.html#the-least-squares-line",
    "href": "slides/week7-lse.html#the-least-squares-line",
    "title": "Modeling concepts; least squares",
    "section": "The least squares line",
    "text": "The least squares line\nA sensible criterion is to find the line for which:\n\nthe average residual \\(\\bar{e}\\) is zero; and\nthe residuals vary the least.\n\n\nIf \\(\\bar{e} = 0\\), then the residual variance is proportional to the sum of squared residuals: \\[\n\\frac{1}{n - 1}\\sum_{i = 1}^n (e_i - \\bar{e})^2 = \\frac{1}{n - 1}\\sum_{i = 1}^n e_i^2\n\\]\n\n\nSo the values of \\(a, b\\) that minimize \\(\\sum_i e_i\\) give the ‘best’ line (in one sense of the word ‘best’). This method is known as least squares. \\[\n(a^*, b^*) = \\arg\\min_{(a, b)}\\left\\{\\sum_{i = 1}^n \\underbrace{\\left(y_i - (a x_i + b)\\right)^2}_{e_i^2}\\right\\}\n\\]"
  },
  {
    "objectID": "slides/week7-lse.html#deriving-the-least-squares-line",
    "href": "slides/week7-lse.html#deriving-the-least-squares-line",
    "title": "Modeling concepts; least squares",
    "section": "Deriving the least squares line",
    "text": "Deriving the least squares line\nLet’s try doing the derivation using univariate calculus. Note: \\(e_i = y_i - a - b x_i\\).\n\\[\n\\frac{d}{da} \\sum_i e_i^2 = \\cdots \\hskip{7in}\n\\]\n\\[\n\\frac{d}{db} \\sum_i e_i^2 = \\cdots \\hskip{7in}\n\\]\nIf \\(\\frac{d}{da} \\sum_i e_i^2 = 0\\) and \\(\\frac{d}{db} \\sum_i e_i^2 = 0\\) then…"
  },
  {
    "objectID": "slides/week7-lse.html#deriving-the-least-squares-line-1",
    "href": "slides/week7-lse.html#deriving-the-least-squares-line-1",
    "title": "Modeling concepts; least squares",
    "section": "Deriving the least squares line",
    "text": "Deriving the least squares line\nAlternatively, the model can be written in matrix form as \\(\\mathbf{y} = \\mathbf{Xa} + \\mathbf{e}\\), where:\n\\[\n\\underbrace{\\left[\\begin{array}{c} y_1 \\\\\\vdots\\\\ y_n \\end{array}\\right]}_{\\mathbf{y}}\n    = \\underbrace{\\left[\\begin{array}{cc}\n        1 & x_1 \\\\\n        \\vdots & \\vdots \\\\\n        1 & x_n\n        \\end{array}\\right]}_{\\mathbf{X}}\n      \\underbrace{\\left[\\begin{array}{c} a \\\\ b \\end{array}\\right]}_{\\mathbf{a}}\n      + \\underbrace{\\left[\\begin{array}{c} e_1 \\\\\\vdots\\\\ e_n \\end{array}\\right]}_{\\mathbf{e}}\n\\]\n\nThen, the sum of squared residuals is: \\[\n\\mathbf{e'e} = (\\mathbf{y} - \\mathbf{Xa})'(\\mathbf{y} - \\mathbf{Xa})\n\\]\n\n\nUsing vector calculus, one can show that: \\[\n\\nabla_\\mathbf{a} \\mathbf{e'e} = 0 \\quad\\Longrightarrow\\quad 2\\mathbf{X'y} = 2\\mathbf{X'Xa} \\quad\\Longrightarrow\\quad \\mathbf{a} = (\\mathbf{X'X})^{-1}\\mathbf{X'y}\n\\] And that this is a minimum."
  },
  {
    "objectID": "slides/week7-lse.html#calculating-the-least-squares-line",
    "href": "slides/week7-lse.html#calculating-the-least-squares-line",
    "title": "Modeling concepts; least squares",
    "section": "Calculating the least squares line",
    "text": "Calculating the least squares line\nThe solution \\(\\mathbf{a} = (\\mathbf{X'X})^{-1}\\mathbf{X'y}\\) is known as the ordinarly least squares (OLS) estimate.\nThe sklearn implementation looks like this:\n\n# save explanatory variable and response variable separately as arrays\nx = regdata.log_income.values[:, np.newaxis]\ny = regdata.gap.values\n\n# configure regression module and fit model\nslr = LinearRegression()\nslr.fit(x, y)\n\n# store estimates\nprint('slope: ', slr.coef_[0])\nprint('intercept: ', slr.intercept_)\n\nslope:  0.12105696076155249\nintercept:  -1.3561699570330037"
  },
  {
    "objectID": "slides/week7-lse.html#calculating-the-least-squares-line-1",
    "href": "slides/week7-lse.html#calculating-the-least-squares-line-1",
    "title": "Modeling concepts; least squares",
    "section": "Calculating the least squares line",
    "text": "Calculating the least squares line\nThe solution \\(\\mathbf{a} = (\\mathbf{X'X})^{-1}\\mathbf{X'y}\\) is known as the ordinarly least squares (OLS) estimate.\nThe statsmodels implementation looks like this:\n\n# add a column of ones (intercept)\nx_aug = sm.tools.add_constant(x)\n\n# fit model\nslr = sm.OLS(endog = y, exog = x_aug)\nslr_fit = slr.fit()\n\n# return estimates\nprint('estimates: ', slr_fit.params)\n\nestimates:  [-1.35616996  0.12105696]\n\n\n\nNote a constant has to be added to x. Why?"
  },
  {
    "objectID": "slides/week7-lse.html#calculating-the-least-squares-line-2",
    "href": "slides/week7-lse.html#calculating-the-least-squares-line-2",
    "title": "Modeling concepts; least squares",
    "section": "Calculating the least squares line",
    "text": "Calculating the least squares line\nAlternatively, statsmodels also has a wrapper around sm.OLS that allows models to be fit based on a dataframe and column names (much like lm() in R):\n\nimport statsmodels.formula.api as smf\n\n# fit model based on dataframe and formula\nslr = smf.ols(formula = 'gap ~ log_income', data = regdata)\nslr_fit = slr.fit()\n\n# return estimates\nslr_fit.params\n\nIntercept    -1.356170\nlog_income    0.121057\ndtype: float64"
  },
  {
    "objectID": "slides/week7-lse.html#calculating-the-least-squares-line-3",
    "href": "slides/week7-lse.html#calculating-the-least-squares-line-3",
    "title": "Modeling concepts; least squares",
    "section": "Calculating the least squares line",
    "text": "Calculating the least squares line\nWe can check the calculations by computing the closed-form expression:\n\n# ols solution, by hand\nx_mx = np.vstack([np.repeat(1, len(x)), x[:, 0]]).transpose() # X\nxtx = x_mx.transpose().dot(x_mx) # X'X\nxtx_inv = np.linalg.inv(xtx) # (X'X)^{-1}\nxtx_inv.dot(x_mx.transpose()).dot(y) # (X'X)^{-1} X'y\n\narray([-1.35616996,  0.12105696])"
  },
  {
    "objectID": "slides/votes-preprocessing.html",
    "href": "slides/votes-preprocessing.html",
    "title": "PSTAT100",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n\nImport raw data tables\n\nmembers = pd.read_csv('data/members.csv')\nvotes = pd.read_csv('data/rollcall-votes.csv')\ninfo = pd.read_csv('data/rollcall-info.csv')\nbills = pd.read_csv('data/bills.csv')\n\nVote types of interest\n\nmotions_to_agree_or_pass =['On Agreeing to the Resolution', \n'On Passage', \n'On Motion to Suspend the Rules and Pass', \n'On Motion to Suspend the Rules and Agree',\n'On Motion to Suspend the Rules and Agree, as Amended',\n'On Motion to Suspend the Rules and Pass, as Amended',\n'On Passage, Objections of the President to the Contrary Notwithstanding']\n\nrollcalls_of_interest = [index for index, row in info.iterrows() if  (row['question'] in motions_to_agree_or_pass) ]\ninfo_sub = info.iloc[rollcalls_of_interest, :]\n\n\nbill info bills and roll call info info are matched on bill_id\ninfo is matched with vote on rollcall_id\n\n\nbills.head(2)\n\n\n\n\n\n\n\n\nbill_id\ntitle\nsponsor\ncosponsors\nrelated_bills\npolicy_area\nsubjects\ncommittees\nbill_progress\nsummary\ndate_introduced\nnumber\nbill_type\n\n\n\n\n0\nH.R.1\nTo expand Americans' access to the ballot box,...\nS001168\n['P000197', 'A000370', 'A000376', 'B001300', '...\n['H.R.44', 'H.R.93', 'H.R.137', 'H.R.138', 'H....\nGovernment Operations and Politics\n['Administrative law and regulatory procedures...\n['House Administration', 'House Intelligence (...\nPassed House\nThis bill addresses voter access, election int...\n2019-01-03\n1\nH.R.\n\n\n1\nH.R.3\nTo establish a fair price negotiation program,...\nP000034\n['N000015', 'S000185', 'L000557', 'K000382', '...\n['H.R.4619', 'H.R.4649', 'H8607', 'H.R.4663']\nHealth\n[]\n['House Energy and Commerce', 'House Ways and ...\nIntroduced\nThis bill establishes several programs and req...\n2019-09-19\n3\nH.R.\n\n\n\n\n\n\n\n\ninfo_sub.head(2)\n\n\n\n\n\n\n\n\nrollcall_id\nroll_num\ndate\nbill_id\nquestion\nresult\ndescription\nyear\ncongress\nsession\n\n\n\n\n3\n2019:006\n6\n3-Jan\nH.RES.5\nOn Agreeing to the Resolution\nP\nProviding for consideration of H.Res. 6, adopt...\n2019\n116\n1\n\n\n6\n2019:009\n9\n3-Jan\nH.J.RES.1\nOn Passage\nP\nMaking further continuing appropriations for t...\n2019\n116\n1\n\n\n\n\n\n\n\nadd roll call id to bill id\n\nbills_aug = pd.merge(\n    bills, \n    info_sub.loc[:, ['rollcall_id', 'bill_id']], \n    on = 'bill_id', \n    how = 'inner')\n\nnumber of votes of potential interest\n\nbills_aug.shape\n\n(144, 14)\n\n\nexamine bill types\n\nbills_aug.policy_area.value_counts()\n\nFinance and Financial Sector                   15\nGovernment Operations and Politics             12\nInternational Affairs                          12\nEconomics and Public Finance                   11\nArmed Forces and National Security             11\nEmergency Management                           10\nCrime and Law Enforcement                       9\nCommerce                                        8\nHealth                                          7\nImmigration                                     7\nPublic Lands and Natural Resources              5\nLabor and Employment                            5\nNative Americans                                5\nTransportation and Public Works                 4\nScience, Technology, Communications             4\nEnvironmental Protection                        4\nHousing and Community Development               3\nAnimals                                         2\nTaxation                                        2\nLaw                                             2\nEnergy                                          2\nCivil Rights and Liberties, Minority Issues     1\nEducation                                       1\nSocial Welfare                                  1\nForeign Trade and International Finance         1\nName: policy_area, dtype: int64\n\n\n\nvotes_clean = votes.set_index('name_id')[bills_aug.rollcall_id.values]\n\n\nvotes_clean.to_csv('data/votes-clean.csv', index = True)\nbills_aug.to_csv('data/votes-info.csv', index = True)"
  },
  {
    "objectID": "slides/data/air-preprocessing.html",
    "href": "slides/data/air-preprocessing.html",
    "title": "PSTAT100",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n\nair = pd.read_csv('air-quality.csv').fillna(\n    axis = 0, \n    method = 'ffill'\n).drop(\n    columns = ['Number of Trends Sites', 'CBSA']\n).melt(\n    id_vars = ['Core Based Statistical Area', 'Pollutant', 'Trend Statistic'],\n    var_name = 'Year',\n    value_name = 'Measurement'\n).pivot_table(\n    index = ['Core Based Statistical Area', 'Year'],\n    columns = ['Pollutant', 'Trend Statistic'],\n    values = 'Measurement'\n).reset_index()\n\nair['State'] = air['Core Based Statistical Area'].str.split(pat = ',', expand = True).loc[:, 1]\nair['City'] = air['Core Based Statistical Area'].str.split(pat = ',', expand = True).loc[:, 0]\n\nair.columns = air.columns.get_level_values(level = 0)\n\nair_tidy = air.rename_axis(columns = '').drop(columns = 'Core Based Statistical Area')\n\nair_tidy.to_csv('air.csv', index = False)\n\n\nair_tidy.head()\n\n\n\n\n\n\n\n\nYear\nCO\nNO2\nNO2\nO3\nPM10\nPM2.5\nPM2.5\nPb\nSO2\nState\nCity\n\n\n\n\n0\n2000\nNaN\nNaN\nNaN\nNaN\n50.0\n23.0\n8.6\nNaN\nNaN\nSD\nAberdeen\n\n\n1\n2001\nNaN\nNaN\nNaN\nNaN\n58.0\n23.0\n8.6\nNaN\nNaN\nSD\nAberdeen\n\n\n2\n2002\nNaN\nNaN\nNaN\nNaN\n59.0\n20.0\n7.9\nNaN\nNaN\nSD\nAberdeen\n\n\n3\n2003\nNaN\nNaN\nNaN\nNaN\n66.0\n21.0\n8.4\nNaN\nNaN\nSD\nAberdeen\n\n\n4\n2004\nNaN\nNaN\nNaN\nNaN\n39.0\n23.0\n8.1\nNaN\nNaN\nSD\nAberdeen\n\n\n\n\n\n\n\n\nus_state_abbrev = {\n    'Alabama': 'AL',\n    'Alaska': 'AK',\n    'American Samoa': 'AS',\n    'Arizona': 'AZ',\n    'Arkansas': 'AR',\n    'California': 'CA',\n    'Colorado': 'CO',\n    'Connecticut': 'CT',\n    'Delaware': 'DE',\n    'District of Columbia': 'DC',\n    'Florida': 'FL',\n    'Georgia': 'GA',\n    'Guam': 'GU',\n    'Hawaii': 'HI',\n    'Idaho': 'ID',\n    'Illinois': 'IL',\n    'Indiana': 'IN',\n    'Iowa': 'IA',\n    'Kansas': 'KS',\n    'Kentucky': 'KY',\n    'Louisiana': 'LA',\n    'Maine': 'ME',\n    'Maryland': 'MD',\n    'Massachusetts': 'MA',\n    'Michigan': 'MI',\n    'Minnesota': 'MN',\n    'Mississippi': 'MS',\n    'Missouri': 'MO',\n    'Montana': 'MT',\n    'Nebraska': 'NE',\n    'Nevada': 'NV',\n    'New Hampshire': 'NH',\n    'New Jersey': 'NJ',\n    'New Mexico': 'NM',\n    'New York': 'NY',\n    'North Carolina': 'NC',\n    'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP',\n    'Ohio': 'OH',\n    'Oklahoma': 'OK',\n    'Oregon': 'OR',\n    'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR',\n    'Rhode Island': 'RI',\n    'South Carolina': 'SC',\n    'South Dakota': 'SD',\n    'Tennessee': 'TN',\n    'Texas': 'TX',\n    'Utah': 'UT',\n    'Vermont': 'VT',\n    'Virgin Islands': 'VI',\n    'Virginia': 'VA',\n    'Washington': 'WA',\n    'West Virginia': 'WV',\n    'Wisconsin': 'WI',\n    'Wyoming': 'WY'\n}\n\n# thank you to @kinghelix and @trevormarburger for this idea\nabbrev_us_state = dict(map(reversed, us_state_abbrev.items()))\n\nabbrev_us_state\n\n{'AL': 'Alabama',\n 'AK': 'Alaska',\n 'AS': 'American Samoa',\n 'AZ': 'Arizona',\n 'AR': 'Arkansas',\n 'CA': 'California',\n 'CO': 'Colorado',\n 'CT': 'Connecticut',\n 'DE': 'Delaware',\n 'DC': 'District of Columbia',\n 'FL': 'Florida',\n 'GA': 'Georgia',\n 'GU': 'Guam',\n 'HI': 'Hawaii',\n 'ID': 'Idaho',\n 'IL': 'Illinois',\n 'IN': 'Indiana',\n 'IA': 'Iowa',\n 'KS': 'Kansas',\n 'KY': 'Kentucky',\n 'LA': 'Louisiana',\n 'ME': 'Maine',\n 'MD': 'Maryland',\n 'MA': 'Massachusetts',\n 'MI': 'Michigan',\n 'MN': 'Minnesota',\n 'MS': 'Mississippi',\n 'MO': 'Missouri',\n 'MT': 'Montana',\n 'NE': 'Nebraska',\n 'NV': 'Nevada',\n 'NH': 'New Hampshire',\n 'NJ': 'New Jersey',\n 'NM': 'New Mexico',\n 'NY': 'New York',\n 'NC': 'North Carolina',\n 'ND': 'North Dakota',\n 'MP': 'Northern Mariana Islands',\n 'OH': 'Ohio',\n 'OK': 'Oklahoma',\n 'OR': 'Oregon',\n 'PA': 'Pennsylvania',\n 'PR': 'Puerto Rico',\n 'RI': 'Rhode Island',\n 'SC': 'South Carolina',\n 'SD': 'South Dakota',\n 'TN': 'Tennessee',\n 'TX': 'Texas',\n 'UT': 'Utah',\n 'VT': 'Vermont',\n 'VI': 'Virgin Islands',\n 'VA': 'Virginia',\n 'WA': 'Washington',\n 'WV': 'West Virginia',\n 'WI': 'Wisconsin',\n 'WY': 'Wyoming'}"
  },
  {
    "objectID": "slides/week7-slr.html#from-last-time",
    "href": "slides/week7-slr.html#from-last-time",
    "title": "Simple linear regression",
    "section": "From last time",
    "text": "From last time\n\nstatistical models are probabilistic represenations of data generating processes\n\nsome randomness must be present (usually from sampling) for this to be sensible\n\nregression models relate a variable of interest to one or more other ‘explanatory’ variables and consist of:\n\na distribution for the variable of interest\nexpression(s) for the distribution’s parameters in terms of the explanatory variable(s)\n\nfor linear regression, the mean of the response is a linear function of the explanatory variable(s)\n\n‘simple’ if one explanatory variable\n‘multiple’ if many explanatory variables\n‘multivariate’ if many variables of interest"
  },
  {
    "objectID": "slides/week7-slr.html#from-last-time-1",
    "href": "slides/week7-slr.html#from-last-time-1",
    "title": "Simple linear regression",
    "section": "From last time",
    "text": "From last time\nA simple linear model is \\(y_i = a x_i + b + e_i\\) where \\(e_i\\) are ‘residuals’ – leftover quantities.\n\nThe least squares line minimizes the sum of squared residuals \\(\\sum_i e_i^2\\) – i.e., the residual variance, assuming the line passes through the mean.\n\\[\n\\begin{align*}\nb &= \\frac{S_x}{S_y} \\text{corr}(x, y) \\\\\na &= \\bar{y} - b\\bar{x}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/week7-slr.html#visualizing-the-line",
    "href": "slides/week7-slr.html#visualizing-the-line",
    "title": "Simple linear regression",
    "section": "Visualizing the line",
    "text": "Visualizing the line\nVerifying that the line plotted on a prediction grid (left) matches the regression transform from Altair (right):"
  },
  {
    "objectID": "slides/week7-slr.html#not-a-statistical-model-yet",
    "href": "slides/week7-slr.html#not-a-statistical-model-yet",
    "title": "Simple linear regression",
    "section": "Not a statistical model, yet",
    "text": "Not a statistical model, yet\nThe least squares line is simply an algebraic transformation of the data – technically, a projection.\n\nThis is not yet a statistical model according to our definition, since there is no probability distribution involved.\n\n\nWe can change that by considering the residuals to be random."
  },
  {
    "objectID": "slides/week7-slr.html#residual-distribution",
    "href": "slides/week7-slr.html#residual-distribution",
    "title": "Simple linear regression",
    "section": "Residual distribution",
    "text": "Residual distribution\nHave a look at the histogram of the residuals (with a KDE curve):\n\n\nDoes this look like any probability density function you encountered in 120A?"
  },
  {
    "objectID": "slides/week7-slr.html#residual-distribution-1",
    "href": "slides/week7-slr.html#residual-distribution-1",
    "title": "Simple linear regression",
    "section": "Residual distribution",
    "text": "Residual distribution\nThe residual distribution is pretty well-approximated by the normal or Gaussian distribution:"
  },
  {
    "objectID": "slides/week7-slr.html#the-error-model",
    "href": "slides/week7-slr.html#the-error-model",
    "title": "Simple linear regression",
    "section": "The error model",
    "text": "The error model\nThis phenomenon – nearly normal residuals – is pretty common in practice.\n\nSo a standard distributional model for the residuals is that they are independent normal random variables. This is written as:\n\\[\ne_i \\stackrel{iid}{\\sim} N\\left(0, \\sigma^2\\right)\n\\]\n\n\nThis is an important modification because it induces a probability distribution on \\(y_i\\)."
  },
  {
    "objectID": "slides/week7-slr.html#the-simple-linear-regression-model",
    "href": "slides/week7-slr.html#the-simple-linear-regression-model",
    "title": "Simple linear regression",
    "section": "The simple linear regression model",
    "text": "The simple linear regression model\nNow we’re in a position to state the simple linear regression model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\quad\n\\begin{cases}\ni = 1, \\dots, n \\\\\n\\epsilon_i \\sim N\\left(0,\\sigma^2\\right)\n\\end{cases}\n\\]\n\n\\(y_i\\) is the response variable\n\\(x_i\\) is the explanatory variable\n\\(\\epsilon_i\\) is the random error\n\\(\\beta_0, \\beta_1, \\sigma^2\\) are the model parameters\n\n\\(\\beta_0\\) is the intercept\n\\(\\beta_1\\) is the coefficient\n\\(\\sigma^2\\) is the error variance"
  },
  {
    "objectID": "slides/week7-slr.html#properties-of-the-normal-distribution",
    "href": "slides/week7-slr.html#properties-of-the-normal-distribution",
    "title": "Simple linear regression",
    "section": "Properties of the normal distribution",
    "text": "Properties of the normal distribution\nAs a refresher from 120A, if \\(X \\sim N(\\mu, \\sigma^2)\\) then:\n\n(Mean) \\(\\mathbb{E}X = \\mu\\)\n(Variance) \\(\\text{var}X = \\sigma^2\\)\n(Linearity) For constants \\(a, b\\), \\(aX + b \\sim N(a\\mu + b, a^2\\sigma^2)\\)"
  },
  {
    "objectID": "slides/week7-slr.html#model-implications",
    "href": "slides/week7-slr.html#model-implications",
    "title": "Simple linear regression",
    "section": "Model implications",
    "text": "Model implications\nTreating the error term as random has a number of implications that follow from the properties of the normal distribution:\n\n(Normality) The response is a normal random variable: \\(y_i \\sim N\\left(\\beta_0 + \\beta_1 x_i, \\sigma^2\\right)\\)\n(Linearity) The mean response is linear in \\(X\\): \\(\\mathbb{E}y_i = \\beta_0 + \\beta_1 x_i\\)\n(Constant variance) The response has variance: \\(\\text{var}y_i = \\sigma^2\\)\n(Independence) The observations are independent (because the errors are): \\(y_i \\perp y_j\\)\n\n\nThese are the assumptions of the simple linear regression model.\n\n\nAside: other error distributions, or conditions that don’t assume a specific distribution, are possible."
  },
  {
    "objectID": "slides/week7-slr.html#estimates",
    "href": "slides/week7-slr.html#estimates",
    "title": "Simple linear regression",
    "section": "Estimates",
    "text": "Estimates\nYou’ve already seen how to compute the least squares estimates of \\(\\beta_0, \\beta_1\\) – these are no different when the errors are random.\n\nEstimates are typically denoted by the corresponding paramater with a hat:\n\\[\n\\begin{align*}\n\\hat{\\beta}_1\n    &= \\frac{\n        \\sum_i (x_i - \\bar{x})(y_i - \\bar{y})\n        }{\n        \\sum_i (x_i - \\bar{x})^2\n        } \\\\\n\\hat{\\beta}_0\n    &= \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n\\end{align*}\n\\]\n\n\nAn estimate of the error variance is:\n\\[\n\\hat{\\sigma}^2 = \\frac{1}{n - 2} \\sum_{i = 1}^n \\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i\\right)^2\n\\]"
  },
  {
    "objectID": "slides/week7-slr.html#estimates-matrix-form",
    "href": "slides/week7-slr.html#estimates-matrix-form",
    "title": "Simple linear regression",
    "section": "Estimates (matrix form)",
    "text": "Estimates (matrix form)\nThese estimates can be expressed in matrix form as:\n\\[\n\\begin{align*}\n\\hat{\\beta} &= (\\mathbf{X'X})^{-1}\\mathbf{X'y} \\\\\n\\hat{\\sigma}^2\n    &= \\frac{1}{n - 2}\\left(\\mathbf{y} - \\mathbf{X}\\hat{\\beta}\\right)'\\left(\\mathbf{y} - \\mathbf{X}\\hat{\\beta}\\right)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/week7-slr.html#fitted-values-and-residuals",
    "href": "slides/week7-slr.html#fitted-values-and-residuals",
    "title": "Simple linear regression",
    "section": "Fitted values and residuals",
    "text": "Fitted values and residuals\nThe projections of the data points onto the line are the estimated values of the response variable for each data point.\nThese are known as fitted values and denoted \\(\\hat{y}_i\\):\n\\[\n\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\n\\]\n\nThen, the differences between observed and fitted values give the model residuals:\n\\[\ne_i = y_i - \\hat{y}_i\n\\]"
  },
  {
    "objectID": "slides/week7-slr.html#computations",
    "href": "slides/week7-slr.html#computations",
    "title": "Simple linear regression",
    "section": "Computations",
    "text": "Computations\n\n# retrieve data\nx = sm.tools.add_constant(regdata.log_income.values)\ny = regdata.gap.values\n\n# fit regression model\nslr = sm.OLS(endog = y, exog = x)\n\n# estimates\nbeta_hat = slr.fit().params\n\n# fitted values\nfitted = slr.fit().fittedvalues\n\n# residuals\nresids = slr.fit().resid\n\n# error variance estimate\nsigmasq_hat = slr.fit().scale\n\nprint('coefficient estimates: ', beta_hat)\nprint('error variance estimate: ', sigmasq_hat)\n\ncoefficient estimates:  [-1.35616996  0.12105696]\nerror variance estimate:  0.01317117061613733"
  },
  {
    "objectID": "slides/week7-slr.html#parameter-interpretations-intercept",
    "href": "slides/week7-slr.html#parameter-interpretations-intercept",
    "title": "Simple linear regression",
    "section": "Parameter interpretations: intercept",
    "text": "Parameter interpretations: intercept\nThe intercept \\(\\beta_0\\) represents the mean response \\(\\mathbb{E}y_i\\) when \\(x_i = 0\\).\n\nIn the SEDA example:\n\nFor districts with a log median income of 0, the mean achievement gap between boys and girls is estimated to be -1.356 standard deviations from the national average.\n\n\n\nNot incorrect, but awkward:\n\nlog median income is not a natural quantity\nthe sign is confusing"
  },
  {
    "objectID": "slides/week7-slr.html#parameter-interpretations-intercept-1",
    "href": "slides/week7-slr.html#parameter-interpretations-intercept-1",
    "title": "Simple linear regression",
    "section": "Parameter interpretations: intercept",
    "text": "Parameter interpretations: intercept\nBetter:\n\nFor school districts with a median income of 1 dollar, the mean achievement gap is estimated to favor girls by 1.356 standard deviations from the national average.\n\n\nCheck your understanding:\n\nwhy 1 dollar and not 0 dollars?\nwhy not -1.356?\n\nNot of particular interest here because no districts have a median income of 1 USD."
  },
  {
    "objectID": "slides/week7-slr.html#parameter-interpretations-slope",
    "href": "slides/week7-slr.html#parameter-interpretations-slope",
    "title": "Simple linear regression",
    "section": "Parameter interpretations: slope",
    "text": "Parameter interpretations: slope\nThe slope \\(\\beta_1\\) represents the change in mean response \\(\\mathbb{E}y_i\\) per unit change in \\(x_i\\).\n\nIn the SEDA example:\n\nEach increase of log median income by 1 is associated with an estimated increase in mean achievement gap of 0.121 standard deviations from the national average in favor of boys.\n\n\n\nNot incorrect, but a bit awkward – how much is a change in log median income of 1 unit?"
  },
  {
    "objectID": "slides/week7-slr.html#parameter-interpretations-slope-1",
    "href": "slides/week7-slr.html#parameter-interpretations-slope-1",
    "title": "Simple linear regression",
    "section": "Parameter interpretations: slope",
    "text": "Parameter interpretations: slope\nBetter:\n\nEvery doubling of median income is associated with an estimated increase in the mean achievement gap of 0.084 standard deviations from the national average in favor of boys.\n\n\nWhy doubling??\nHint: \\(\\hat{\\beta}_1\\log (2x) = \\hat{\\beta}_1\\log x + \\hat{\\beta}_1 \\log 2\\)"
  },
  {
    "objectID": "slides/week7-slr.html#parameter-interpretations-error-variance",
    "href": "slides/week7-slr.html#parameter-interpretations-error-variance",
    "title": "Simple linear regression",
    "section": "Parameter interpretations: error variance",
    "text": "Parameter interpretations: error variance\nThe error variance \\(\\sigma^2\\) represents the variability in the response \\(y\\) after accounting for the explanatory variable \\(x\\).\n\nIn the SEDA example:\n\nAfter adjusting for log median income, the gender achievement gap varies among districts by an estimated 0.11 standard deviations from the national average.\n\n\n\nNote that \\(\\hat{\\sigma}\\) is reported for interpretation on the original scale, rather than \\(\\hat{\\sigma}^2\\)."
  },
  {
    "objectID": "slides/week7-slr.html#parameter-interpretations-error-variance-1",
    "href": "slides/week7-slr.html#parameter-interpretations-error-variance-1",
    "title": "Simple linear regression",
    "section": "Parameter interpretations: error variance",
    "text": "Parameter interpretations: error variance\nCompare the estimated ‘raw’ variance in gender gap with the estimated residual variance after accounting for log median income:\n\n\nCode\nprint('raw variance: ', y.var(ddof = 1))\nprint('estimated residual variance: ', sigmasq_hat)\n\n\nraw variance:  0.015355417268162816\nestimated residual variance:  0.01317117061613733\n\n\n\nThe estimated variability in achievement gap diminishes a little after adjusting for log median income. The relative reduction is:\n\\[\n\\frac{\\hat{\\sigma}^2_\\text{raw} - \\hat{\\sigma}^2}{\\hat{\\sigma}^2_\\text{raw}}\n\\]\n\n\nIn the SEDA example, the reduction was about 14%:\n\n\nCode\nprint('relative reduction in variance: ',\n    (y.var(ddof = 1) - sigmasq_hat)/y.var(ddof = 1)\n)\n\n\nrelative reduction in variance:  0.1422459978703541"
  },
  {
    "objectID": "slides/week7-slr.html#parameter-interpretations-variance",
    "href": "slides/week7-slr.html#parameter-interpretations-variance",
    "title": "Simple linear regression",
    "section": "Parameter interpretations: variance",
    "text": "Parameter interpretations: variance\nA closely related quantity is the R-squared statistic, which simply adjusts the denominator of the error variance estimate:\n\\[\n\\frac{\\hat{\\sigma}^2_\\text{raw} - \\frac{n - 2}{n - 1}\\hat{\\sigma}^2}{\\hat{\\sigma}^2_\\text{raw}}\n\\]\n\nused as a measure of fit\ninterpreted as the proportion of variation in the response explained by the model"
  },
  {
    "objectID": "slides/week7-slr.html#general-parameter-interpretations",
    "href": "slides/week7-slr.html#general-parameter-interpretations",
    "title": "Simple linear regression",
    "section": "General parameter interpretations",
    "text": "General parameter interpretations\nThere is some general language for interpreting the parameter estimates:\n\n(Intercept) When [\\(x_i = 0\\)] the mean [response variable] is estimated to be [\\(\\hat{\\beta}_0\\) units].\n(Slope) Every [one-unit increase in \\(x_i\\)] is associated with an estimated change in mean [response variable] of [\\(\\hat{\\beta}_1\\) units].\n(Error variance) After adjusting for [explanatory variable], the remaining variability in [response variable] is an estimated [\\(\\hat{\\sigma}\\) units] about the mean.\n\n\nYou can use this standard language as a formulaic template for interpreting estimated parameters."
  },
  {
    "objectID": "slides/week7-slr.html#centering-the-explanatory-variable",
    "href": "slides/week7-slr.html#centering-the-explanatory-variable",
    "title": "Simple linear regression",
    "section": "Centering the explanatory variable",
    "text": "Centering the explanatory variable\nIf we want the intercept to be meaningful, we could center the explanatory variable and instead fit:\n\\[\ny_i = \\beta_0 + \\beta_1 (x_i - \\bar{x}) + \\epsilon_i\n\\]\n\n\nCode\n# center log median income\nlog_income_ctr = (regdata.log_income - regdata.log_income.mean()).values\n\n# form x matrix\nx_ctr = sm.tools.add_constant(log_income_ctr)\n\n# refit model\nslr_ctr = sm.OLS(endog = y, exog = x_ctr)\nbeta_hat_ctr = slr_ctr.fit().params\n\n# display parameter estimates\nprint('coefficient estimates: ', beta_hat_ctr)\nprint('error variance estimate: ', slr_ctr.fit().scale)\n\n\ncoefficient estimates:  [-0.02105724  0.12105696]\nerror variance estimate:  0.01317117061613733\n\n\n\n\nFor a district with average log median income, the estimated achievement gap favors boys by 0.021 standard deviations from the national average.\n\n\nnote that this is just a location shift so other estimates are unchanged\ncan recover the original intercept estimate as \\(\\hat{\\beta}_0 - \\hat{\\beta}_1 \\bar{x}\\)\n\n\n\n\nbeta_hat_ctr[0] - beta_hat_ctr[1]*regdata.log_income.mean()\n\n-1.3561699570330035"
  },
  {
    "objectID": "slides/week7-slr.html#other-transformations",
    "href": "slides/week7-slr.html#other-transformations",
    "title": "Simple linear regression",
    "section": "Other transformations",
    "text": "Other transformations\nWe could seek to adjust the model so that the intercept is interpreted as the gap at the district with the smallest median income:\n\\[\ny_i = \\beta_0 + \\beta_1 \\log\\left(x_i - x_{(1)} + 1 \\right), \\quad x_i: \\text{median income for district } i\n\\]\n\nBut this changes the meaning of the other model terms:\n\n\\(\\beta_1\\) represents the change in mean gap associated with multiplicative changes in the amount by which a district’s median income exceeds that of the poorest district\n\\(\\sigma^2\\) is the variability of the gap after adjusting for the log of the difference in median income from the median income of the poorest district"
  },
  {
    "objectID": "slides/week7-slr.html#other-transformations-1",
    "href": "slides/week7-slr.html#other-transformations-1",
    "title": "Simple linear regression",
    "section": "Other transformations",
    "text": "Other transformations\nUnsurprisingly, estimates are not invariant under arbitrary transformations, so if the meanings of the other parameters change, then so do the estimates:\n\n# center log median income\nincome = np.exp(regdata.log_income) \nincome_shifted = income - income.min()\nlog_income_shifted = np.log(income_shifted + 1)\n\n# form x matrix\nx_shifted = sm.tools.add_constant(log_income_shifted)\n\n# refit model\nslr_shifted = sm.OLS(endog = y, exog = x_shifted)\nbeta_hat_shifted = slr_shifted.fit().params\n\n# display parameter estimates\nprint('coefficient estimates: ', beta_hat_shifted)\nprint('error variance estimate: ', slr_shifted.fit().scale)\n\ncoefficient estimates:  const        -0.544166\nlog_income    0.049851\ndtype: float64\nerror variance estimate:  0.01390497936420638\n\n\n\nNote also it’s not possible to express the old parameters as functions of the new parameters; this is a fundamentally different model."
  },
  {
    "objectID": "slides/week7-slr.html#uncertainty-quantification",
    "href": "slides/week7-slr.html#uncertainty-quantification",
    "title": "Simple linear regression",
    "section": "Uncertainty quantification",
    "text": "Uncertainty quantification\nA great benefit of the simple linear regression model relative to a best-fit line is that the error variance estimate allows for uncertainty quantification.\n\nWhat that means is that one can describe precisely:\n\nvariation in the estimates (i.e., estimated model reliability);\nvariation in predictions made using the estimated model (i.e., predictive reliability)."
  },
  {
    "objectID": "slides/week7-slr.html#understanding-variation-in-estimates",
    "href": "slides/week7-slr.html#understanding-variation-in-estimates",
    "title": "Simple linear regression",
    "section": "Understanding variation in estimates",
    "text": "Understanding variation in estimates\nWhat would happen to the estimates if they were computed from a different sample?\n\nWe can explore this idea a little by calculating least squares estimates from several distinct subsamples of the dataset.\n\n\n\nThe lines are pretty similar, but they change a bit from subsample to subsample."
  },
  {
    "objectID": "slides/week7-slr.html#variance-of-least-squares",
    "href": "slides/week7-slr.html#variance-of-least-squares",
    "title": "Simple linear regression",
    "section": "Variance of least squares",
    "text": "Variance of least squares\n\nHow much should one expect the estimates to change depending on the data they are fit to?\n\n\nIt can be shown that the variances and covariance of the estimates are:\n\\[\n\\left[\\begin{array}{cc}\n    \\text{var}\\hat{\\beta}_0 & \\text{cov}\\left(\\hat{\\beta}_0, \\hat{\\beta}_1\\right) \\\\\n    \\text{cov}\\left(\\hat{\\beta}_0, \\hat{\\beta}_1\\right) &\\text{var}\\hat{\\beta}_1\n    \\end{array}\\right]\n   = \\sigma^2 \\left(\\mathbf{X'X}\\right)^{-1}\n\\]\n\n\nBear in mind that the randomness comes from the \\(\\epsilon_i\\) model term.\n\nthese quantify how much the parameters vary across collections of \\(y_i\\)’s measured at exactly the same values of \\(x_i\\)\nthese are not variances of the parameters; \\(\\beta_0\\) and \\(\\beta_1\\) are constants, i.e., not random\nthey are also not variances of the estimates – e.g., \\(0.121\\) is yet another constant"
  },
  {
    "objectID": "slides/week7-slr.html#standard-errors",
    "href": "slides/week7-slr.html#standard-errors",
    "title": "Simple linear regression",
    "section": "Standard errors",
    "text": "Standard errors\nSo the variances can be estimated by plugging in \\(\\color{red}{\\hat{\\sigma}^2}\\) for \\(\\sigma\\) in the variance-covariance matrix from the previous slide.\n\nThe estimated standard deviations are known as standard errors:\n\\[\n\\text{SE}(\\hat{\\beta}_0) = \\sqrt{\\color{red}{\\hat{\\sigma}^2}(\\mathbf{X'X})^{-1}_{11}} \\qquad\\text{and}\\qquad \\text{SE}(\\hat{\\beta}_1) = \\sqrt{\\color{red}{\\hat{\\sigma}^2}(\\mathbf{X'X})^{-1}_{22}}\n\\]"
  },
  {
    "objectID": "slides/week7-slr.html#computations-and-intepretations",
    "href": "slides/week7-slr.html#computations-and-intepretations",
    "title": "Simple linear regression",
    "section": "Computations and intepretations",
    "text": "Computations and intepretations\nThe estimated variance-covariance of the least squares estimates is computed by the .cov_params() method:\n\nslr.fit().cov_params()\n\narray([[ 0.01708179, -0.00154692],\n       [-0.00154692,  0.00014026]])\n\n\n\nthe intercept estimate varies by an estimated \\(\\sqrt{0.0171} = 0.131\\) across datasets, with \\(x_i\\) fixed\nthe slope estimate varies by an estimated \\(\\sqrt{0.00014} = 0.0118\\) across datasets, with \\(x_i\\) fixed"
  },
  {
    "objectID": "slides/week7-slr.html#confidence-intervals",
    "href": "slides/week7-slr.html#confidence-intervals",
    "title": "Simple linear regression",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nAbout 95% of the time, the true values \\(\\beta_0, \\beta_1\\) will be within 2SE of any particular estimates \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\).\n\nThe intervals \\(\\hat{\\beta}_j \\pm 2 SE(\\hat{\\beta}_j)\\) provide ranges of possible values for the true values \\(\\beta_j\\):\n\nslr.fit().conf_int()\n\narray([[-1.61283063, -1.09950928],\n       [ 0.09779946,  0.14431446]])\n\n\n\n\nThis supports two important inferences:\n\nwith 95% confidence, the intercept is estimated to be between -1.613 and -1.010\nwith 95% confidence, the slope is estimated to be between 0.098 and 0.144\n\n\n\nAnd the width of those intervals conveys a sense of the uncertainty associated with the estimates."
  },
  {
    "objectID": "slides/week7-slr.html#visualizing-uncertainty",
    "href": "slides/week7-slr.html#visualizing-uncertainty",
    "title": "Simple linear regression",
    "section": "Visualizing uncertainty",
    "text": "Visualizing uncertainty\nIt’s fairly common practice to add a band around the plotted line to indicate estimated variability.\n\n\n\n\n\nrepresents the uncertainty/variability of the parameter estimates, i.e., of the trend line\ndoes not represent the uncertainty/variability of the observations – note the spread of data is considerably broader than the uncertainty band"
  },
  {
    "objectID": "slides/week1-intro.html#attendance-form",
    "href": "slides/week1-intro.html#attendance-form",
    "title": "Course introduction",
    "section": "Attendance form",
    "text": "Attendance form"
  },
  {
    "objectID": "slides/week1-intro.html#case-study-1-ace-and-health",
    "href": "slides/week1-intro.html#case-study-1-ace-and-health",
    "title": "Course introduction",
    "section": "Case study 1: ACE and health",
    "text": "Case study 1: ACE and health\n\nAssociation between adverse childhood experiences and general health, by sex."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-1-ace-and-health-1",
    "href": "slides/week1-intro.html#case-study-1-ace-and-health-1",
    "title": "Course introduction",
    "section": "Case study 1: ACE and health",
    "text": "Case study 1: ACE and health\nYou will:\n\nprocess and recode 10K survey responses from CDC’s 2019 behavior risk factor surveillance survey (BRFSS)\ncross-tabulate health-related measurements with frequency of adverse childhood experiences"
  },
  {
    "objectID": "slides/week1-intro.html#case-study-2-seda",
    "href": "slides/week1-intro.html#case-study-2-seda",
    "title": "Course introduction",
    "section": "Case study 2: SEDA",
    "text": "Case study 2: SEDA\n\nEducation achievement gaps as functions of socioeconomic indicators, by gender."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-2-seda-1",
    "href": "slides/week1-intro.html#case-study-2-seda-1",
    "title": "Course introduction",
    "section": "Case study 2: SEDA",
    "text": "Case study 2: SEDA\nYou will:\n\nmerge test scores and socioeconomic indicators from the 2018 Standford Education Data Archive by school district\nvisually assess correlations between gender achievement gaps among grade schoolers and socioeconomic indicators across school districts in CA"
  },
  {
    "objectID": "slides/week1-intro.html#case-study-3-paleoclimatology",
    "href": "slides/week1-intro.html#case-study-3-paleoclimatology",
    "title": "Course introduction",
    "section": "Case study 3: Paleoclimatology",
    "text": "Case study 3: Paleoclimatology\n\nSea surface temperature reconstruction over the past 16,000 years."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-3-paleoclimatology-1",
    "href": "slides/week1-intro.html#case-study-3-paleoclimatology-1",
    "title": "Course introduction",
    "section": "Case study 3: Paleoclimatology",
    "text": "Case study 3: Paleoclimatology\n\nClustering of diatom relative abundances in pleistocene (pre-11KyBP) vs. holocene (post-11KyBP) epochs."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-3-paleoclimatology-2",
    "href": "slides/week1-intro.html#case-study-3-paleoclimatology-2",
    "title": "Course introduction",
    "section": "Case study 3: Paleoclimatology",
    "text": "Case study 3: Paleoclimatology\nYou will:\n\nexplore ecological community structure from relative abundances of diatoms measured in ocean sediment core samples spanning ~15,000 years\nuse dimension reduction techniques to obtain measures of community structure\nidentify shifts associated with the transition from pleistocene to holocene epochs"
  },
  {
    "objectID": "slides/week1-intro.html#case-study-4-discrimination-at-dds",
    "href": "slides/week1-intro.html#case-study-4-discrimination-at-dds",
    "title": "Course introduction",
    "section": "Case study 4: Discrimination at DDS?",
    "text": "Case study 4: Discrimination at DDS?\n\nApparent disparity in allocation of DDS benefits across racial groups."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-4-discrimination-at-dds-1",
    "href": "slides/week1-intro.html#case-study-4-discrimination-at-dds-1",
    "title": "Course introduction",
    "section": "Case study 4: Discrimination at DDS?",
    "text": "Case study 4: Discrimination at DDS?\n\nExpenditure is strongly associated with age."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-4-discrimination-at-dds-2",
    "href": "slides/week1-intro.html#case-study-4-discrimination-at-dds-2",
    "title": "Course introduction",
    "section": "Case study 4: Discrimination at DDS?",
    "text": "Case study 4: Discrimination at DDS?\n\nCorrecting for age shows comparable expenditure across racial groups."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-4-discrimination-at-dds-3",
    "href": "slides/week1-intro.html#case-study-4-discrimination-at-dds-3",
    "title": "Course introduction",
    "section": "Case study 4: Discrimination at DDS?",
    "text": "Case study 4: Discrimination at DDS?\nYou will:\n\nassess the case for discrimination in allocation of DDS benefits\nidentify confounding factors present in the sample\nmodel median expenditure by racial group after correcting for age"
  },
  {
    "objectID": "slides/week1-intro.html#scope",
    "href": "slides/week1-intro.html#scope",
    "title": "Course introduction",
    "section": "Scope",
    "text": "Scope\nThis course is about developing your data science toolkit with foundational skills:\n\nCore competency with Python data science libraries\nCritical thinking about data\nVisualization and exploratory analysis\nApplication of basic statistical concepts and methods in practice\nCommunication and interpretation of results"
  },
  {
    "objectID": "slides/week1-intro.html#whats-unique-about-pstat100",
    "href": "slides/week1-intro.html#whats-unique-about-pstat100",
    "title": "Course introduction",
    "section": "What’s unique about PSTAT100?",
    "text": "What’s unique about PSTAT100?\nThere are a few distinctive aspects:\n\nmultiple end-to-end case studies\nquestion-driven rather than method-driven\nemphasis on project workflow\ndata storytelling and communication"
  },
  {
    "objectID": "slides/week1-intro.html#limitations",
    "href": "slides/week1-intro.html#limitations",
    "title": "Course introduction",
    "section": "Limitations",
    "text": "Limitations\nThere are also some things we won’t cover:\n\nPredictive modeling or machine learning\nAlgorithm design and implementation\nTechniques and methods for big data\nTheoretical basis for methods"
  },
  {
    "objectID": "slides/week1-intro.html#weekly-pattern",
    "href": "slides/week1-intro.html#weekly-pattern",
    "title": "Course introduction",
    "section": "Weekly Pattern",
    "text": "Weekly Pattern\nWe’ll follow a simple weekly pattern:\n\nMondays\n\nLecture\nAssignments due 11:59pm PST\n\nWednesdays\n\nLecture\n\nThursdays\n\nSection"
  },
  {
    "objectID": "slides/week1-intro.html#course-pages-materials",
    "href": "slides/week1-intro.html#course-pages-materials",
    "title": "Course introduction",
    "section": "Course pages & materials",
    "text": "Course pages & materials\n\nMaterials via course website lnbaracaldol.github.io/PSTAT100-F23\nComputing at pstat100.lsit.ucsb.edu\nAssignments/gradebook at Gradescope\nNectir at Nectir"
  },
  {
    "objectID": "slides/week1-intro.html#tentative-schedule",
    "href": "slides/week1-intro.html#tentative-schedule",
    "title": "Course introduction",
    "section": "Tentative schedule",
    "text": "Tentative schedule\n\n\n\nWeek\nTopic\nLab\nHomework\nProject\n\n\n\n\n1\nData science life cycle\n\n\n\n\n\n2\nTidy data\nL0\n\n\n\n\n3\nSampling and bias\nL1\n\n\n\n\n4\nStatistical graphics\nL2\nH1\n\n\n\n5\nKernel density estimation\nL3\n\nMP1\n\n\n6\nPrincipal components\nL4\nH2\n\n\n\n7\nSimple regression\n\n\nMP2\n\n\n8\nMultiple regression\nL5\nH3\n\n\n\n9\nClassification and clustering\n\n\nCP1\n\n\n10\nCase study\n\nH4\n\n\n\n11\nFinals week\n\n\nCP2"
  },
  {
    "objectID": "slides/week1-intro.html#assessments",
    "href": "slides/week1-intro.html#assessments",
    "title": "Course introduction",
    "section": "Assessments",
    "text": "Assessments\n\nLabs introduce and develop core skills\nHomeworks apply core skills to case studies\nProjects practice creative problem-solving"
  },
  {
    "objectID": "slides/week1-intro.html#policies",
    "href": "slides/week1-intro.html#policies",
    "title": "Course introduction",
    "section": "Policies",
    "text": "Policies\n\nCommunication\n\nIf you have questions, please come to office hours\nAvoid email except for personal matters\n\nDeadlines and late work\n\nOne-hour grace period on all deadlines\n24-hour late submissions\n75% partial credit thereafter for late work"
  },
  {
    "objectID": "slides/week1-intro.html#policies-1",
    "href": "slides/week1-intro.html#policies-1",
    "title": "Course introduction",
    "section": "Policies",
    "text": "Policies\n\nGrades\n\nFinal weighting and grade assignment at instructor’s discretion\nDo not expect 92+% = A, 90-92% = A-, 87-89.9 = B+, etc.\nA’s are awarded sparingly and indicate exceptional work"
  },
  {
    "objectID": "slides/week1-intro.html#other-info",
    "href": "slides/week1-intro.html#other-info",
    "title": "Course introduction",
    "section": "Other info",
    "text": "Other info\n\nInformal section swaps are allowed with TA permission\nAttendance required at all class meetings, but a few absences without notice are okay\nHonors contracts not available this quarter\nOffice hours start week 2, check website for schedule"
  },
  {
    "objectID": "slides/week1-intro.html#getting-started",
    "href": "slides/week1-intro.html#getting-started",
    "title": "Course introduction",
    "section": "Getting started",
    "text": "Getting started\n\nLab this week will introduce you to computing and course infrastructure\nCheck access to Gradescope, LSIT, course page\nReview syllabus"
  },
  {
    "objectID": "activities/week5-activity-smoothing.html",
    "href": "activities/week5-activity-smoothing.html",
    "title": "Smoothing activity",
    "section": "",
    "text": "import os\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\nimport numpy as np\nimport pandas as pd\nimport altair as alt\nimport statsmodels.api as sm\nfrom statsmodels.nonparametric.kde import kernel_switch\nfrom sklearn import mixture\nalt.data_transformers.disable_max_rows()\nimport warnings\n\n# Suppress all warnings\nwarnings.filterwarnings(\"ignore\")\nWe’ll work with sustainability index data for US cities to explore density estimation further. These data are imported below\nsust = pd.read_csv('sustainability.csv')\nsust.head()\n\n\n\n\n\n\n\n\nGEOID_MSA\nName\nEcon_Domain\nSocial_Domain\nEnv_Domain\nSustain_Index\n\n\n\n\n0\n310M300US10100\nAberdeen, SD Micro Area\n0.565264\n0.591259\n0.444472\n1.600995\n\n\n1\n310M300US10140\nAberdeen, WA Micro Area\n0.427671\n0.520744\n0.429274\n1.377689\n\n\n2\n310M300US10180\nAbilene, TX Metro Area\n0.481092\n0.496874\n0.454192\n1.432157\n\n\n3\n310M300US10220\nAda, OK Micro Area\n0.466566\n0.526206\n0.425964\n1.418737\n\n\n4\n310M300US10300\nAdrian, MI Micro Area\n0.497908\n0.602440\n0.282499\n1.382847\nThere are 933 distinct cities; each row in the dataset is an observation for one city.\nn, p = sust.shape\nn\n\n933\nsust.Name.unique().shape\n\n(933,)"
  },
  {
    "objectID": "activities/week5-activity-smoothing.html#exploration",
    "href": "activities/week5-activity-smoothing.html#exploration",
    "title": "Smoothing activity",
    "section": "Exploration",
    "text": "Exploration\nUse the cell below to experiment and answer the following.\n\nHow does the KDE differ if a parabolic (epa) kernel is used in place of a Gaussian (gau) kernel while the bandwidth is held constant?\nWhat effect does a triangular kernel (tri) have on how local peaks appear?\nPick two kernels. What will happen to the KDE for large bandwidths?\nWhich kernel seems to do the best job at capturing the shape closely without under-smoothing?\n\n\n# compute density estimate\nkde = sm.nonparametric.KDEUnivariate(sust.Env_Domain)\nkde.fit(kernel = 'uni', fft = False, bw = 0.02)\n\n# arrange as dataframe\nkde_df = pd.DataFrame({'Environmental sustainability index': kde.support, 'Density': kde.density})\n\n# plot\nsmooth2 = alt.Chart(\n    kde_df\n).mark_line(color = 'black').encode(\n    x = 'Environmental sustainability index:Q',\n    y = alt.Y('Density:Q', scale = alt.Scale(domain = [0, 12]))\n)\n\nhist + smooth2 + smooth2.mark_area(opacity = 0.3)\n\n\n\n\n\n\n\nRecord your observations here."
  },
  {
    "objectID": "activities/week5-activity-smoothing.html#explore",
    "href": "activities/week5-activity-smoothing.html#explore",
    "title": "Smoothing activity",
    "section": "Explore",
    "text": "Explore\nUse the cell below to explore the following questions.\n\nWhat happens if you fit the GMM with different numbers of components?\nDoes the solution change if the GMM is re-fitted?\n\n\n# configure and fit mixture model\ngmm_hawks = mixture.GaussianMixture(n_components = 2)\ngmm_hawks.fit(samp.length.values.reshape(-1, 1))\n\n# print centers\nprint('component center(s): ', gmm_hawks.means_)\n\n# compute a grid of lengths\ngrid_hawks = np.linspace(population_hawks.length.min(), population_hawks.length.max(), num = 500)\ndens = np.exp(gmm_hawks.score_samples(grid_hawks.reshape(-1, 1)))\n\n# plot\ngmm_smooth_hawks = alt.Chart(\n    pd.DataFrame({'length': grid_hawks, 'density': dens})\n).mark_line(color = 'black').encode(\n    x = 'length',\n    y = 'density'\n)\n\nhist_hawks + gmm_smooth_hawks\n\ncomponent center(s):  [[50.70543698]\n [57.33071086]]"
  },
  {
    "objectID": "activities/week5-activity-smoothing.html#explore-1",
    "href": "activities/week5-activity-smoothing.html#explore-1",
    "title": "Smoothing activity",
    "section": "Explore",
    "text": "Explore\nUse the cell below to answer the following questions.\n\nAre there any bandwidths that give you a straight-line fit?\nWhat seems to be the minimum bandwidth?\nWhich bandwidth best captures the pattern of scatter?\n\n\n# fit loess smooth\nloess = sm.nonparametric.lowess(endog = life.life_exp.values,\n                                exog = life.log_gdp.values, \n                                frac = 0.3,\n                                xvals = gdp_grid)\n\n# store as data frame\nloess_df = pd.DataFrame({'log_gdp': gdp_grid, 'life_exp': loess})\n\n# plot\nloess_smooth = alt.Chart(\n    loess_df\n).mark_line(\n    color = 'black'\n).encode(\n    x = 'log_gdp', \n    y = alt.Y('life_exp', scale = alt.Scale(zero = False))\n)\n\nscatter + loess_smooth"
  },
  {
    "objectID": "slides/week2-tidy.html",
    "href": "slides/week2-tidy.html",
    "title": "Tidy data",
    "section": "",
    "text": "Complete Q1-Q4 (fruit_info section) of Lab 1 before section"
  },
  {
    "objectID": "slides/week5-smoothing.html",
    "href": "slides/week5-smoothing.html",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "",
    "text": "More on density estimation\n\nnon-Gaussian kernels\nmultivariate KDE\nmixture models\n\nScatterplot smoothing\n\nKernel smoothing\nLOESS"
  },
  {
    "objectID": "slides/week5-density.html#drawing-a-local-histogram-1",
    "href": "slides/week5-density.html#drawing-a-local-histogram-1",
    "title": "Exploratory analysis and density estimation",
    "section": "Drawing a local histogram",
    "text": "Drawing a local histogram\n\\[\n\\text{hist}_\\text{local}(\\color{red}{x}) = \\frac{1}{nb} \\sum_{i = 1}^n \\mathbf{1}\\left\\{|x_i - \\color{red}{x}| &lt; \\frac{b}{2}\\right\\}\n\\]"
  },
  {
    "objectID": "slides/week4-principles.html",
    "href": "slides/week4-principles.html",
    "title": "Figure design",
    "section": "",
    "text": "DataTransformerRegistry.enable('default')"
  },
  {
    "objectID": "slides/week6-pca.html",
    "href": "slides/week6-pca.html",
    "title": "Principal components",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport altair as alt\nfrom sklearn.decomposition import PCA\nfrom scipy import linalg\nalt.data_transformers.disable_max_rows()\n\n# data import\ncity_sust = pd.read_csv('data/uscity-sustainability-indices.csv')\n\n# extract social and economic indices\nx_mx = city_sust.iloc[:, 2:5]\n\n# center and scale\nn = len(x_mx) # sample size\nz_mx = (x_mx - x_mx.mean())/x_mx.std() # (xi - xbar)/sx"
  },
  {
    "objectID": "slides/week6-covariation.html",
    "href": "slides/week6-covariation.html",
    "title": "Covariance and correlation",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport altair as alt\nfrom sklearn.decomposition import PCA\nfrom scipy import linalg\nalt.data_transformers.disable_max_rows()\n\ncity_sust = pd.read_csv('data/uscity-sustainability-indices.csv')"
  }
]